----- Start of src\analysis.py -----
from dataclasses import (
    dataclass,
)  # dataclass is a decorator that allows you to create a class with a minimal amount of code (see https://docs.python.org/3/library/dataclasses.html)


@dataclass
class Parameters:
    reward_live: float
    """Reward for living at each time step"""

    gamma: float
    """Discount factor"""
    noise: float
    """Probability of taking a random action instead of the chosen one"""


def prefer_close_exit_following_the_cliff() -> Parameters:
    return Parameters(reward_live=-0.01, gamma=0.9, noise=0.1)


def prefer_close_exit_avoiding_the_cliff() -> Parameters:
    return Parameters(...)


def prefer_far_exit_following_the_cliff() -> Parameters:
    return Parameters(...)


def prefer_far_exit_avoiding_the_cliff() -> Parameters:
    return Parameters(...)


def never_end_the_game() -> Parameters:
    return Parameters(...)


----- End of src\analysis.py -----

----- Start of src\mdp.py -----
from typing import TypeVar, Generic
from abc import abstractmethod, ABC


A = TypeVar("A")
S = TypeVar("S")


class MDP(ABC, Generic[S, A]):
    """Adversarial Markov Decision Process"""

    @abstractmethod
    def is_final(self, state: S) -> bool:
        """Returns true if the given state is final (i.e. the game is over)."""

    @abstractmethod
    def available_actions(self, state: S) -> list[A]:
        """Returns the list of available actions for the current agent from the given state."""

    @abstractmethod
    def transitions(self, state: S, action: A) -> list[tuple[S, float]]:
        """Returns the list of next states with the probability of reaching it by performing the given action."""

    @abstractmethod
    def states(self) -> list[S]:
        """Returns the list of all states."""

    @abstractmethod
    def reward(self, state: S, action: A, new_state) -> float:
        """Reward function"""


----- End of src\mdp.py -----

----- Start of src\qlearning.py -----
from rlenv import RLEnv, Observation
import numpy as np
import numpy.typing as npt


class QLearning:
    """Tabular QLearning"""

    def __init__(self, learning_rate: float, discount_factor: float, epsilon: float):
        # Initialize parameters
        self.learning_rate = learning_rate
        self.discount_factor = discount_factor
        self.epsilon = epsilon
        # Initialize Q-table
        self.q_table = {}

    def choose_action(self, state):
        # Implement action selection using epsilon-greedy strategy
        pass

    def update(self, state, action, reward, next_state):
        # Implement Q-table update
        pass


----- End of src\qlearning.py -----

----- Start of src\value_iteration.py -----
from mdp import MDP, S, A
from typing import Generic


class ValueIteration(Generic[S, A]):
    def __init__(self, mdp: MDP[S, A], gamma: float):
        # senf.values est nÃ©cessaire pour fonctionner avec utils.show_values
        self.values = dict[S, float]()

    def value(self, state: S) -> float:
        """Returns the value of the given state."""

    def policy(self, state: S) -> A:
        """Returns the action that maximizes the Q-value of the given state."""

    def qvalue(self, state: S, action: A) -> float:
        """Returns the Q-value of the given state-action pair based on the state values."""

    def _compute_value_from_qvalues(self, state: S) -> float:
        """
        Returns the value of the given state based on the Q-values.

        This is a private method, meant to be used by the value_iteration method.
        """

    def value_iteration(self, n: int):
        for _ in range(n):
            new_values = dict[S, float]()
            for state in self.mdp.states():
                if self.mdp.is_final(state):
                    new_values[state] = 0
                else:
                    new_values[state] = max(self.qvalue(state, action) for action in self.mdp.available_actions(state))
            self.values = new_values



----- End of src\value_iteration.py -----

