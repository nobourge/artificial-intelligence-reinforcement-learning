----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2023-2024 projet RL for chatGPT.txt -----Dans ce projet, vous allez implÃ©menter des algorithmes dâ€™apprentissage par renforcement (reinforce-
ment learning). On vous fournit des fichiers de base pour le projet que vous pouvez les tÃ©lÃ©charger sur
lâ€™universitÃ© virtuelle.
2 Introduction
Intuitivement, un processus de dÃ©cision markovien (Markov Decision Process, MDP) est composÃ© dâ€™Ã©-
tats dans lesquels un agent effectue des actions qui ont une certaine probabilitÃ© dâ€™atterrir dans un autre
Ã©tat.
AprÃ¨s chaque transition, lâ€™agent reÃ§oit une rÃ©compense qui donne une indication sur la qualitÃ© de
lâ€™action: plus la rÃ©compense est Ã©levÃ©e, meilleure est lâ€™action. Le but dâ€™un agent est de maximiser la
somme des rÃ©compenses au cours dâ€™un Ã©pisode (câ€™est-Ã -dire une partie).
Par consÃ©quent, pour chaque Ã©tat ğ‘ , on peut calculer sa valeur ğ‘‰(ğ‘ ) qui correspond Ã  la meilleure
somme des rÃ©compenses possible Ã  partir de ğ‘  grÃ¢ce Ã  lâ€™Ã©quation de Bellman prÃ©sentÃ©e dans lâ€™E-
quation 1.
ğ‘‰(ğ‘ ) = maxâˆ‘ğ‘ƒ(ğ‘ ,ğ‘,ğ‘ â€²)[ğ‘…(ğ‘ ,ğ‘,ğ‘ â€²)+ğ›¾ğ‘‰(ğ‘ â€²)] (1)
ğ‘âˆˆğ´
3 Value iteration
Lâ€™algorithme de Â«value iterationÂ» a pour but dâ€™approximer itÃ©rativement la valeur ğ‘‰(ğ‘ ) de chaque Ã©tat
s Ã  lâ€™aide de lâ€™Equation 2.
ğ‘‰ = maxâˆ‘ğ‘ƒ(ğ‘ ,ğ‘,ğ‘ â€²)[ğ‘…(ğ‘ ,ğ‘,ğ‘ â€²)+ğ›¾ğ‘‰ ]
ğ‘˜+1 ğ‘˜(ğ‘ â€²) (2)
ğ‘
ğ‘ â€²
Lâ€™algorithme de value iteration prend en paramÃ¨tre un entier ğ‘˜ qui dÃ©termine combien dâ€™itÃ©ration ef-
fectuer. ImplÃ©mentez lâ€™algorithme value iteration dans le fichier value_iteration.py.
Conseils:
â€¢ Quand vous implÃ©mentez lâ€™algorithme, faites attention Ã  vous baser sur ğ‘‰ pour calculer ğ‘‰
ğ‘˜ ğ‘˜+1
et Ã  ne pas modifier ğ‘‰ durant lâ€™itÃ©ration.
ğ‘˜
â€¢ Nâ€™oubliez pas que la valeur dâ€™un Ã©tat terminal est 0 par dÃ©finition.
4 Trouvez les bons paramÃ¨tres
Dans cet exercice, on vous demande de trouver les bons paramÃ¨tres du MDP et de value iteration pour
induire un certain comportement Ã  lâ€™agent.
Le MDP considÃ©rÃ© est celui prÃ©sentÃ© dans la Figure 1. Il sâ€™agit dâ€™un lle.World dont les rewards ont Ã©tÃ©
modifiÃ©es et qui comprend sept Ã©tats terminaux:
â€¢ Cinq â€œravinsâ€ en bas de la carte. Lâ€™agent meurt sâ€™il sâ€™y rend et reÃ§oit une â€œrÃ©compenseâ€ de âˆ’10;
â€¢ Une sortie proche qui rapporte une rÃ©compense de 1;
â€¢ Une sortie lointaine qui rapporte une rÃ©compense de 10.
Figure 1: MDP pour lequel trouver les bons paramÃ¨tres
Le but de cet exercice est dâ€™induire les comportements suivants:
1. PrÃ©fÃ©rer la sortie proche (+1) en longeant la falaise.
2. PrÃ©fÃ©rer la sortie proche (+1) en Ã©vitant la falaise.
3. PrÃ©fÃ©rer la sortie distante (+10) en longeant la falaise.
4. PrÃ©fÃ©rer la sortie distante (+10) en Ã©vitant la falaise.
5. Eviter de terminer le jeu (lâ€™Ã©pisode ne se termine jamais).
Pour ce faire, choisissez des valeurs adÃ©quates de chaque comportement dans la fonction correspon-
dante pour reward_live (la rÃ©compense Ã  chaque Ã©tape pour continuer le jeu), gamma (le discount fac-
tor) et noise (la probabilitÃ© de prendre une action alÃ©atoire) dans le fichier analysis.py.
5 Q-learning
Lâ€™algorithme de Q-learning consiste Ã  interagir avec lâ€™environnement pour mettre Ã  jour une fonction
dâ€™Ã©valuation ğ‘„(ğ‘ ,ğ‘). Contrairement aux exercices prÃ©cÃ©dents, on Ã©value ici la valeur dâ€™une action dans
un Ã©tat ğ‘„(ğ‘ ,ğ‘) et pas la valeur de lâ€™Ã©tat ğ‘‰(ğ‘ ).
Une fois que votre agent est entraÃ®nÃ©, vous pouvez exploiter la stratÃ©gie apprise en prenant lâ€™action
ayant la plus haute q-value max ğ‘„(ğ‘ ,ğ‘) dans chaque Ã©tat.
ğ‘âˆˆğ´
Pour entraÃ®ner votre agent, il est nÃ©cessaire dâ€™explorer lâ€™environnement. Pour ce faire, vous devez im-
plÃ©menter la stratÃ©gie dâ€™exploration dite Â«ğœº-greedyÂ» qui consiste Ã  choisir une action alÃ©atoire avec
une probabilitÃ© Îµ, et Ã  prendre la meilleure action avec une probabilitÃ© 1âˆ’ğœ€.
Note: Il nâ€™y a quasiment pas de tests pour les exercices liÃ©s au q-learning car ceux-ci sont trÃ¨s diffi-
ciles Ã  tester. On vous demande par contre dâ€™analyser vos rÃ©sultats dans le rapport (voir Section 6).
5.1 Q-learning tabulaire
Pour entraÃ®ner la fonction ğ‘„(ğ‘ ,ğ‘), on emploie une mÃ©thode adaptÃ©e de lâ€™Ã©quation de Bellman (Equa-
tion 1) illustrÃ©e dans lâ€™Equation 3, oÃ¹ ğ›¼ est le learning rate.
ğ‘„(ğ‘ ,ğ‘) â† (1âˆ’ğ›¼)ğ‘„(ğ‘ ,ğ‘)+ğ›¼[ğ‘…(ğ‘ ,ğ‘,ğ‘ â€²)+ğ›¾ğ‘‰(ğ‘ â€²)] (3)
ImplÃ©mentation
ImplÃ©mentez lâ€™algorithme de Q-learning dans le fichier qlearning.py. Faites en sorte que cette implÃ©-
mentation utilise un dictionnaire pour stocker les qvalues de chaque Ã©tat.
Conseils:
â€¢ Pour hasher des tableaux numpy, nous vous conseillons dâ€™utiliser hash(array.tobytes()).
â€¢ Pour favoriser lâ€™exploration, initialisez vos ğ‘„(ğ‘ ,ğ‘) Ã  1 et non Ã  0.
EntraÃ®nement et rapport
EntraÃ®nez votre algorithme sur les niveaux 1, 3 et 6 de LLE. Dans votre rapport, crÃ©ez un graphique
pour chacun de ces trois niveaux qui montre le score (câ€™est-Ã -dire la somme des rewards par Ã©pisode)
au cours de lâ€™entrainement. Voir Section 6 pour plus dâ€™informations sur le rapport.
Table 1: De gauche Ã  droite: niveau 1, niveau 3 et niveau 6 de LLE
Exemple
Lâ€™extrait de code ci-dessous montre ce Ã  quoi pourrait ressembler votre boucle dâ€™entrainement. Il nâ€™y
a aucune obligation de suivre ce canvas.
from lle import LLE
from rlenv.wrappers import TimeLimit
env = TimeLimit(LLE.level(1), 80) # Maximum 80 time steps
agents = [QAgent(lr, gamma, ...), QAgent(lr, gamma, ...), ...]
observation = env.reset()
done = truncated = False
score = 0
while not (done or truncated):
actions = [a.choose_action(observation) for a in agents]
next_observation, reward, done, truncated, info = env.step(actions)
for a in agents:
a.update(...)
score += reward
...
Lorsque vous rÃ©cupÃ©rez une observation, vous pouvez accÃ©der Ã  son contenu avec observation.data
qui contient un tableau numpy dont la forme est (n_agents, ...).
Conseil: LLE est un environnement multi-agent et utilise la librairie rlenv prÃ©vue Ã  cet effet. Nâ€™ou-
bliez pas de bien prendre en compte le caractÃ¨re multi-agent dans la reprÃ©sentation de vos donnÃ©es.
5.2 Approximate Q-learning
Le but de cet exercice est dâ€™implÃ©menter lâ€™Approximate Q-Learning (AQL) pour votre agent. Pour rap-
pel, AQL suppose lâ€™existence dâ€™une fonction ğ‘“(ğ‘ ) qui associe Ã  chaque Ã©tat (et pour chaque agent) un
vecteur de features [ğ‘“ (ğ‘ ),ğ‘“ (ğ‘ ),â€¦,ğ‘“ (ğ‘ )] avec ğ‘“ (ğ‘ ) : ğ‘† â†’ â„.
1 2 ğ‘› ğ‘˜
ConcrÃ¨tement, dans le cas de LLE, ğ‘“ (ğ‘ ) pourrait Ãªtre le nombre de gemmes non collectÃ©es, ğ‘“ (ğ‘ ) la
1 2
distance (en lignes) Ã  la gemme la plus proche, ğ‘“ (ğ‘ ) la distance (en colonnes) Ã  la gemme la plus
3
proche, ğ‘“ (ğ‘ ) la prÃ©sense (1) ou lâ€™absence (0) dâ€™un laser nord de lâ€™agent, etc.
4
Dans AQL, la fonction ğ‘„(ğ‘ ,ğ‘) dâ€™un agent ğ‘› est dÃ©finie comme indiquÃ© dans lâ€™Equation 4.
ğ‘˜
ğ‘„ (ğ‘ ,ğ‘) = âˆ‘ğ‘“ (ğ‘ )ğ‘¤ (4)
ğ‘› ğ‘– ğ‘,ğ‘–
ğ‘–=1
Dans le Q-learning, on dÃ©finit la TD-error ğ›¿ (temporal difference error) comme la diffÃ©rence entre la
Q-value estimÃ©e et la Q-value effective (estimÃ©e avec lâ€™Ã©quation de Bellman).
ğ›¿ = [ğ‘…(ğ‘ ,ğ‘,ğ‘ â€²)+ğ›¾ğ‘‰(ğ‘ â€²)]âˆ’ğ‘„(ğ‘ ,ğ‘) (5)
Dans AQL, le principe est de mettre Ã  jour les poids ğ‘¤ de ğ‘„ (ğ‘ ,ğ‘) de sorte Ã  diminuer la TD-error ğ›¿
ğ‘›
conformÃ©ment Ã  lâ€™Equation 6
ğ‘¤ â† ğ‘¤ +ğ›¼ğ›¿ğ‘“(ğ‘ ) (6)
ğ‘,ğ‘– ğ‘,ğ‘–
ImplÃ©mentation
Dans le fichier approximate_qlearning.py:
1. DÃ©finissez une fonction feature_extraction(world) qui prend en entrÃ©e le World et renvoie
pour chaque agent les features que vous aurez choisies.
2. ImplÃ©mentez lâ€™algorithme de Approximate Q-Learning.
Entrainement et rapport
EntraÃ®nez votre algorithme AQL sur les niveaux 1, 3 et 6 de LLE. Dans votre rapport, crÃ©ez un graphique
pour chacun de ces trois niveaux qui montre le score (câ€™est-Ã -dire la somme des rewards par Ã©pisode)
au cours de lâ€™entrainement. Voir Section 6 pour plus dâ€™informations sur le rapport.
6 Rapport
6.1 Value iteration
â€¢ Quelle limitation voyez-vous Ã  lâ€™algorithme de Value Iteration ? Dans quelle mesure est-il applic-
able Ã  un niveau avec un nombre plus important dâ€™agents ou avec une carte plus grande ?
â€¢ Expliquez pourquoi Value Iteration nâ€™est pas un algorithme de RL mais plutÃ´t un planificateur
hors-ligne.
â€¢ Appliquez lâ€™algorithme de Value Iteration sur le niveau 1 de LLE jusquâ€™Ã  ce que la policy se stabilise.
Ensuite, montrez graphiquement la stratÃ©gie dÃ©couverte par lâ€™algorithme.
6.2 ExpÃ©riences avec le Q-learning
Le but ici est de mesurer les performances des deux algorithmes (Q-learning et AQL) dans les niveaux
Ã©voquÃ©s prÃ©cedemment. Vous devez ensuite prÃ©senter ces donnÃ©es dans des graphiques adaptÃ©s (type
de graphique, Ã©chelle, axes, titre).
MÃ©trique
La mÃ©trique que vous devez tracer est le score, câ€™est-Ã -dire la somme des rÃ©compenses obtenues au
cours dâ€™un Ã©pisode.
MÃ©thode
Pour chaque expÃ©rience, donnez les paramÃ¨tres que vous avez utilisÃ©s:
â€¢ le discount factor ğ›¾
â€¢ le learning rate ğ›¼
â€¢ la fonction de feature extraction ğ‘“(ğ‘ )
â€¢ le niveau dont il est question
â€¢ la limite de temps que vous avez utilisÃ©e
â€¢ la maniÃ¨re dont Ã©volue votre ğœ€ au cours du temps
â€¢ â€¦
Graphiques
Nous vous conseillons la librairie matplotlib pour tracer des graphiques. Cette librairie est installÃ©e
automatiquement si vous avez utilisÃ© poetry.
Les graphiques que vous devez produire sont ceux qui indiquent le score moyen au cours du temps,
calculÃ© sur minimum 10 entrainements. Vos graphiques doivent aussi montrer la dÃ©viation standard
en plus de la moyenne, comme dans ce deuxiÃ¨me exemple de la documentation de matplotlib.
Remise
Le livrable de ce projet se prÃ©sente sous la forme dâ€™un fichier zip contenant vos sources python ainsi
que votre rapport en PDF. Nous vous encourageons Ã  utiliser un outil tel que Typst ou Latex (par ex-
emple avec Overleaf) pour rÃ©diger votre rapport.
Ce travail est individuel et doit Ãªtre rendu sur lâ€™UniversitÃ© Virtuelle pour le 10/12/2023 Ã  23:59.
7 Annexes
7.1 PrÃ©cisions sur lle.LLE
La classe lle.LLE est directement dÃ©diÃ©e au RL multi-agents (MARL) coopÃ©ratif. Par soucis de gÃ©nÃ©ral-
itÃ©, LLE utilise le vocabulaire des observations plutÃ´t que des Ã©tats afin de diffÃ©rencier ce que les agents
voient de lâ€™Ã©tat de lâ€™environnement.
Dans LLE, une observation peut se reprÃ©senter couche par couche, comme montrÃ© dans la Figure 2.
Figure 2: ReprÃ©sentation dâ€™une observation correspondant Ã  la situation de lâ€™imade de droite dans la
Table 1. Les cases noires reprÃ©sentent des 1, les cases grises des âˆ’1 et les cases blanches des 0. Par
soucis de concision, certaines couches (agent 2, agent 3, laser 2, laser 3) ont Ã©tÃ© ommise.
Le type dâ€™observation indiquÃ© dans la Figure 2 peut Ãªtre utilisÃ© en utilisant ObservationType.LAYERED.
Dans ce cas, chaque agent reÃ§oit une observation (x, height, width) oÃ¹ x dÃ©pend du nombre dâ€™a-
gents.
from lle import LLE, ObservationType
env = LLE.level(6, ObservationType.LAYERED)
print(env.observation_shape) # (12, 12, 13)
Cependant, par dÃ©faut (et pour pouvoir Ãªtre utilisÃ©es dans des rÃ©seaux de neurones linÃ©aires), LLE utilise
le type dâ€™observation ObservationType.FLATTENED, qui encodent exactement les mÃªmes informations
que ObservationType.LAYERED, mais applaties sur une seule dimension.
from lle import LLE, ObservationType
env = LLE.level(6, ObservationType.FLATTENED) # On peut ommettre le second paramÃ¨tre
print(env.observation_shape) # Affiche (1872,), cÃ d 12 * 12 * 13
Notez que chaque observation possÃ¨de aussi des informations sur lâ€™Ã©tat du World dans
observation.state, Ã  la diffÃ©rence que le WorldState auquel vous Ãªtes habituÃ©s a lui aussi Ã©tÃ© trans-
formÃ© en tableau numpy pour Ãªtre utilisable dans le contexte du deep MARL.
7----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2023-2024 projet RL for chatGPT.txt -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\src\almost_equal.py -----def almost_equal(a, b):
    return abs(a - b) < 1e-6
----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\src\almost_equal.py -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\src\analysis.py -----from dataclasses import (
    dataclass,
)  
@dataclass
class Parameters:
    """Parameters for the MDP and the algorithm
    ModifiedRewardWorld.reward values"""
    reward_live: float
    """Reward for living at each time step"""
    gamma: float
    """Discount factor"""
    noise: float
    """Probability of taking a random action instead of the chosen one"""

def prefer_close_exit_following_the_cliff() -> Parameters:
    # A strategy focusing on reaching the closest exit quickly,
    # even if it means taking risks.
    return Parameters(
        reward_live=-1,  # negative to encourage speed,
        gamma=0.1,  # enough to value exit but not too much for preffering the close exit to the far one
        noise=0,  # low to avoid random actions and exploration
    )

def prefer_close_exit_avoiding_the_cliff() -> Parameters:
    # A cautious strategy aiming for the nearest exit while avoiding risks.
    return Parameters(
        reward_live=-1, 
        gamma=0.1, 
        noise=0.5  # noise is higher to encourage exploration
    )

def prefer_far_exit_following_the_cliff() -> Parameters:
    # A strategy that targets a distant exit but might involve risk-taking.
    return Parameters(
        reward_live=0,  # no hurry
        gamma=0.8,  # high for focusing on distant rewards,
        noise=0.2,  # moderate for some randomness in path selection
    )

def prefer_far_exit_avoiding_the_cliff() -> Parameters:
    # A strategy preferring a distant exit with an emphasis on safety and planning.
    # Less negative reward_live for longer routes,
    # high gamma for future-oriented planning, and
    # low noise for consistent decision-making.
    return Parameters(reward_live=1, 
                      gamma=0.8, 
                      noise=0.9
                      )

def never_end_the_game() -> Parameters:
    # A unique strategy to avoid reaching terminal states and keep the game ongoing.
    return Parameters(
        reward_live=11,  # reward_live bigger than biggest reward to encourage continual play
        gamma=0,
        noise=0,
    )
----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\src\analysis.py -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\src\ApproximateQLearning.py -----# -Min 20 features pour 4 agents
----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\src\ApproximateQLearning.py -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\src\mdp.py -----from typing import TypeVar, Generic
from abc import abstractmethod, ABC


A = TypeVar("A")
S = TypeVar("S")


class MDP(ABC, Generic[S, A]):
    """Adversarial Markov Decision Process"""

    # gems_quantity = S.
    @abstractmethod
    def is_final(self, state: S) -> bool:
        """Returns true 
        if the given state is final (i.e. the game is over)."""


    @abstractmethod
    def available_actions(self, state: S) -> list[A]:
        """Returns the list of available actions for the current agent from the given state."""

    @abstractmethod
    def transitions(self, state: S, action: A) -> list[tuple[S, float]]:
        """Returns the list of next states with the probability of reaching it by performing the given action."""

    @abstractmethod
    def states(self) -> list[S]:
        """Returns the list of all states."""

    @abstractmethod
    def reward(self, state: S, action: A, new_state) -> float:
        """Reward function"""
        
----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\src\mdp.py -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\src\qagent.py -----# qlearning inheriting agent

from itertools import product
import numpy as np
import random
import sys
import os
import time
from rlenv import Observation, RLEnv
from typing import Dict, Tuple, List, Iterable, Generic, Optional, Callable, Set
from mdp import MDP, S, A
from auto_indent import AutoIndent
from world_mdp import WorldMDP
import utils
from lle import LLE, Action, Agent, AgentId, Position, WorldState
from rlenv.wrappers import TimeLimit

# import Action class :


sys.stdout = AutoIndent(sys.stdout)


# class QAgent(QLearning):
class QAgent:
    def __init__(
        self,
        # env: RLEnv,
        mdp: MDP[S, A],
        learning_rate: float = 0.1,
        discount_factor: float = 0.9,
        epsilon: float = 0.1,
        seed: int = None,
        id: AgentId = None,
    ):
        # Initialize the MDP
        self.mdp = mdp
        # Initialize parameters
        self.learning_rate = learning_rate
        self.discount_factor = discount_factor
        self.epsilon = epsilon
        self.id = id
        print("self.id:", self.id)

        # Initialize the environment
        # self.env = env

        # Initialize Q-table as a dictionary
        # Pour favoriser lâ€™exploration, initialisez vos ğ‘„(ğ‘ , ğ‘) Ã  1 et non Ã  0
        self.q_table = {
            state: {action.value: 1 for action in Action.ALL} for state in mdp.states()
        }  # dict of dicts

        # Initialize a random number generator
        self.rng = np.random.default_rng(seed)  # Random number generator instance

    def hash(self, state: WorldState, action: Action) -> Tuple[WorldState, Action]:
        return (state, action.value)

    def observe(self, observation: Observation):
        """Observe the given observation"""
        # Lorsque vous rÃ©cupÃ©rez une observation, vous pouvez accÃ©der Ã  son contenu avec observation.data
        # qui contient un tableau numpy dont la forme est (n_agents, ...)

        observation_data = observation.data
        print("observation_data:", observation_data)

    def get_state_observation(
        self,
        state: WorldState,
    ) -> Observation:
        """Get the observation for the given state"""
        return Observation(state)

    def get_position(self, matrix: np.ndarray) -> np.ndarray:
        """Get the position of the agent"""
        # return np.where(matrix == 1)

        print("matrix:", matrix)
        print("np.nonzero(matrix):", np.nonzero(matrix))
        print("np.transpose(np.nonzero(matrix)):", np.transpose(np.nonzero(matrix)))
        return np.transpose(np.nonzero(matrix))

    def get_valid_actions(
        self,
        observation: Observation,
    ) -> List[Action]:
        """Get the list of valid actions for the given observation"""
        observation_data = observation.data
        print("observation_data:", observation_data)
        agent_position = self.get_position(observation_data[0][0])

        return world.available_actions()

    def agent_position_after_action(
        self, agent_pos: Position, action: Action
    ) -> Position:
        """The position of an agent after applying the given action."""
        try:
            print("agent_pos", agent_pos)
            print("action", action)
            agent_pos_after_action = agent_pos + action.delta
            print("agent_pos_after_action", agent_pos_after_action)
        except ValueError:
            raise ValueError("Invalid action")
        return agent_pos_after_action

    def are_valid_joint_actions(
        self, state: WorldState, joint_actions: Tuple[Action, ...]
    ) -> bool:
        """Whether the given joint actions are valid.
        an action is valid if it is available for an agent
        and if it does not lead the agent to be on the same position as another agent"""
        # print("are_valid_joint_actions()")
        # print("state", state)
        # print("joint_actions", joint_actions)
        # print("state.agents_positions", state.agents_positions)
        # # calculate agent positions after applying the joint action
        agents_positions_after_joint_actions = []
        for i, agent_pos in enumerate(state.agents_positions):
            agent_pos_after_action = self.agent_position_after_action(
                agent_pos, joint_actions[i]
            )
            agents_positions_after_joint_actions.append(agent_pos_after_action)
        return self.no_duplicate_in(agents_positions_after_joint_actions)

    def get_valid_joint_actions(
        self, state: WorldState, available_actions: Tuple[Tuple[Action, ...], ...]
    ) -> Iterable[Tuple[Action, ...]]:
        """Yield all possible joint actions that can be taken from the given state.
        Hint: you can use `self.world.available_actions()` to get the available actions for each agent.
        """
        # print("available_actions", available_actions)
        # cartesian product of the agents' actions
        for joint_actions in product(*available_actions):
            # print("joint_actions", joint_actions)

            if self.are_valid_joint_actions(state, joint_actions):
                yield joint_actions

    def get_agents_positions(
        self,
        #  state: np.ndarray
        observation: Observation,
    ) -> List[Position]:
        """Get the positions of all agents in the given observation.state"""
        observation_data = observation.data
        print("observation_data:", observation_data)
        state = observation.state
        print("state:", state)

        agents_positions = []
        for agent in self.mdp.world.agents:
            agent_position = self.get_position(state[agent.id])
            agents_positions.append(agent_position)

        return agents_positions

    def get_ones_indexes(
        self,
        array: np.ndarray,
    ) -> List[int]:
        """Get the indexes of all ones in the given array"""
        ones_indexes = []
        for i, value in enumerate(array):
            if value == 1:
                ones_indexes.append(i)
     
        print("ones_indexes:", ones_indexes)
        return ones_indexes

    def choose_action(
        self,
        observation: Observation,  # from instructions
    ):
        """Choose an action using the epsilon-greedy policy"""
        state = observation.state
        # #state type:
        # print("type(state):", type(state))
        # print("state:", state)
        # world = self.mdp.world
        # print("world:", world)

        # # valid_actions = self.get_valid_joint_actions(
        # # world_available_actions = self.mdp.world.available_actions()
        # world_available_actions = self.mdp.available_actions(observation.state)
        # print("world_available_actions:", world_available_actions)
        # valid_actions = self.mdp.world.available_actions()[self.id]
        # print("valid_actions:", valid_actions)
        # self.get_position(observation.data[0][0])
        observation_available_actions = observation.available_actions
        print("observation_available_actions:", observation_available_actions)
        current_agent_available_actions = observation_available_actions[self.id]

        valid_actions = self.get_ones_indexes(current_agent_available_actions)
        print("valid_actions:", valid_actions)
        if self.rng.uniform(0, 1) < self.epsilon:
            # Exploration: Random Action

            action = self.rng.choice(valid_actions)
        else:
            # Exploitation: Best known action
            state_actions = self.q_table.get(observation, {})
            if state_actions:
                action = max(state_actions, key=state_actions.get)
            else:
                action = self.rng.choice(valid_actions)
        return action

    def update(self, state, action, reward, next_state):
        """Update the Q-table using the Bellman equation"""
        # Get the current Q value
        current_q = self.q_table.get(state, {}).get(action, 1)  # 1 = default value
        # Find the max Q value for the actions in the next state
        next_state_actions = self.q_table.get(next_state, {})
        max_next_q = max(next_state_actions.values(), default=0)
        # Update the Q value using the Bellman equation
        new_q = current_q + self.learning_rate * (
            reward + self.discount_factor * max_next_q - current_q
        )
        # Update the Q-table
        self.q_table.setdefault(state, {})[action] = new_q


if __name__ == "__main__":
    # Create the environment
    env = LLE.level(1)
    # Create the MDP
    mdp = WorldMDP(env.world)
    print(mdp.world)

    # Create the agents
    agent = QAgent(mdp, AgentId(1))

    # # Train the agent
    # agent.train(env, episodes_quantity=100)
    # # Test the agent
    # agent.test(env, episodes_quantity=100)
    # # Save the agent
    # agent.save(
    #     "qlearning_agent.pkl"
    # )  # pkl = pickle = sÃ©rialisation de donnÃ©es en Python
----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\src\qagent.py -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\src\qlearning.py -----from lle import Agent, ObservationType
from rlenv import RLEnv, Observation
import numpy as np
import numpy.typing as npt
import sys
from mdp import MDP, S, A
from lle import LLE, Action, Agent, AgentId, WorldState
from rlenv.wrappers import TimeLimit
from qagent import QAgent
from auto_indent import AutoIndent
from world_mdp import WorldMDP

sys.stdout = AutoIndent(sys.stdout)


class QLearning:
    """Tabular QLearning"""

    def __init__(
        self,
        mdp: MDP[S, A],
        learning_rate: float,
        discount_factor: float,
        epsilon: float,
        seed: int = None,
    ):
        # Initialize parameters
        self.mdp = mdp
        self.learning_rate = learning_rate
        self.discount_factor = discount_factor
        self.epsilon = epsilon
        # Create the agents
        self.agents = [
            QAgent(mdp, learning_rate, discount_factor, epsilon, id=AgentId(i))
            # for i in range(env.world.n_agents)
            for i in range(mdp.world.n_agents)
        ]

    # # Initialize a random number generator
    # self.rng = np.random.default_rng(seed)  # Random number generator instance

    def train(self, agents, episodes_quantity: int):
        """Train the agent for the given number of episodes"""
        # from instructions:
        env = TimeLimit(
            LLE.level(1, ObservationType.LAYERED), 80
        )  # Maximum 80 time steps         # from instructions

        observation = env.reset()  # from instructions
        observation_data = observation.data
        observation_hash = self.numpy_table_hash(observation_data)
        print("observation_data:", observation_data)
        print("observation_hash:", observation_hash)

        done = truncated = False  # from instructions
        score = 0  # from instructions
        while not (done or truncated):  # from instructions
            actions = [  # from instructions
                a.choose_action(observation) for a in agents  # from instructions
            ]  # from instructions
            print("actions:", actions)
            # get action[0] type:
            print("type(actions[0]):", type(actions[0]))
            # north = Action(0)
            # print("north:", north)
            # south = Action(1)
            # print("south:", south)

            next_observation, reward, done, truncated, info = env.step(
                actions
            )  # from instructions
            print("observation:", next_observation)
            print("reward:", reward)
            print("done:", done)
            print("truncated:", truncated)
            print("info:", info)

            for a in agents:  # from instructions
                print("a:", a)
                print("a.id:", a.id)
                a.update(  # from instructions
                    observation, actions[a.id], reward, next_observation
                )
            score += reward  # from instructions
            print("score:", score)
            observation = next_observation

    def test(self, env: RLEnv, trained_agents, episodes_quantity: int):
        """Test the agent for the given number of episodes"""
        for episode in range(episodes_quantity):
            # Reset the environment
            observation = env.reset()
            done = False
            actions_taken = []
            while not done:
                actions = [a.choose_action(observation) for a in trained_agents]
                actions_taken.append(actions)
                next_observation, _, done = env.step(actions)
                observation = next_observation
            # Print the result of the episode
            if done:
                print(f"Episode {episode + 1} finished. Actions taken: {actions_taken}")
            else:
                print(f"Episode {episode + 1} did not finish.")

    def show(self):
        """Show the Q-table"""
        print(self.q_table)

    def __str__(self):
        """Return the Q-table as a string"""
        return str(self.q_table)

    def __repr__(self):
        """Return the Q-table as a string"""
        return str(self.q_table)

    def numpy_table_hash(self, numpy_table: npt.ArrayLike) -> int:
        """Return the hash of the Q-table as a numpy array"""
        # return np.array(list(self.q_table.items()), dtype=object).hash

        # using  hash(array.tobytes())
        return hash(np.array(list(numpy_table), dtype=object).tobytes())


if __name__ == "__main__":
    # Create the environment
    env = LLE.level(1, ObservationType.LAYERED)
    mdp = WorldMDP(env.world)
    print("mdp.world :", mdp.world)

    # Train the agent
    qlearning = QLearning(mdp, 0.1, 0.9, 0.1)
    trained_agents = qlearning.train(qlearning.agents, episodes_quantity=100)
    # Test the agents
    qlearning.test(env, trained_agents, episodes_quantity=1)
    # Save the agent
----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\src\qlearning.py -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\src\qlearning_training.py -----# EntraÃ®nez votre algorithme sur les niveaux 1, 3 et 6 de LLE. Dans votre rapport, crÃ©ez un graphique
# pour chacun de ces trois niveaux qui montre le score (câ€™est-Ã -dire la somme des rewards par Ã©pisode)
# au cours de lâ€™entrainement. 


from rlenv import RLEnv


def train_qlearning_agent(env: RLEnv, episodes_quantity: int):
    """Train the agent for the given number of episodes"""
    # Create the agent
    agent = QLearningAgent(env)
    # Train the agent
    agent.train(env, episodes_quantity)
    # Test the agent
    agent.test(env, episodes_quantity)
    # Return the agent
    return agent

----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\src\qlearning_training.py -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\src\value_iteration.py -----import copy
import sys

from lle import World, WorldState
from almost_equal import almost_equal
from graph_mdp import GraphMDP
from mdp import MDP, S, A
from typing import Generic

from auto_indent import AutoIndent
from world_mdp import WorldMDP
import utils

sys.stdout = AutoIndent(sys.stdout)


class ValueIteration(Generic[S, A]):
    def __init__(self, mdp: MDP[S, A], gamma: float):  # discount factor
        # senf.values est nÃ©cessaire pour fonctionner avec utils.show_values
        self.mdp = mdp
        self.gamma = gamma
        # Initialize all states as dictionary keys
        # with a default value of 0.0
        self.values = {state: 0.0 for state in mdp.states()}
        # utils.show_values(self.values)
        # utils.

    def value(self, state: S) -> float:
        """Returns the value of the given state."""
        # return self.values[state]
        # return self.values.get(state, 0.0)  # Default value if state not found
        if state not in self.values:
            return 0.0
        return self.values.get(state)

    def policy(self, state: S) -> A:
        """Returns the action
        that maximizes the Q-value of the given state."""
        available_actions = self.mdp.available_actions(state)
        if not available_actions:
            print("No available actions for state", state)
            return None  # Or some default action if appropriate
        return max(available_actions, key=lambda action: self.qvalue(state, action))

    def qvalue(self, state: S, action: A) -> float:
        """
        Returns the Q-value
        of the given state-action pair
        based on the state values.
        from Bellman equation:
        Q(s,a) = Sum(P(s,a,s') * (R(s,a,s') + gamma * V(s')))
        """
        qvalue = 0.0
        next_states_and_probs = self.mdp.transitions(state, action)
        # print("next_states_and_probs: \n", next_states_and_probs, "\n")
        for next_state, prob in next_states_and_probs:
            reward = self.mdp.reward(state, action, next_state)
            # print("P(", state, action, next_state, "):", prob)
            # print("R(", state, action, next_state, "):", reward)
            next_state_value = self.value(next_state)
            # print("V(", next_state, "):", next_state_value)
            qvalue += prob * (reward + self.gamma * next_state_value)
        # print("Q-value of", state, action, ":", qvalue, "\n")
        return qvalue

    def _compute_value_from_qvalues(self, state: S) -> float:
        """
        Returns the value of the given state based on the Q-values.
        from Bellman equation:
        V(s) = max_a Sum(P(s,a,s') * (R(s,a,s') + gamma * V(s')))

        This is a private method,
        meant to be used by the value_iteration method.
        """
        # print("Computing value of", state)
        value = max(
            self.qvalue(state, action) for action in self.mdp.available_actions(state)
        )
        if value is None:
            return 0.0
        return value

    def get_values_at_position(self, i: int, j: int) -> list[float]:
        """Returns the values of the states at the given position."""
        # world_gems_quantity = self.mdp.
        states_at_position = [
            state for state in self.mdp.states() if state.agents_positions[0] == (i, j)
        ]
        values_at_position = [self.value(state) for state in states_at_position]
        increasing_values = sorted(values_at_position)

        return increasing_values

    def print_values_table(self, n: int = 0):
        """In a map's representation table,
        each tile contains the possible values at that position."""
        if not isinstance(self.mdp, WorldMDP):
            # print("Cannot print values table for non-world MDP")
            return None
        print("Iteration", n, "Values table: ")
        max_len = 0
        for i in range(self.mdp.world.height):
            for j in range(self.mdp.world.width):
                values = self.get_values_at_position(i, j)
                # Convert the list of values to a string and find the maximum length
                values_str = str(values)
                max_len = max(max_len, len(values_str))

        for i in range(self.mdp.world.height):
            for j in range(self.mdp.world.width):
                values = self.get_values_at_position(i, j)
                # Format each string to have the same width
                print(f"{str(values):<{max_len}}", end=" ")
            print()

    def print_iteration_values(self, iteration: int):
        """Prints the states and their values."""
        print("Iteration", iteration, "States and their values:")
        for state in self.mdp.states():
            print(state, self.value(state))

    def value_iteration(self, n: int):  # number of iterations
        """Performs value iteration for the given number of iterations."""
        for _ in range(n):
            # print("Iteration", _)
            new_values = copy.deepcopy(self.values)
            for state in self.mdp.states():  # All states generator (not a list)
                # print("State", state)
                if self.mdp.is_final(state):
                    # print("Final state", state)
                    new_values[state] = 0.0
                else:
                    new_values[state] = self._compute_value_from_qvalues(state)
            self.values = new_values
            self.print_values_table(_)
        # self.print_iteration_values(n)


if __name__ == "__main__":
    # graph
    # b - +1 - a - -1 - c
    # graph_file_name = "tests/graphs/graph1.json"
    # mdp = GraphMDP.from_json(graph_file_name)
    # gamma = 0.9
    # algo = ValueIteration(mdp, gamma)
    # # algo.value_iteration(10)
    # algo.value_iteration(100)
    # assert almost_equal(algo.qvalue("a", "left"), 0.6)  # no change from iteration 0
    # assert almost_equal(
    #     algo.qvalue("a", "right"), 0.90909090909
    # )  # more than iteration 0 & 1
    mdp = WorldMDP(
        World(
            """
    .  . . X
    .  @ . V
    S0 . . ."""
        )
    )
    algo = ValueIteration(mdp, 0.9)
    algo.value_iteration(100)
    expected = [
        [1.62, 1.80, 2.0, 0.0],
        [1.458, 0.0, 1.80, 0.0],
        [1.3122, 1.458, 1.62, 1.458],
    ]
    for i in range(mdp.world.height):
        for j in range(mdp.world.width):
            print("State", (i, j))
            state = WorldState([(i, j)], [])
            print("Value:", algo.value(state))
            assert almost_equal(algo.value(state), expected[i][j])
----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\src\value_iteration.py -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\src\world_mdp.py -----from itertools import product
from typing import Iterable
from lle import World, WorldState, Action
from mdp import MDP


class WorldMDP(MDP[WorldState, list[Action]]):
    def __init__(self, world: World):
        self.world = world

    def available_actions(self, state: WorldState):
        self.world.set_state(state)
        available = self.world.available_actions()
        return list(product(*available))

    def transitions(
        self, state: WorldState, action: list[Action]
    ) -> list[tuple[WorldState, float]]:
        self.world.set_state(state)
        self.world.step(action)
        return [(self.world.get_state(), 1.0)]

    def is_final(self, state: WorldState) -> bool:
        self.world.set_state(state)
        return self.world.done

    def reward(
        self,
        state: WorldState,
        action: list[Action],
        new_state: WorldState,
    ) -> float:
        # Step the world and check if the new state is the same as the given one
        # If if is not the same, then test all the other available actions.
        self.world.set_state(state)
        actions = [action] + [[a] for a in self.world.available_actions()[0]]
        for a in actions:
            r = self.world.step(a)
            if self.world.get_state() == new_state:
                return r
            self.world.set_state(state)
        raise ValueError("The new state is not reachable from the given state")

    def states(self) -> Iterable[WorldState]:
        all_positions = set(product(range(self.world.height), range(self.world.width)))
        all_positions = all_positions.difference(set(self.world.wall_pos))
        agents_positions = list(product(all_positions, repeat=self.world.n_agents))
        collection_status = list(product([True, False], repeat=self.world.n_gems))

        for pos, collected in product(agents_positions, collection_status):
            s = WorldState(list(pos), list(collected))
            yield s
----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\src\world_mdp.py -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\tests\modified_reward_world.py -----from .world_mdp import WorldMDP
from lle import Action, World, WorldState


class ModifiedRewardWorld(WorldMDP):
    def __init__(self, 
                 reward_live: float # Reward for living at each time step
                 ):
        super().__init__(World.from_file("tests/graphs/cliff"))
        self.world.reset() # Reset the world to its initial state
        self.reward_live = reward_live

    def reward(
        self, 
        state: WorldState, 
        action: list[Action], 
        new_state: WorldState
    ) -> float:
        reward = super().reward(state, action, new_state)
        # The agent has died
        if reward < 0:
            return -10.0
        # The agent has collected a gem
        # The agent has reached the exit
        if reward > 0:
            # Close exit
            if new_state.agents_positions[0] == (2, 2):
                return 1.0
            # Far exit
            return 10.0
        # The agent just lives (reward should be 0)
        
        return self.reward_live
----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\tests\modified_reward_world.py -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\tests\random_wrapper.py -----from src.mdp import MDP, S, A


class RandomWrapper(MDP[S, A]):
    """Wrapper around an MDP such that the action taken can be random with probability p.
    It only changes the `transitions` method. The other methods are unchanged."""

    def __init__(self, mdp: MDP[S, A], p: float):
        super().__init__()
        self.mdp = mdp
        self.p = p
        """The probability of the agent to perform a random action"""

    def is_final(self, state: S) -> bool:
        return self.mdp.is_final(state)

    def available_actions(self, state: S) -> list[A]:
        return self.mdp.available_actions(state)

    def transitions(self, state: S, action: A) -> list[tuple[S, float]]:
        if self.p == 0:
            return self.mdp.transitions(state, action)
        # Create the dictionary of "normal" destinations
        destination_probs = dict(self.mdp.transitions(state, action))
        # The probabilities must be multiplied by (1 - p) because of the wrapper
        for s, prob in destination_probs.items():
            destination_probs[s] = prob * (1 - self.p)
        # Add the random destinations
        available_actions = list(self.available_actions(state))
        n_actions = len(available_actions)
        for action in available_actions:
            for s, prob in self.mdp.transitions(state, action):
                # The probability of taking this action must be multiplied by (p / n_actions)
                prob = (prob * self.p) / n_actions
                destination_probs[s] = destination_probs.get(s, 0.0) + prob
        return list(destination_probs.items())

    def states(self) -> list[S]:
        return self.mdp.states()

    def reward(self, state: S, action: A, new_state) -> float:
        return self.mdp.reward(state, action, new_state)
----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\tests\random_wrapper.py -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\tests\test_analysis.py -----from src.analysis import (
    prefer_close_exit_avoiding_the_cliff,
    prefer_close_exit_following_the_cliff,
    prefer_far_exit_avoiding_the_cliff,
    prefer_far_exit_following_the_cliff,
    never_end_the_game,
    Parameters,
)
from src.mdp import MDP
from src.value_iteration import ValueIteration
from .random_wrapper import RandomWrapper
from .modified_reward_world import ModifiedRewardWorld


def setup_mdp_and_get_path(param: Parameters):
    mdp = ModifiedRewardWorld(param.reward_live)
    start_state = mdp.world.get_state()
    noisy_mdp = RandomWrapper(mdp, param.noise) # Add noise to the MDP
    algo = ValueIteration(noisy_mdp, param.gamma) # Create the algorithm 
    algo.value_iteration(100)
    return apply_policy(start_state, mdp, algo)


def apply_policy(state, 
                 mdp: MDP, 
                 algo: ValueIteration
                 ):
    """Apply the policy 
    until the end of the game 
    or a loop is detected. 
    Returns the path taken."""
    path = []
    while not mdp.is_final(state) and state not in path: # While the game is not over and the state is not in the path
        path.append(state)
        action = algo.policy(state)
        print(action)
        transitions = mdp.transitions(state, action) # Get the possible transitions
        assert len(transitions) == 1 # There is only one possible transition
        state, _ = transitions[0]
    path.append(state)
    print(path)
    return path


def test_close_exit_following_the_cliff():
    params = prefer_close_exit_following_the_cliff()
    path = setup_mdp_and_get_path(params)
    expected = [(3, 0), (3, 1), (3, 2), (2, 2)]
    assert len(path) == len(expected)
    for exp, state in zip(expected, path):
        assert state.agents_positions[0] == exp


def test_close_exit_avoiding_the_cliff():
    params = prefer_close_exit_avoiding_the_cliff()
    path = setup_mdp_and_get_path(params)
    expected = [(3, 0), (2, 0), (1, 0), (0, 0), (0, 1), (0, 2), (1, 2), (2, 2)]
    assert len(path) == len(expected)
    for exp, state in zip(expected, path):
        assert state.agents_positions[0] == exp


def test_far_exit_following_the_cliff():
    params = prefer_far_exit_following_the_cliff()
    path = setup_mdp_and_get_path(params)
    expected = [(3, 0), (3, 1), (3, 2), (3, 3), (3, 4), (2, 4)]
    assert len(path) == len(expected)
    for exp, state in zip(expected, path):
        assert state.agents_positions[0] == exp


def test_far_exit_avoiding_the_cliff():
    params = prefer_far_exit_avoiding_the_cliff()
    path = setup_mdp_and_get_path(params)
    assert len(path) == 10
    # This is only the start of the path because the second half could vary
    expected = [(3, 0), (2, 0), (1, 0), (0, 0), (0, 1), (0, 2)]
    for exp, state in zip(expected, path):
        assert state.agents_positions[0] == exp


def test_never_end():
    params = never_end_the_game()
    path = setup_mdp_and_get_path(params)
    # Check that there is a loop in the path
    assert path[-1] in path[:-1] # The last state is in the path before the last state
----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\tests\test_analysis.py -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\tests\test_value_iteration.py -----from src.value_iteration import ValueIteration
from lle import World, WorldState, Action, REWARD_AGENT_JUST_ARRIVED, REWARD_END_GAME
from tests.world_mdp import WorldMDP
from .graph_mdp import GraphMDP
from matplotlib import pyplot as plt


def almost_equal(a, b):
    return abs(a - b) < 1e-6


graph_file_name = "tests/graphs/graph1.json"

# 4-rl\tests\graphs\graph1.json
def test_value_0():
    mdp = GraphMDP.from_json(graph_file_name)
    algo = ValueIteration(mdp, 0.9)
    algo.value_iteration(0)
    for s in mdp.states():
        assert algo.value(s) == 0.0


def test_value_end_states():
    """
    tests that the value of end states is 0 after 100 iterations
    """
    mdp = GraphMDP.from_json(graph_file_name)
    algo = ValueIteration(mdp, 0.9)
    algo.value_iteration(100)
    for s in mdp.states():
        if mdp.is_final(s):
            assert algo.value(s) == 0.0


def test_qvalues_0():
    """
    test qvalues for the graph mdp after 0 iterations
    """
    mdp = GraphMDP.from_json(graph_file_name)
    algo = ValueIteration(mdp, 0.9)
    algo.value_iteration(0)
    assert almost_equal(algo.qvalue("a", "left"), 0.6)
    assert algo.qvalue("a", "right") == 0.5


def test_max_action_0():
    """
    test policy max_action for the graph mdp after 0 iterations
    """
    mdp = GraphMDP.from_json(graph_file_name)
    algo = ValueIteration(mdp, 0.9)
    algo.value_iteration(0)
    assert algo.policy("a") == "left"


def test_value_1():
    """
    test value of states after 1 iteration
    """
    mdp = GraphMDP.from_json(graph_file_name)
    gamma = 0.9
    algo = ValueIteration(mdp, gamma)
    algo.value_iteration(1)
    assert almost_equal(algo.qvalue("a", "left"), 0.6) # no change from iteration 0
    expected = 0.5 + 0.5 * gamma * 0.6 # 0.77 # = 0.5 + 0.5 * 0.9 * 0.6
    assert almost_equal(algo.qvalue("a", "right"), expected) # more than iteration 0


def test_value_100():
    """
    test value of states after 100 iterations
    """
    mdp = GraphMDP.from_json(graph_file_name)
    gamma = 0.9
    algo = ValueIteration(mdp, gamma)
    algo.value_iteration(100)
    assert almost_equal(algo.qvalue("a", "left"), 0.6) # no change from iteration 0
    assert almost_equal(algo.qvalue("a", "right"), 0.90909090909) # more than iteration 0 & 1


def test_value_world_0():
    """
    test value of world states after 0 iterations
    """
    mdp = WorldMDP(
        World(
            """
    .  . . X
    .  @ . V
    S0 . . ."""
        )
    )
    algo = ValueIteration(mdp, 0.9)
    algo.value_iteration(0)
    for s in mdp.states():
        assert algo.value(s) == 0.0


def test_qvalues_world():
    """
    test qvalues for the world mdp after 0 iterations
    """
    mdp = WorldMDP(
        World(
            """
    .  . . X
    .  @ . V
    S0 . . ."""
        )
    )
    algo = ValueIteration(mdp, 0.9)
    algo.value_iteration(0)
    state = WorldState([(0, 2)], [])
    assert (
        algo.qvalue(state, [Action.EAST]) == REWARD_END_GAME + REWARD_AGENT_JUST_ARRIVED
    )


def test_value_world_100():
    """
    test value of world states after 100 iterations
    """
    mdp = WorldMDP(
        World(
            """
    .  . . X
    .  @ . V
    S0 . . ."""
        )
    )
    algo = ValueIteration(mdp, 0.9)
    algo.value_iteration(100)
    expected = [
        [1.62, 1.80, 2.0, 0.0],
        [1.458, 0.0, 1.80, 0.0],
        [1.3122, 1.458, 1.62, 1.458],
    ]
    for i in range(mdp.world.height):
        for j in range(mdp.world.width):
            state = WorldState([(i, j)], [])
            assert almost_equal(algo.value(state), expected[i][j])


if __name__ == "__main__":
    print("hello world")
    test_value_0()
    test_value_end_states()
    test_qvalues_0()
    test_max_action_0()
    test_value_1()
    test_value_100()
    test_value_world_0()
    test_qvalues_world()
    test_value_world_100()
    print("ok")
----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\tests\test_value_iteration.py -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\tests\world_mdp.py -----from itertools import product
from typing import Iterable
from lle import World, WorldState, Action
from src.mdp import MDP


class WorldMDP(MDP[WorldState, list[Action]]):
    def __init__(self, world: World):
        self.world = world

    def available_actions(self, state: WorldState):
        self.world.set_state(state)
        available = self.world.available_actions()
        return list(product(*available))

    def transitions(
        self, 
        state: WorldState, 
        action: list[Action]
    ) -> list[tuple[WorldState, float]]:
        self.world.set_state(state)
        self.world.step(action)
        return [(self.world.get_state(), 1.0)]

    def is_final(self, state: WorldState) -> bool:
        self.world.set_state(state)
        return self.world.done

    def reward(
        self,
        state: WorldState,
        action: list[Action],
        new_state: WorldState,
    ) -> float:
        # Step the world and check if the new state is the same as the given one
        # If if is not the same, then test all the other available actions.
        self.world.set_state(state)
        actions = [action] + [[a] for a in self.world.available_actions()[0]]
        for a in actions:
            r = self.world.step(a)
            if self.world.get_state() == new_state:
                return r
            self.world.set_state(state) # Reset the world to the given state
        raise ValueError("The new state is not reachable from the given state")

    def states(self) -> Iterable[WorldState]:
        all_positions = set(product(range(self.world.height), range(self.world.width)))
        all_positions = all_positions.difference(set(self.world.wall_pos))
        agents_positions = list(product(all_positions, repeat=self.world.n_agents))
        collection_status = list(product([True, False], repeat=self.world.n_gems))

        for pos, collected in product(agents_positions, collection_status):
            s = WorldState(list(pos), list(collected))
            yield s
----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\tests\world_mdp.py -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\tests\graphs\cliff -----.  . . . .
.  @ . . .
.  @ X @ X
S0 . . . .
V  V V V V----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\tests\graphs\cliff -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\tests\graphs\cliff.json -----{
    "states": [
        "A1",
        "A2",
        "A3",
        "A4",
        "A5",
        "B1",
        "B2",
        "B3",
        "B4",
        "B5",
        "C1",
        "C2",
        "C3",
        "C4",
        "C5",
        "D1",
        "D2",
        "D3",
        "D4",
        "D5",
        "E1",
        "E2",
        "E3",
        "E4",
        "E5"
    ],
    "start_state": "A2",
    "end_states": [
        "A1",
        "B1",
        "C1",
        "D1",
        "E1"
    ]
}----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\tests\graphs\cliff.json -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\tests\graphs\graph1.json -----{
    "states": [
        "a",
        "b",
        "c"
    ],
    "start_state": "a",
    "end_states": [
        "b",
        "c"
    ],
    "actions": [
        "left",
        "right"
    ],
    "transitions": {
        "a": {
            "left": [
                {
                    "to": "b",
                    "probability": 0.8,
                    "reward": 1
                },
                {
                    "to": "c",
                    "probability": 0.2,
                    "reward": -1
                }
            ],
            "right": [
                {
                    "to": "a",
                    "probability": 0.5,
                    "reward": 0
                },
                {
                    "to": "b",
                    "probability": 0.5,
                    "reward": 1
                }
            ]
        }
    }
}----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\tests\graphs\graph1.json -----

