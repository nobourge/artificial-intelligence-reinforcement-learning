----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\extract_pdf_txt.py -----import pdfplumber


def extract_text_from_pdf(pdf_path):
    text = ""
    with pdfplumber.open(pdf_path) as pdf:
        for page in pdf.pages:
            text += page.extract_text() + "\n"
    return text
----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\extract_pdf_txt.py -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\pyproject.toml -----[tool.poetry]
name = "4-rl"
version = "0.1.0"
description = ""
authors = ["Yannick Molinghen <yannick.molinghen@ulb.be>"]
readme = "README.md"

[tool.poetry.dependencies]
python = ">=3.10, <3.13"
matplotlib = "3.6"
seaborn = "^0.12.2"
pytest = "^7.4.2"
gymnasium = "^0.29.1"
laser-learning-environment = "0.1.8"
opencv-python = "^4.8.1.78"
rlenv = { git = "https://github.com/yamoling/rlenv", tag = "v0.4.4" }
numpy = "^1.21.2"
pandas = "^1.3.3"
scipy = "^1.7.1"
scikit-learn = "^1.0"
loguru = "^0.7.2"
watchdog = "^3.0.0"
pdfplumber = "^0.10.3"


[tool.poetry.group.dev.dependencies]
pytest = "^7.4.2"


[build-system]
requires = ["poetry-core"]
build-backend = "poetry.core.masonry.api"

[tool.pytest.ini_options]
pythonpath = ["src"]


[tool.ruff]
# Check https://beta.ruff.rs/docs/rules/ for all rules
fixable = ["ALL"]
line-length = 140
----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\pyproject.toml -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\__init__.py ---------- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\__init__.py -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2023-2024 projet RL-1.pdf -----FACULTÉ DES SCIENCES
DÉPARTEMENT D’INFORMATIQUE
INFO-F-311: Intelligence Artificielle
Projet 4: Reinforcement Learning
Yannick Molinghen Pascal Tribel Tom Lenaerts
1 Préambule
Dans ce projet, vous allez implémenter des algorithmes d’apprentissage par renforcement (reinforce-
ment learning). On vous fournit des fichiers de base pour le projet que vous pouvez les télécharger sur
l’université virtuelle.
1.1 Poetry
Le projet nécessite Python ≥ 3.10 et utilise le système de build Poetry (https://python-poetry.org/docs/
#installation).
Après avoir installé Poetry, ouvrez un terminal dans le répertoire du projet et lancez les commandes:
poetry shell
poetry install
Ces commandes vont automatiquement créer un environnement virtuel puis installer les dépendances
du projet. Vous pouvez aussi utiliser pip pour installer les dépendences du projet en regardant les
dépendences à installer dans le fichier pyproject.toml.
1.2 Tests automatiques
Vous pouvez tester vos fonctionnalités avec la commande pytest. Pour lancer uniquement les tests
d’un fichier en particulier, vous pouvez le donner en paramètre. Il y a un fichier de test pour chacune
des tâches que vous devez remplir.
Note: Il est très difficile de tester des algorithmes de RL. Par conséquent, il n’y a des tests que pour
les exercices de la Section 3 et de la Section 4.
2 Introduction
Intuitivement, un processus de décision markovien (Markov Decision Process, MDP) est composé d’é-
tats dans lesquels un agent effectue des actions qui ont une certaine probabilité d’atterrir dans un autre
état.
Après chaque transition, l’agent reçoit une récompense qui donne une indication sur la qualité de
l’action: plus la récompense est élevée, meilleure est l’action. Le but d’un agent est de maximiser la
somme des récompenses au cours d’un épisode (c’est-à-dire une partie).
Par conséquent, pour chaque état 𝑠, on peut calculer sa valeur 𝑉(𝑠) qui correspond à la meilleure
somme des récompenses possible à partir de 𝑠 grâce à l’équation de Bellman présentée dans l’E-
quation 1.
𝑉(𝑠) = max∑𝑃(𝑠,𝑎,𝑠′)[𝑅(𝑠,𝑎,𝑠′)+𝛾𝑉(𝑠′)] (1)
𝑎∈𝐴
1
FACULTÉ DES SCIENCES
DÉPARTEMENT D’INFORMATIQUE
3 Value iteration
L’algorithme de «value iteration» a pour but d’approximer itérativement la valeur 𝑉(𝑠) de chaque état
s à l’aide de l’Equation 2.
𝑉 = max∑𝑃(𝑠,𝑎,𝑠′)[𝑅(𝑠,𝑎,𝑠′)+𝛾𝑉 ]
𝑘+1 𝑘(𝑠′) (2)
𝑎
𝑠′
L’algorithme de value iteration prend en paramètre un entier 𝑘 qui détermine combien d’itération ef-
fectuer. Implémentez l’algorithme value iteration dans le fichier value_iteration.py.
Conseils:
• Quand vous implémentez l’algorithme, faites attention à vous baser sur 𝑉 pour calculer 𝑉
𝑘 𝑘+1
et à ne pas modifier 𝑉 durant l’itération.
𝑘
• N’oubliez pas que la valeur d’un état terminal est 0 par définition.
4 Trouvez les bons paramètres
Dans cet exercice, on vous demande de trouver les bons paramètres du MDP et de value iteration pour
induire un certain comportement à l’agent.
Le MDP considéré est celui présenté dans la Figure 1. Il s’agit d’un lle.World dont les rewards ont été
modifiées et qui comprend sept états terminaux:
• Cinq “ravins” en bas de la carte. L’agent meurt s’il s’y rend et reçoit une “récompense” de −10;
• Une sortie proche qui rapporte une récompense de 1;
• Une sortie lointaine qui rapporte une récompense de 10.
Figure 1: MDP pour lequel trouver les bons paramètres
2
FACULTÉ DES SCIENCES
DÉPARTEMENT D’INFORMATIQUE
Le but de cet exercice est d’induire les comportements suivants:
1. Préférer la sortie proche (+1) en longeant la falaise.
2. Préférer la sortie proche (+1) en évitant la falaise.
3. Préférer la sortie distante (+10) en longeant la falaise.
4. Préférer la sortie distante (+10) en évitant la falaise.
5. Eviter de terminer le jeu (l’épisode ne se termine jamais).
Pour ce faire, choisissez des valeurs adéquates de chaque comportement dans la fonction correspon-
dante pour reward_live (la récompense à chaque étape pour continuer le jeu), gamma (le discount fac-
tor) et noise (la probabilité de prendre une action aléatoire) dans le fichier analysis.py.
5 Q-learning
L’algorithme de Q-learning consiste à interagir avec l’environnement pour mettre à jour une fonction
d’évaluation 𝑄(𝑠,𝑎). Contrairement aux exercices précédents, on évalue ici la valeur d’une action dans
un état 𝑄(𝑠,𝑎) et pas la valeur de l’état 𝑉(𝑠).
Une fois que votre agent est entraîné, vous pouvez exploiter la stratégie apprise en prenant l’action
ayant la plus haute q-value max 𝑄(𝑠,𝑎) dans chaque état.
𝑎∈𝐴
Pour entraîner votre agent, il est nécessaire d’explorer l’environnement. Pour ce faire, vous devez im-
plémenter la stratégie d’exploration dite «𝜺-greedy» qui consiste à choisir une action aléatoire avec
une probabilité ε, et à prendre la meilleure action avec une probabilité 1−𝜀.
Note: Il n’y a quasiment pas de tests pour les exercices liés au q-learning car ceux-ci sont très diffi-
ciles à tester. On vous demande par contre d’analyser vos résultats dans le rapport (voir Section 6).
5.1 Q-learning tabulaire
Pour entraîner la fonction 𝑄(𝑠,𝑎), on emploie une méthode adaptée de l’équation de Bellman (Equa-
tion 1) illustrée dans l’Equation 3, où 𝛼 est le learning rate.
𝑄(𝑠,𝑎) ← (1−𝛼)𝑄(𝑠,𝑎)+𝛼[𝑅(𝑠,𝑎,𝑠′)+𝛾𝑉(𝑠′)] (3)
Implémentation
Implémentez l’algorithme de Q-learning dans le fichier qlearning.py. Faites en sorte que cette implé-
mentation utilise un dictionnaire pour stocker les qvalues de chaque état.
Conseils:
• Pour hasher des tableaux numpy, nous vous conseillons d’utiliser hash(array.tobytes()).
• Pour favoriser l’exploration, initialisez vos 𝑄(𝑠,𝑎) à 1 et non à 0.
Entraînement et rapport
Entraînez votre algorithme sur les niveaux 1, 3 et 6 de LLE. Dans votre rapport, créez un graphique
pour chacun de ces trois niveaux qui montre le score (c’est-à-dire la somme des rewards par épisode)
au cours de l’entrainement. Voir Section 6 pour plus d’informations sur le rapport.
3
FACULTÉ DES SCIENCES
DÉPARTEMENT D’INFORMATIQUE
Table 1: De gauche à droite: niveau 1, niveau 3 et niveau 6 de LLE
Exemple
L’extrait de code ci-dessous montre ce à quoi pourrait ressembler votre boucle d’entrainement. Il n’y
a aucune obligation de suivre ce canvas.
from lle import LLE
from rlenv.wrappers import TimeLimit
env = TimeLimit(LLE.level(1), 80) # Maximum 80 time steps
agents = [QAgent(lr, gamma, ...), QAgent(lr, gamma, ...), ...]
observation = env.reset()
done = truncated = False
score = 0
while not (done or truncated):
actions = [a.choose_action(observation) for a in agents]
next_observation, reward, done, truncated, info = env.step(actions)
for a in agents:
a.update(...)
score += reward
...
Lorsque vous récupérez une observation, vous pouvez accéder à son contenu avec observation.data
qui contient un tableau numpy dont la forme est (n_agents, ...).
Conseil: LLE est un environnement multi-agent et utilise la librairie rlenv prévue à cet effet. N’ou-
bliez pas de bien prendre en compte le caractère multi-agent dans la représentation de vos données.
5.2 Approximate Q-learning
Le but de cet exercice est d’implémenter l’Approximate Q-Learning (AQL) pour votre agent. Pour rap-
pel, AQL suppose l’existence d’une fonction 𝑓(𝑠) qui associe à chaque état (et pour chaque agent) un
vecteur de features [𝑓 (𝑠),𝑓 (𝑠),…,𝑓 (𝑠)] avec 𝑓 (𝑠) : 𝑆 → ℝ.
1 2 𝑛 𝑘
Concrètement, dans le cas de LLE, 𝑓 (𝑠) pourrait être le nombre de gemmes non collectées, 𝑓 (𝑠) la
1 2
distance (en lignes) à la gemme la plus proche, 𝑓 (𝑠) la distance (en colonnes) à la gemme la plus
3
proche, 𝑓 (𝑠) la présense (1) ou l’absence (0) d’un laser nord de l’agent, etc.
4
Dans AQL, la fonction 𝑄(𝑠,𝑎) d’un agent 𝑛 est définie comme indiqué dans l’Equation 4.
𝑘
𝑄 (𝑠,𝑎) = ∑𝑓 (𝑠)𝑤 (4)
𝑛 𝑖 𝑎,𝑖
𝑖=1
4
FACULTÉ DES SCIENCES
DÉPARTEMENT D’INFORMATIQUE
Dans le Q-learning, on définit la TD-error 𝛿 (temporal difference error) comme la différence entre la
Q-value estimée et la Q-value effective (estimée avec l’équation de Bellman).
𝛿 = [𝑅(𝑠,𝑎,𝑠′)+𝛾𝑉(𝑠′)]−𝑄(𝑠,𝑎) (5)
Dans AQL, le principe est de mettre à jour les poids 𝑤 de 𝑄 (𝑠,𝑎) de sorte à diminuer la TD-error 𝛿
𝑛
conformément à l’Equation 6
𝑤 ← 𝑤 +𝛼𝛿𝑓(𝑠) (6)
𝑎,𝑖 𝑎,𝑖
Implémentation
Dans le fichier approximate_qlearning.py:
1. Définissez une fonction feature_extraction(world) qui prend en entrée le World et renvoie
pour chaque agent les features que vous aurez choisies.
2. Implémentez l’algorithme de Approximate Q-Learning.
Entrainement et rapport
Entraînez votre algorithme AQL sur les niveaux 1, 3 et 6 de LLE. Dans votre rapport, créez un graphique
pour chacun de ces trois niveaux qui montre le score (c’est-à-dire la somme des rewards par épisode)
au cours de l’entrainement. Voir Section 6 pour plus d’informations sur le rapport.
6 Rapport
6.1 Value iteration
• Quelle limitation voyez-vous à l’algorithme de Value Iteration ? Dans quelle mesure est-il applic-
able à un niveau avec un nombre plus important d’agents ou avec une carte plus grande ?
• Expliquez pourquoi Value Iteration n’est pas un algorithme de RL mais plutôt un planificateur
hors-ligne.
• Appliquez l’algorithme de Value Iteration sur le niveau 1 de LLE jusqu’à ce que la policy se stabilise.
Ensuite, montrez graphiquement la stratégie découverte par l’algorithme.
6.2 Expériences avec le Q-learning
Le but ici est de mesurer les performances des deux algorithmes (Q-learning et AQL) dans les niveaux
évoqués précedemment. Vous devez ensuite présenter ces données dans des graphiques adaptés (type
de graphique, échelle, axes, titre).
Métrique
La métrique que vous devez tracer est le score, c’est-à-dire la somme des récompenses obtenues au
cours d’un épisode.
Méthode
Pour chaque expérience, donnez les paramètres que vous avez utilisés:
• le discount factor 𝛾
• le learning rate 𝛼
• la fonction de feature extraction 𝑓(𝑠)
• le niveau dont il est question
• la limite de temps que vous avez utilisée
• la manière dont évolue votre 𝜀 au cours du temps
• …
5
FACULTÉ DES SCIENCES
DÉPARTEMENT D’INFORMATIQUE
Graphiques
Nous vous conseillons la librairie matplotlib pour tracer des graphiques. Cette librairie est installée
automatiquement si vous avez utilisé poetry.
Les graphiques que vous devez produire sont ceux qui indiquent le score moyen au cours du temps,
calculé sur minimum 10 entrainements. Vos graphiques doivent aussi montrer la déviation standard
en plus de la moyenne, comme dans ce deuxième exemple de la documentation de matplotlib.
Remise
Le livrable de ce projet se présente sous la forme d’un fichier zip contenant vos sources python ainsi
que votre rapport en PDF. Nous vous encourageons à utiliser un outil tel que Typst ou Latex (par ex-
emple avec Overleaf) pour rédiger votre rapport.
Ce travail est individuel et doit être rendu sur l’Université Virtuelle pour le 10/12/2023 à 23:59.
6
----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2023-2024 projet RL-1.pdf -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2023-2024 projet RL-v2.pdf -----FACULTÉ DES SCIENCES
DÉPARTEMENT D’INFORMATIQUE
INFO-F-311: Intelligence Artificielle
Projet 4: Reinforcement Learning
Yannick Molinghen Pascal Tribel Tom Lenaerts
1 Préambule
Dans ce projet, vous allez implémenter des algorithmes d’apprentissage par renforcement (reinforce-
ment learning). On vous fournit des fichiers de base pour le projet que vous pouvez les télécharger sur
l’université virtuelle.
1.1 Poetry
Le projet nécessite Python ≥ 3.10 et utilise le système de build Poetry (https://python-poetry.org/docs/
#installation).
Après avoir installé Poetry, ouvrez un terminal dans le répertoire du projet et lancez les commandes:
poetry shell
poetry install
Ces commandes vont automatiquement créer un environnement virtuel puis installer les dépendances
du projet. Vous pouvez aussi utiliser pip pour installer les dépendences du projet en regardant les
dépendences à installer dans le fichier pyproject.toml.
1.2 Tests automatiques
Vous pouvez tester vos fonctionnalités avec la commande pytest. Pour lancer uniquement les tests
d’un fichier en particulier, vous pouvez le donner en paramètre. Il y a un fichier de test pour chacune
des tâches que vous devez remplir.
Note: Il est très difficile de tester des algorithmes de RL. Par conséquent, il n’y a des tests que pour
les exercices de la Section 3 et de la Section 4.
2 Introduction
Intuitivement, un processus de décision markovien (Markov Decision Process, MDP) est composé d’é-
tats dans lesquels un agent effectue des actions qui ont une certaine probabilité d’atterrir dans un autre
état.
Après chaque transition, l’agent reçoit une récompense qui donne une indication sur la qualité de
l’action: plus la récompense est élevée, meilleure est l’action. Le but d’un agent est de maximiser la
somme des récompenses au cours d’un épisode (c’est-à-dire une partie).
Par conséquent, pour chaque état 𝑠, on peut calculer sa valeur 𝑉(𝑠) qui correspond à la meilleure
somme des récompenses possible à partir de 𝑠 grâce à l’équation de Bellman présentée dans l’E-
quation 1.
𝑉(𝑠) = max∑𝑃(𝑠,𝑎,𝑠′)[𝑅(𝑠,𝑎,𝑠′)+𝛾𝑉(𝑠′)] (1)
𝑎∈𝐴
1
FACULTÉ DES SCIENCES
DÉPARTEMENT D’INFORMATIQUE
3 Value iteration
L’algorithme de «value iteration» a pour but d’approximer itérativement la valeur 𝑉(𝑠) de chaque état
s à l’aide de l’Equation 2.
𝑉 = max∑𝑃(𝑠,𝑎,𝑠′)[𝑅(𝑠,𝑎,𝑠′)+𝛾𝑉 ]
𝑘+1 𝑘(𝑠′) (2)
𝑎
𝑠′
L’algorithme de value iteration prend en paramètre un entier 𝑘 qui détermine combien d’itération ef-
fectuer. Implémentez l’algorithme value iteration dans le fichier value_iteration.py.
Conseils:
• Quand vous implémentez l’algorithme, faites attention à vous baser sur 𝑉 pour calculer 𝑉
𝑘 𝑘+1
et à ne pas modifier 𝑉 durant l’itération.
𝑘
• N’oubliez pas que la valeur d’un état terminal est 0 par définition.
4 Trouvez les bons paramètres
Dans cet exercice, on vous demande de trouver les bons paramètres du MDP et de value iteration pour
induire un certain comportement à l’agent.
Le MDP considéré est celui présenté dans la Figure 1. Il s’agit d’un lle.World dont les rewards ont été
modifiées et qui comprend sept états terminaux:
• Cinq “ravins” en bas de la carte. L’agent meurt s’il s’y rend et reçoit une “récompense” de −10;
• Une sortie proche qui rapporte une récompense de 1;
• Une sortie lointaine qui rapporte une récompense de 10.
Figure 1: MDP pour lequel trouver les bons paramètres
2
FACULTÉ DES SCIENCES
DÉPARTEMENT D’INFORMATIQUE
Le but de cet exercice est d’induire les comportements suivants:
1. Préférer la sortie proche (+1) en longeant la falaise.
2. Préférer la sortie proche (+1) en évitant la falaise.
3. Préférer la sortie distante (+10) en longeant la falaise.
4. Préférer la sortie distante (+10) en évitant la falaise.
5. Eviter de terminer le jeu (l’épisode ne se termine jamais).
Pour ce faire, choisissez des valeurs adéquates de chaque comportement dans la fonction correspon-
dante pour reward_live (la récompense à chaque étape pour continuer le jeu), gamma (le discount fac-
tor) et noise (la probabilité de prendre une action aléatoire) dans le fichier analysis.py.
5 Q-learning
L’algorithme de Q-learning consiste à interagir avec l’environnement pour mettre à jour une fonction
d’évaluation 𝑄(𝑠,𝑎). Contrairement aux exercices précédents, on évalue ici la valeur d’une action dans
un état 𝑄(𝑠,𝑎) et pas la valeur de l’état 𝑉(𝑠).
Une fois que votre agent est entraîné, vous pouvez exploiter la stratégie apprise en prenant l’action
ayant la plus haute q-value max 𝑄(𝑠,𝑎) dans chaque état.
𝑎∈𝐴
Pour entraîner votre agent, il est nécessaire d’explorer l’environnement. Pour ce faire, vous devez im-
plémenter la stratégie d’exploration dite «𝜺-greedy» qui consiste à choisir une action aléatoire avec
une probabilité ε, et à prendre la meilleure action avec une probabilité 1−𝜀.
Note: Il n’y a quasiment pas de tests pour les exercices liés au q-learning car ceux-ci sont très diffi-
ciles à tester. On vous demande par contre d’analyser vos résultats dans le rapport (voir Section 6).
5.1 Q-learning tabulaire
Pour entraîner la fonction 𝑄(𝑠,𝑎), on emploie une méthode adaptée de l’équation de Bellman (Equa-
tion 1) illustrée dans l’Equation 3, où 𝛼 est le learning rate.
𝑄(𝑠,𝑎) ← (1−𝛼)𝑄(𝑠,𝑎)+𝛼[𝑅(𝑠,𝑎,𝑠′)+𝛾𝑉(𝑠′)] (3)
Implémentation
Implémentez l’algorithme de Q-learning dans le fichier qlearning.py. Faites en sorte que cette implé-
mentation utilise un dictionnaire pour stocker les qvalues de chaque état.
Conseils:
• Pour hasher des tableaux numpy, nous vous conseillons d’utiliser hash(array.tobytes()).
• Pour favoriser l’exploration, initialisez vos 𝑄(𝑠,𝑎) à 1 et non à 0.
Entraînement et rapport
Entraînez votre algorithme sur les niveaux 1, 3 et 6 de LLE. Dans votre rapport, créez un graphique
pour chacun de ces trois niveaux qui montre le score (c’est-à-dire la somme des rewards par épisode)
au cours de l’entrainement. Voir Section 6 pour plus d’informations sur le rapport.
3
FACULTÉ DES SCIENCES
DÉPARTEMENT D’INFORMATIQUE
Table 1: De gauche à droite: niveau 1, niveau 3 et niveau 6 de LLE
Exemple
L’extrait de code ci-dessous montre ce à quoi pourrait ressembler votre boucle d’entrainement. Il n’y
a aucune obligation de suivre ce canvas.
from lle import LLE
from rlenv.wrappers import TimeLimit
env = TimeLimit(LLE.level(1), 80) # Maximum 80 time steps
agents = [QAgent(lr, gamma, ...), QAgent(lr, gamma, ...), ...]
observation = env.reset()
done = truncated = False
score = 0
while not (done or truncated):
actions = [a.choose_action(observation) for a in agents]
next_observation, reward, done, truncated, info = env.step(actions)
for a in agents:
a.update(...)
score += reward
...
Lorsque vous récupérez une observation, vous pouvez accéder à son contenu avec observation.data
qui contient un tableau numpy dont la forme est (n_agents, ...).
Conseil: LLE est un environnement multi-agent et utilise la librairie rlenv prévue à cet effet. N’ou-
bliez pas de bien prendre en compte le caractère multi-agent dans la représentation de vos données.
5.2 Approximate Q-learning
Le but de cet exercice est d’implémenter l’Approximate Q-Learning (AQL) pour votre agent. Pour rap-
pel, AQL suppose l’existence d’une fonction 𝑓(𝑠) qui associe à chaque état (et pour chaque agent) un
vecteur de features [𝑓 (𝑠),𝑓 (𝑠),…,𝑓 (𝑠)] avec 𝑓 (𝑠) : 𝑆 → ℝ.
1 2 𝑛 𝑘
Concrètement, dans le cas de LLE, 𝑓 (𝑠) pourrait être le nombre de gemmes non collectées, 𝑓 (𝑠) la
1 2
distance (en lignes) à la gemme la plus proche, 𝑓 (𝑠) la distance (en colonnes) à la gemme la plus
3
proche, 𝑓 (𝑠) la présense (1) ou l’absence (0) d’un laser nord de l’agent, etc.
4
Dans AQL, la fonction 𝑄(𝑠,𝑎) d’un agent 𝑛 est définie comme indiqué dans l’Equation 4.
𝑘
𝑄 (𝑠,𝑎) = ∑𝑓 (𝑠)𝑤 (4)
𝑛 𝑖 𝑎,𝑖
𝑖=1
4
FACULTÉ DES SCIENCES
DÉPARTEMENT D’INFORMATIQUE
Dans le Q-learning, on définit la TD-error 𝛿 (temporal difference error) comme la différence entre la
Q-value estimée et la Q-value effective (estimée avec l’équation de Bellman).
𝛿 = [𝑅(𝑠,𝑎,𝑠′)+𝛾𝑉(𝑠′)]−𝑄(𝑠,𝑎) (5)
Dans AQL, le principe est de mettre à jour les poids 𝑤 de 𝑄 (𝑠,𝑎) de sorte à diminuer la TD-error 𝛿
𝑛
conformément à l’Equation 6
𝑤 ← 𝑤 +𝛼𝛿𝑓(𝑠) (6)
𝑎,𝑖 𝑎,𝑖
Implémentation
Dans le fichier approximate_qlearning.py:
1. Définissez une fonction feature_extraction(world) qui prend en entrée le World et renvoie
pour chaque agent les features que vous aurez choisies.
2. Implémentez l’algorithme de Approximate Q-Learning.
Entrainement et rapport
Entraînez votre algorithme AQL sur les niveaux 1, 3 et 6 de LLE. Dans votre rapport, créez un graphique
pour chacun de ces trois niveaux qui montre le score (c’est-à-dire la somme des rewards par épisode)
au cours de l’entrainement. Voir Section 6 pour plus d’informations sur le rapport.
6 Rapport
6.1 Value iteration
• Quelle limitation voyez-vous à l’algorithme de Value Iteration ? Dans quelle mesure est-il applic-
able à un niveau avec un nombre plus important d’agents ou avec une carte plus grande ?
• Expliquez pourquoi Value Iteration n’est pas un algorithme de RL mais plutôt un planificateur
hors-ligne.
• Appliquez l’algorithme de Value Iteration sur le niveau 1 de LLE jusqu’à ce que la policy se stabilise.
Ensuite, montrez graphiquement la stratégie découverte par l’algorithme.
6.2 Expériences avec le Q-learning
Le but ici est de mesurer les performances des deux algorithmes (Q-learning et AQL) dans les niveaux
évoqués précedemment. Vous devez ensuite présenter ces données dans des graphiques adaptés (type
de graphique, échelle, axes, titre).
Métrique
La métrique que vous devez tracer est le score, c’est-à-dire la somme des récompenses obtenues au
cours d’un épisode.
Méthode
Pour chaque expérience, donnez les paramètres que vous avez utilisés:
• le discount factor 𝛾
• le learning rate 𝛼
• la fonction de feature extraction 𝑓(𝑠)
• le niveau dont il est question
• la limite de temps que vous avez utilisée
• la manière dont évolue votre 𝜀 au cours du temps
• …
5
FACULTÉ DES SCIENCES
DÉPARTEMENT D’INFORMATIQUE
Graphiques
Nous vous conseillons la librairie matplotlib pour tracer des graphiques. Cette librairie est installée
automatiquement si vous avez utilisé poetry.
Les graphiques que vous devez produire sont ceux qui indiquent le score moyen au cours du temps,
calculé sur minimum 10 entrainements. Vos graphiques doivent aussi montrer la déviation standard
en plus de la moyenne, comme dans ce deuxième exemple de la documentation de matplotlib.
Remise
Le livrable de ce projet se présente sous la forme d’un fichier zip contenant vos sources python ainsi
que votre rapport en PDF. Nous vous encourageons à utiliser un outil tel que Typst ou Latex (par ex-
emple avec Overleaf) pour rédiger votre rapport.
Ce travail est individuel et doit être rendu sur l’Université Virtuelle pour le 10/12/2023 à 23:59.
6
FACULTÉ DES SCIENCES
DÉPARTEMENT D’INFORMATIQUE
7 Annexes
7.1 Précisions sur lle.LLE
La classe lle.LLE est directement dédiée au RL multi-agents (MARL) coopératif. Par soucis de général-
ité, LLE utilise le vocabulaire des observations plutôt que des états afin de différencier ce que les agents
voient de l’état de l’environnement.
Dans LLE, une observation peut se représenter couche par couche, comme montré dans la Figure 2.
Figure 2: Représentation d’une observation correspondant à la situation de l’imade de droite dans la
Table 1. Les cases noires représentent des 1, les cases grises des −1 et les cases blanches des 0. Par
soucis de concision, certaines couches (agent 2, agent 3, laser 2, laser 3) ont été ommise.
Le type d’observation indiqué dans la Figure 2 peut être utilisé en utilisant ObservationType.LAYERED.
Dans ce cas, chaque agent reçoit une observation (x, height, width) où x dépend du nombre d’a-
gents.
from lle import LLE, ObservationType
env = LLE.level(6, ObservationType.LAYERED)
print(env.observation_shape) # (12, 12, 13)
Cependant, par défaut (et pour pouvoir être utilisées dans des réseaux de neurones linéaires), LLE utilise
le type d’observation ObservationType.FLATTENED, qui encodent exactement les mêmes informations
que ObservationType.LAYERED, mais applaties sur une seule dimension.
from lle import LLE, ObservationType
env = LLE.level(6, ObservationType.FLATTENED) # On peut ommettre le second paramètre
print(env.observation_shape) # Affiche (1872,), càd 12 * 12 * 13
Notez que chaque observation possède aussi des informations sur l’état du World dans
observation.state, à la différence que le WorldState auquel vous êtes habitués a lui aussi été trans-
formé en tableau numpy pour être utilisable dans le contexte du deep MARL.
7
----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2023-2024 projet RL-v2.pdf -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\2023-2024_projet_2.pdf -----FACULTÉ DES SCIENCES
DÉPARTEMENT D’INFORMATIQUE
INFO-F-311: Intelligence Artificielle
Projet 2: Recherche adversariale
Yannick Molinghen Pascal Tribel Tom Lenaerts
1. Préambule
Dans ce projet, vous allez implémenter des techniques d’intelligence articifielle basées sur de la
recherche dans des graphes en considérant un ou plusieurs adversaires. On vous fournit des fichiers
de base pour le projet que vous pouvez les télécharger sur l’université virtuelle.
1.1. Rust
L’environnement dans lequel vous allez travailler est implémenté en Rust. Pour pouvoir compiler le
projet, vous aurez besoin d’installer le compilateur adapté avec la commande ci-dessous issue du site
officiel.
curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
Si vous utilisez Windows, vous pouvez utiliser Windows Subsystem for Linux (WSL) ou vous référer
à la procédure décrite dans la documentation.
1.2. Poetry
Le projet nécessite Python ≥ 3.10 et utilise Poetry pour gérer les dépendances Python et les environ-
nements virtuels. Ouvrez un terminal dans le répertoire du projet, installez Poetry puis installez les
mises dépendances du projet:
poetry shell
poetry install
Ces commandes vont automatiquement créer un environnement virtuel puis installer les dépendances
du projet.
1.3. Tests automatiques
Vous pouvez tester vos méthodes avec la commande pytest. Pour lancer uniquement les tests d’un
fichier en particulier, vous pouvez le donner en paramètre:
pytest tests/tests_bfs.py
Il y a un fichier de test pour chacune des tâches que vous devez remplir.
1.4. Fichiers existants
On vous donne les fichiers mdp.py, adversarial_search.py et world_mdp.py, mais vous ne devrez tra-
vailler que dans les deux derniers. mdp.py contient la déclaration de l’interface d’un Markov Decision
Process, adversarial_search.py contient les algorithmes de recherche que vous allez implémenter
tandis que world_mdp.py contient l’adaptation du lle.World en MDP dans lequel chaque agent agit
chacun à son tour.
Important: Vous pouvez modifier TOUT ce que vous voulez dans le code qui vous est donné à
l’exception des tests unitaires. Ce code vous est uniquement donné comme guide.
1
FACULTÉ DES SCIENCES
DÉPARTEMENT D’INFORMATIQUE
1.5. Laser Learning Environment
Comme lors du premier projet, vous allez utiliser la librairie lle illustrée dans la Figure 1 pour ce
projet. Il existe un jupyter notebook sur le git de du projet qui vous montre les fonctionnalités de l’en-
vironnement et qui vous permet de vous familiariser à son utilisation.
Figure 1: Rendu de l'environnement
Rapport de bugs: L’environnement LLE est encore récent, et quelques bugs pourraient subsister.
Dans le cas où vous pensez en avoir trouvé un, n’hésitez pas à le signaler par email à l’adresse
yannick.molinghen@ulb.be en joignant le code minimal nécessaire à le reproduire. Chaque pre-
mière personne à rapporter un bug (et qui est confirmé) donnera lieu à un point supplémentaire
avec un maximum de 2 points sur le projet.
2. Adaptation du World en MDP
Comme LLE est initialement un environnement cooperatif, vous allez adapter l’environnement de
sorte qu’il fonctionne comme un MDP compétitif dans lequel un agent agit à la fois. Votre but est
de maximiser le nombre de gemmes collectées par l’agent 0, les autres agents étant considérés
comme des adversaires. Pour ce faire, implémentez les classes WorldMDP et WorldState dans le fichier
world_mdp.py.
WorldState
Comme il s’agit d’un MDP à plusieurs agents et à tour par tour, chaque état doit retenir à quel agent
c’est le tour d’effectuer une action.
WorldMDP
• Après avoir fait reset(), c’est à l’agent 0 d’effectuer une action. Ainsi,
world.trasition(Action.NORTH) fera uniquement bouger l’agent 0 vers le nord tandis que tous les
autres resteront sur place. Ensuite, c’est à l’agent 1 de bouger et ainsi de suite.
• La méthode world.available_actions(state) renvoie les actions accessibles à l’agent courant.
• La valeur d’un état correspond à la somme des rewards obtenues par les actions de l’agent 0 (c’est-
à-dire les gemmes collectées + arriver sur une case de fin).
• Si l’agent 0 meurt lors d’une transition, la valeur de l’état tombe à la valeur lle.REWARD_AGENT_DIED
(-1) immédiatement, sans prendre en compte les gemmes déjà collectées.
2
FACULTÉ DES SCIENCES
DÉPARTEMENT D’INFORMATIQUE
3. Algorithmes de recherche
3.1. Minimax
Dans le fichier adversarial_search.py implémentez l’algorithme du minimax dans la fonction
minimax qui renvoie l’action à effectuer par l’agent 0 dans l’état donné. Cette fonction n’accepte que
des états pour lesquels c’est à l’agent 0 de jouer et lève une ValueError dans le cas contraire. On vous
suggère de diviser cet algorithme en une fonction _min et une fonction _max. N’oubliez pas qu’il peut
y avoir plus d’un adversaire.
3.2. Alpha-beta pruning
De manière similaire à la tâche précédente, implémentez l’algorithme 𝛼𝛽-pruning dans la fonction
alpha_beta. Cette fonction renvoie l’action à effectuer pour l’agent 0 dans l’état donné.
3.3. Expectimax
Dans minimax et 𝛼𝛽-pruning, nous avons supposé que l’adversaire agissait de manière optimale.
Cependant, ce n’est pas toujours le cas pour des humains. L’algorithme « expectimax » permet de
modéliser le comportement probabiliste d’humains qui pourraient prendre des choix sous-optimaux.
La nature d’expectimax demande que nous connaissions la probabilité que l’adversaire prenne chaque
action. Nous allons ici supposer que les autres agents prennent des actions uniformément aléatoires.
3.4. Meilleure évaluation
Comme dernier exercice, on vous demande d’implémenter une sous-classe de WorldMDP dans laquelle
la valeur d’un état est calculée de manière plus intelligente que simplement en considérant le score de
l’agent 0. Ecrivez cette sous-classe et vérifiez que pour une même carte, le nombre de noeuds étendus
est en effet plus bas qu’avec la fonction d’évaluation de base. Il n’y a pas de test pour cet exercice.
4. Rapport
On vous demande d’écrire un court rapport (environ 2 pages) sur deux sujets:
• Expliquez l’idée derrière votre fonction d’évaluation utilisée en Section 3.4
• Comparez le nombre d’états étendus dans différents algorithmes pour un même niveau. Pour ce faire,
inventez trois cartes (documenation dans le jupyter notebook et sur le wiki du git) que vous illustrez
dans le rapport. Pour chacune des cartes, reportez graphiquement le nombre d’états étendus dans la
recherche d’action pour
• minimax,
• minimax avec votre meilleure fonction d’évaluation,
• alpha_beta,
• alpha_beta avec la meilleure fonction d’évaluation.
Veillez à choisir une méthode de représentation graphique adaptée.
Remise
Le livrable de ce projet se présente sous la forme d’un fichier zip contenant les fichiers:
adversarial_search.py, world_mdp.py et votre rapport au format PDF.
Le travail est individuel et doit être rendu sur l’Université Virtuelle pour le 29/10/2023 à 23:59.
3
----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\2023-2024_projet_2.pdf -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\map2023_10_29_13_48_15.png -----FACULTÉ DES SCIENCES
DÉPARTEMENT D’INFORMATIQUE
INFO-F-311: Intelligence Artificielle
Projet 2: Recherche adversariale
Yannick Molinghen Pascal Tribel Tom Lenaerts
1. Préambule
Dans ce projet, vous allez implémenter des techniques d’intelligence articifielle basées sur de la
recherche dans des graphes en considérant un ou plusieurs adversaires. On vous fournit des fichiers
de base pour le projet que vous pouvez les télécharger sur l’université virtuelle.
1.1. Rust
L’environnement dans lequel vous allez travailler est implémenté en Rust. Pour pouvoir compiler le
projet, vous aurez besoin d’installer le compilateur adapté avec la commande ci-dessous issue du site
officiel.
curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
Si vous utilisez Windows, vous pouvez utiliser Windows Subsystem for Linux (WSL) ou vous référer
à la procédure décrite dans la documentation.
1.2. Poetry
Le projet nécessite Python ≥ 3.10 et utilise Poetry pour gérer les dépendances Python et les environ-
nements virtuels. Ouvrez un terminal dans le répertoire du projet, installez Poetry puis installez les
mises dépendances du projet:
poetry shell
poetry install
Ces commandes vont automatiquement créer un environnement virtuel puis installer les dépendances
du projet.
1.3. Tests automatiques
Vous pouvez tester vos méthodes avec la commande pytest. Pour lancer uniquement les tests d’un
fichier en particulier, vous pouvez le donner en paramètre:
pytest tests/tests_bfs.py
Il y a un fichier de test pour chacune des tâches que vous devez remplir.
1.4. Fichiers existants
On vous donne les fichiers mdp.py, adversarial_search.py et world_mdp.py, mais vous ne devrez tra-
vailler que dans les deux derniers. mdp.py contient la déclaration de l’interface d’un Markov Decision
Process, adversarial_search.py contient les algorithmes de recherche que vous allez implémenter
tandis que world_mdp.py contient l’adaptation du lle.World en MDP dans lequel chaque agent agit
chacun à son tour.
Important: Vous pouvez modifier TOUT ce que vous voulez dans le code qui vous est donné à
l’exception des tests unitaires. Ce code vous est uniquement donné comme guide.
1
FACULTÉ DES SCIENCES
DÉPARTEMENT D’INFORMATIQUE
1.5. Laser Learning Environment
Comme lors du premier projet, vous allez utiliser la librairie lle illustrée dans la Figure 1 pour ce
projet. Il existe un jupyter notebook sur le git de du projet qui vous montre les fonctionnalités de l’en-
vironnement et qui vous permet de vous familiariser à son utilisation.
Figure 1: Rendu de l'environnement
Rapport de bugs: L’environnement LLE est encore récent, et quelques bugs pourraient subsister.
Dans le cas où vous pensez en avoir trouvé un, n’hésitez pas à le signaler par email à l’adresse
yannick.molinghen@ulb.be en joignant le code minimal nécessaire à le reproduire. Chaque pre-
mière personne à rapporter un bug (et qui est confirmé) donnera lieu à un point supplémentaire
avec un maximum de 2 points sur le projet.
2. Adaptation du World en MDP
Comme LLE est initialement un environnement cooperatif, vous allez adapter l’environnement de
sorte qu’il fonctionne comme un MDP compétitif dans lequel un agent agit à la fois. Votre but est
de maximiser le nombre de gemmes collectées par l’agent 0, les autres agents étant considérés
comme des adversaires. Pour ce faire, implémentez les classes WorldMDP et WorldState dans le fichier
world_mdp.py.
WorldState
Comme il s’agit d’un MDP à plusieurs agents et à tour par tour, chaque état doit retenir à quel agent
c’est le tour d’effectuer une action.
WorldMDP
• Après avoir fait reset(), c’est à l’agent 0 d’effectuer une action. Ainsi,
world.trasition(Action.NORTH) fera uniquement bouger l’agent 0 vers le nord tandis que tous les
autres resteront sur place. Ensuite, c’est à l’agent 1 de bouger et ainsi de suite.
• La méthode world.available_actions(state) renvoie les actions accessibles à l’agent courant.
• La valeur d’un état correspond à la somme des rewards obtenues par les actions de l’agent 0 (c’est-
à-dire les gemmes collectées + arriver sur une case de fin).
• Si l’agent 0 meurt lors d’une transition, la valeur de l’état tombe à la valeur lle.REWARD_AGENT_DIED
(-1) immédiatement, sans prendre en compte les gemmes déjà collectées.
2
FACULTÉ DES SCIENCES
DÉPARTEMENT D’INFORMATIQUE
3. Algorithmes de recherche
3.1. Minimax
Dans le fichier adversarial_search.py implémentez l’algorithme du minimax dans la fonction
minimax qui renvoie l’action à effectuer par l’agent 0 dans l’état donné. Cette fonction n’accepte que
des états pour lesquels c’est à l’agent 0 de jouer et lève une ValueError dans le cas contraire. On vous
suggère de diviser cet algorithme en une fonction _min et une fonction _max. N’oubliez pas qu’il peut
y avoir plus d’un adversaire.
3.2. Alpha-beta pruning
De manière similaire à la tâche précédente, implémentez l’algorithme 𝛼𝛽-pruning dans la fonction
alpha_beta. Cette fonction renvoie l’action à effectuer pour l’agent 0 dans l’état donné.
3.3. Expectimax
Dans minimax et 𝛼𝛽-pruning, nous avons supposé que l’adversaire agissait de manière optimale.
Cependant, ce n’est pas toujours le cas pour des humains. L’algorithme « expectimax » permet de
modéliser le comportement probabiliste d’humains qui pourraient prendre des choix sous-optimaux.
La nature d’expectimax demande que nous connaissions la probabilité que l’adversaire prenne chaque
action. Nous allons ici supposer que les autres agents prennent des actions uniformément aléatoires.
3.4. Meilleure évaluation
Comme dernier exercice, on vous demande d’implémenter une sous-classe de WorldMDP dans laquelle
la valeur d’un état est calculée de manière plus intelligente que simplement en considérant le score de
l’agent 0. Ecrivez cette sous-classe et vérifiez que pour une même carte, le nombre de noeuds étendus
est en effet plus bas qu’avec la fonction d’évaluation de base. Il n’y a pas de test pour cet exercice.
4. Rapport
On vous demande d’écrire un court rapport (environ 2 pages) sur deux sujets:
• Expliquez l’idée derrière votre fonction d’évaluation utilisée en Section 3.4
• Comparez le nombre d’états étendus dans différents algorithmes pour un même niveau. Pour ce faire,
inventez trois cartes (documenation dans le jupyter notebook et sur le wiki du git) que vous illustrez
dans le rapport. Pour chacune des cartes, reportez graphiquement le nombre d’états étendus dans la
recherche d’action pour
• minimax,
• minimax avec votre meilleure fonction d’évaluation,
• alpha_beta,
• alpha_beta avec la meilleure fonction d’évaluation.
Veillez à choisir une méthode de représentation graphique adaptée.
Remise
Le livrable de ce projet se présente sous la forme d’un fichier zip contenant les fichiers:
adversarial_search.py, world_mdp.py et votre rapport au format PDF.
Le travail est individuel et doit être rendu sur l’Université Virtuelle pour le 29/10/2023 à 23:59.
3
----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\map2023_10_29_13_48_15.png -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\map2023_10_29_13_49_35.png -----FACULTÉ DES SCIENCES
DÉPARTEMENT D’INFORMATIQUE
INFO-F-311: Intelligence Artificielle
Projet 2: Recherche adversariale
Yannick Molinghen Pascal Tribel Tom Lenaerts
1. Préambule
Dans ce projet, vous allez implémenter des techniques d’intelligence articifielle basées sur de la
recherche dans des graphes en considérant un ou plusieurs adversaires. On vous fournit des fichiers
de base pour le projet que vous pouvez les télécharger sur l’université virtuelle.
1.1. Rust
L’environnement dans lequel vous allez travailler est implémenté en Rust. Pour pouvoir compiler le
projet, vous aurez besoin d’installer le compilateur adapté avec la commande ci-dessous issue du site
officiel.
curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
Si vous utilisez Windows, vous pouvez utiliser Windows Subsystem for Linux (WSL) ou vous référer
à la procédure décrite dans la documentation.
1.2. Poetry
Le projet nécessite Python ≥ 3.10 et utilise Poetry pour gérer les dépendances Python et les environ-
nements virtuels. Ouvrez un terminal dans le répertoire du projet, installez Poetry puis installez les
mises dépendances du projet:
poetry shell
poetry install
Ces commandes vont automatiquement créer un environnement virtuel puis installer les dépendances
du projet.
1.3. Tests automatiques
Vous pouvez tester vos méthodes avec la commande pytest. Pour lancer uniquement les tests d’un
fichier en particulier, vous pouvez le donner en paramètre:
pytest tests/tests_bfs.py
Il y a un fichier de test pour chacune des tâches que vous devez remplir.
1.4. Fichiers existants
On vous donne les fichiers mdp.py, adversarial_search.py et world_mdp.py, mais vous ne devrez tra-
vailler que dans les deux derniers. mdp.py contient la déclaration de l’interface d’un Markov Decision
Process, adversarial_search.py contient les algorithmes de recherche que vous allez implémenter
tandis que world_mdp.py contient l’adaptation du lle.World en MDP dans lequel chaque agent agit
chacun à son tour.
Important: Vous pouvez modifier TOUT ce que vous voulez dans le code qui vous est donné à
l’exception des tests unitaires. Ce code vous est uniquement donné comme guide.
1
FACULTÉ DES SCIENCES
DÉPARTEMENT D’INFORMATIQUE
1.5. Laser Learning Environment
Comme lors du premier projet, vous allez utiliser la librairie lle illustrée dans la Figure 1 pour ce
projet. Il existe un jupyter notebook sur le git de du projet qui vous montre les fonctionnalités de l’en-
vironnement et qui vous permet de vous familiariser à son utilisation.
Figure 1: Rendu de l'environnement
Rapport de bugs: L’environnement LLE est encore récent, et quelques bugs pourraient subsister.
Dans le cas où vous pensez en avoir trouvé un, n’hésitez pas à le signaler par email à l’adresse
yannick.molinghen@ulb.be en joignant le code minimal nécessaire à le reproduire. Chaque pre-
mière personne à rapporter un bug (et qui est confirmé) donnera lieu à un point supplémentaire
avec un maximum de 2 points sur le projet.
2. Adaptation du World en MDP
Comme LLE est initialement un environnement cooperatif, vous allez adapter l’environnement de
sorte qu’il fonctionne comme un MDP compétitif dans lequel un agent agit à la fois. Votre but est
de maximiser le nombre de gemmes collectées par l’agent 0, les autres agents étant considérés
comme des adversaires. Pour ce faire, implémentez les classes WorldMDP et WorldState dans le fichier
world_mdp.py.
WorldState
Comme il s’agit d’un MDP à plusieurs agents et à tour par tour, chaque état doit retenir à quel agent
c’est le tour d’effectuer une action.
WorldMDP
• Après avoir fait reset(), c’est à l’agent 0 d’effectuer une action. Ainsi,
world.trasition(Action.NORTH) fera uniquement bouger l’agent 0 vers le nord tandis que tous les
autres resteront sur place. Ensuite, c’est à l’agent 1 de bouger et ainsi de suite.
• La méthode world.available_actions(state) renvoie les actions accessibles à l’agent courant.
• La valeur d’un état correspond à la somme des rewards obtenues par les actions de l’agent 0 (c’est-
à-dire les gemmes collectées + arriver sur une case de fin).
• Si l’agent 0 meurt lors d’une transition, la valeur de l’état tombe à la valeur lle.REWARD_AGENT_DIED
(-1) immédiatement, sans prendre en compte les gemmes déjà collectées.
2
FACULTÉ DES SCIENCES
DÉPARTEMENT D’INFORMATIQUE
3. Algorithmes de recherche
3.1. Minimax
Dans le fichier adversarial_search.py implémentez l’algorithme du minimax dans la fonction
minimax qui renvoie l’action à effectuer par l’agent 0 dans l’état donné. Cette fonction n’accepte que
des états pour lesquels c’est à l’agent 0 de jouer et lève une ValueError dans le cas contraire. On vous
suggère de diviser cet algorithme en une fonction _min et une fonction _max. N’oubliez pas qu’il peut
y avoir plus d’un adversaire.
3.2. Alpha-beta pruning
De manière similaire à la tâche précédente, implémentez l’algorithme 𝛼𝛽-pruning dans la fonction
alpha_beta. Cette fonction renvoie l’action à effectuer pour l’agent 0 dans l’état donné.
3.3. Expectimax
Dans minimax et 𝛼𝛽-pruning, nous avons supposé que l’adversaire agissait de manière optimale.
Cependant, ce n’est pas toujours le cas pour des humains. L’algorithme « expectimax » permet de
modéliser le comportement probabiliste d’humains qui pourraient prendre des choix sous-optimaux.
La nature d’expectimax demande que nous connaissions la probabilité que l’adversaire prenne chaque
action. Nous allons ici supposer que les autres agents prennent des actions uniformément aléatoires.
3.4. Meilleure évaluation
Comme dernier exercice, on vous demande d’implémenter une sous-classe de WorldMDP dans laquelle
la valeur d’un état est calculée de manière plus intelligente que simplement en considérant le score de
l’agent 0. Ecrivez cette sous-classe et vérifiez que pour une même carte, le nombre de noeuds étendus
est en effet plus bas qu’avec la fonction d’évaluation de base. Il n’y a pas de test pour cet exercice.
4. Rapport
On vous demande d’écrire un court rapport (environ 2 pages) sur deux sujets:
• Expliquez l’idée derrière votre fonction d’évaluation utilisée en Section 3.4
• Comparez le nombre d’états étendus dans différents algorithmes pour un même niveau. Pour ce faire,
inventez trois cartes (documenation dans le jupyter notebook et sur le wiki du git) que vous illustrez
dans le rapport. Pour chacune des cartes, reportez graphiquement le nombre d’états étendus dans la
recherche d’action pour
• minimax,
• minimax avec votre meilleure fonction d’évaluation,
• alpha_beta,
• alpha_beta avec la meilleure fonction d’évaluation.
Veillez à choisir une méthode de représentation graphique adaptée.
Remise
Le livrable de ce projet se présente sous la forme d’un fichier zip contenant les fichiers:
adversarial_search.py, world_mdp.py et votre rapport au format PDF.
Le travail est individuel et doit être rendu sur l’Université Virtuelle pour le 29/10/2023 à 23:59.
3
----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\map2023_10_29_13_49_35.png -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\map2023_10_29_13_53_25.png -----FACULTÉ DES SCIENCES
DÉPARTEMENT D’INFORMATIQUE
INFO-F-311: Intelligence Artificielle
Projet 2: Recherche adversariale
Yannick Molinghen Pascal Tribel Tom Lenaerts
1. Préambule
Dans ce projet, vous allez implémenter des techniques d’intelligence articifielle basées sur de la
recherche dans des graphes en considérant un ou plusieurs adversaires. On vous fournit des fichiers
de base pour le projet que vous pouvez les télécharger sur l’université virtuelle.
1.1. Rust
L’environnement dans lequel vous allez travailler est implémenté en Rust. Pour pouvoir compiler le
projet, vous aurez besoin d’installer le compilateur adapté avec la commande ci-dessous issue du site
officiel.
curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
Si vous utilisez Windows, vous pouvez utiliser Windows Subsystem for Linux (WSL) ou vous référer
à la procédure décrite dans la documentation.
1.2. Poetry
Le projet nécessite Python ≥ 3.10 et utilise Poetry pour gérer les dépendances Python et les environ-
nements virtuels. Ouvrez un terminal dans le répertoire du projet, installez Poetry puis installez les
mises dépendances du projet:
poetry shell
poetry install
Ces commandes vont automatiquement créer un environnement virtuel puis installer les dépendances
du projet.
1.3. Tests automatiques
Vous pouvez tester vos méthodes avec la commande pytest. Pour lancer uniquement les tests d’un
fichier en particulier, vous pouvez le donner en paramètre:
pytest tests/tests_bfs.py
Il y a un fichier de test pour chacune des tâches que vous devez remplir.
1.4. Fichiers existants
On vous donne les fichiers mdp.py, adversarial_search.py et world_mdp.py, mais vous ne devrez tra-
vailler que dans les deux derniers. mdp.py contient la déclaration de l’interface d’un Markov Decision
Process, adversarial_search.py contient les algorithmes de recherche que vous allez implémenter
tandis que world_mdp.py contient l’adaptation du lle.World en MDP dans lequel chaque agent agit
chacun à son tour.
Important: Vous pouvez modifier TOUT ce que vous voulez dans le code qui vous est donné à
l’exception des tests unitaires. Ce code vous est uniquement donné comme guide.
1
FACULTÉ DES SCIENCES
DÉPARTEMENT D’INFORMATIQUE
1.5. Laser Learning Environment
Comme lors du premier projet, vous allez utiliser la librairie lle illustrée dans la Figure 1 pour ce
projet. Il existe un jupyter notebook sur le git de du projet qui vous montre les fonctionnalités de l’en-
vironnement et qui vous permet de vous familiariser à son utilisation.
Figure 1: Rendu de l'environnement
Rapport de bugs: L’environnement LLE est encore récent, et quelques bugs pourraient subsister.
Dans le cas où vous pensez en avoir trouvé un, n’hésitez pas à le signaler par email à l’adresse
yannick.molinghen@ulb.be en joignant le code minimal nécessaire à le reproduire. Chaque pre-
mière personne à rapporter un bug (et qui est confirmé) donnera lieu à un point supplémentaire
avec un maximum de 2 points sur le projet.
2. Adaptation du World en MDP
Comme LLE est initialement un environnement cooperatif, vous allez adapter l’environnement de
sorte qu’il fonctionne comme un MDP compétitif dans lequel un agent agit à la fois. Votre but est
de maximiser le nombre de gemmes collectées par l’agent 0, les autres agents étant considérés
comme des adversaires. Pour ce faire, implémentez les classes WorldMDP et WorldState dans le fichier
world_mdp.py.
WorldState
Comme il s’agit d’un MDP à plusieurs agents et à tour par tour, chaque état doit retenir à quel agent
c’est le tour d’effectuer une action.
WorldMDP
• Après avoir fait reset(), c’est à l’agent 0 d’effectuer une action. Ainsi,
world.trasition(Action.NORTH) fera uniquement bouger l’agent 0 vers le nord tandis que tous les
autres resteront sur place. Ensuite, c’est à l’agent 1 de bouger et ainsi de suite.
• La méthode world.available_actions(state) renvoie les actions accessibles à l’agent courant.
• La valeur d’un état correspond à la somme des rewards obtenues par les actions de l’agent 0 (c’est-
à-dire les gemmes collectées + arriver sur une case de fin).
• Si l’agent 0 meurt lors d’une transition, la valeur de l’état tombe à la valeur lle.REWARD_AGENT_DIED
(-1) immédiatement, sans prendre en compte les gemmes déjà collectées.
2
FACULTÉ DES SCIENCES
DÉPARTEMENT D’INFORMATIQUE
3. Algorithmes de recherche
3.1. Minimax
Dans le fichier adversarial_search.py implémentez l’algorithme du minimax dans la fonction
minimax qui renvoie l’action à effectuer par l’agent 0 dans l’état donné. Cette fonction n’accepte que
des états pour lesquels c’est à l’agent 0 de jouer et lève une ValueError dans le cas contraire. On vous
suggère de diviser cet algorithme en une fonction _min et une fonction _max. N’oubliez pas qu’il peut
y avoir plus d’un adversaire.
3.2. Alpha-beta pruning
De manière similaire à la tâche précédente, implémentez l’algorithme 𝛼𝛽-pruning dans la fonction
alpha_beta. Cette fonction renvoie l’action à effectuer pour l’agent 0 dans l’état donné.
3.3. Expectimax
Dans minimax et 𝛼𝛽-pruning, nous avons supposé que l’adversaire agissait de manière optimale.
Cependant, ce n’est pas toujours le cas pour des humains. L’algorithme « expectimax » permet de
modéliser le comportement probabiliste d’humains qui pourraient prendre des choix sous-optimaux.
La nature d’expectimax demande que nous connaissions la probabilité que l’adversaire prenne chaque
action. Nous allons ici supposer que les autres agents prennent des actions uniformément aléatoires.
3.4. Meilleure évaluation
Comme dernier exercice, on vous demande d’implémenter une sous-classe de WorldMDP dans laquelle
la valeur d’un état est calculée de manière plus intelligente que simplement en considérant le score de
l’agent 0. Ecrivez cette sous-classe et vérifiez que pour une même carte, le nombre de noeuds étendus
est en effet plus bas qu’avec la fonction d’évaluation de base. Il n’y a pas de test pour cet exercice.
4. Rapport
On vous demande d’écrire un court rapport (environ 2 pages) sur deux sujets:
• Expliquez l’idée derrière votre fonction d’évaluation utilisée en Section 3.4
• Comparez le nombre d’états étendus dans différents algorithmes pour un même niveau. Pour ce faire,
inventez trois cartes (documenation dans le jupyter notebook et sur le wiki du git) que vous illustrez
dans le rapport. Pour chacune des cartes, reportez graphiquement le nombre d’états étendus dans la
recherche d’action pour
• minimax,
• minimax avec votre meilleure fonction d’évaluation,
• alpha_beta,
• alpha_beta avec la meilleure fonction d’évaluation.
Veillez à choisir une méthode de représentation graphique adaptée.
Remise
Le livrable de ce projet se présente sous la forme d’un fichier zip contenant les fichiers:
adversarial_search.py, world_mdp.py et votre rapport au format PDF.
Le travail est individuel et doit être rendu sur l’Université Virtuelle pour le 29/10/2023 à 23:59.
3
----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\map2023_10_29_13_53_25.png -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\map2023_10_29_13_55_47.png -----FACULTÉ DES SCIENCES
DÉPARTEMENT D’INFORMATIQUE
INFO-F-311: Intelligence Artificielle
Projet 2: Recherche adversariale
Yannick Molinghen Pascal Tribel Tom Lenaerts
1. Préambule
Dans ce projet, vous allez implémenter des techniques d’intelligence articifielle basées sur de la
recherche dans des graphes en considérant un ou plusieurs adversaires. On vous fournit des fichiers
de base pour le projet que vous pouvez les télécharger sur l’université virtuelle.
1.1. Rust
L’environnement dans lequel vous allez travailler est implémenté en Rust. Pour pouvoir compiler le
projet, vous aurez besoin d’installer le compilateur adapté avec la commande ci-dessous issue du site
officiel.
curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
Si vous utilisez Windows, vous pouvez utiliser Windows Subsystem for Linux (WSL) ou vous référer
à la procédure décrite dans la documentation.
1.2. Poetry
Le projet nécessite Python ≥ 3.10 et utilise Poetry pour gérer les dépendances Python et les environ-
nements virtuels. Ouvrez un terminal dans le répertoire du projet, installez Poetry puis installez les
mises dépendances du projet:
poetry shell
poetry install
Ces commandes vont automatiquement créer un environnement virtuel puis installer les dépendances
du projet.
1.3. Tests automatiques
Vous pouvez tester vos méthodes avec la commande pytest. Pour lancer uniquement les tests d’un
fichier en particulier, vous pouvez le donner en paramètre:
pytest tests/tests_bfs.py
Il y a un fichier de test pour chacune des tâches que vous devez remplir.
1.4. Fichiers existants
On vous donne les fichiers mdp.py, adversarial_search.py et world_mdp.py, mais vous ne devrez tra-
vailler que dans les deux derniers. mdp.py contient la déclaration de l’interface d’un Markov Decision
Process, adversarial_search.py contient les algorithmes de recherche que vous allez implémenter
tandis que world_mdp.py contient l’adaptation du lle.World en MDP dans lequel chaque agent agit
chacun à son tour.
Important: Vous pouvez modifier TOUT ce que vous voulez dans le code qui vous est donné à
l’exception des tests unitaires. Ce code vous est uniquement donné comme guide.
1
FACULTÉ DES SCIENCES
DÉPARTEMENT D’INFORMATIQUE
1.5. Laser Learning Environment
Comme lors du premier projet, vous allez utiliser la librairie lle illustrée dans la Figure 1 pour ce
projet. Il existe un jupyter notebook sur le git de du projet qui vous montre les fonctionnalités de l’en-
vironnement et qui vous permet de vous familiariser à son utilisation.
Figure 1: Rendu de l'environnement
Rapport de bugs: L’environnement LLE est encore récent, et quelques bugs pourraient subsister.
Dans le cas où vous pensez en avoir trouvé un, n’hésitez pas à le signaler par email à l’adresse
yannick.molinghen@ulb.be en joignant le code minimal nécessaire à le reproduire. Chaque pre-
mière personne à rapporter un bug (et qui est confirmé) donnera lieu à un point supplémentaire
avec un maximum de 2 points sur le projet.
2. Adaptation du World en MDP
Comme LLE est initialement un environnement cooperatif, vous allez adapter l’environnement de
sorte qu’il fonctionne comme un MDP compétitif dans lequel un agent agit à la fois. Votre but est
de maximiser le nombre de gemmes collectées par l’agent 0, les autres agents étant considérés
comme des adversaires. Pour ce faire, implémentez les classes WorldMDP et WorldState dans le fichier
world_mdp.py.
WorldState
Comme il s’agit d’un MDP à plusieurs agents et à tour par tour, chaque état doit retenir à quel agent
c’est le tour d’effectuer une action.
WorldMDP
• Après avoir fait reset(), c’est à l’agent 0 d’effectuer une action. Ainsi,
world.trasition(Action.NORTH) fera uniquement bouger l’agent 0 vers le nord tandis que tous les
autres resteront sur place. Ensuite, c’est à l’agent 1 de bouger et ainsi de suite.
• La méthode world.available_actions(state) renvoie les actions accessibles à l’agent courant.
• La valeur d’un état correspond à la somme des rewards obtenues par les actions de l’agent 0 (c’est-
à-dire les gemmes collectées + arriver sur une case de fin).
• Si l’agent 0 meurt lors d’une transition, la valeur de l’état tombe à la valeur lle.REWARD_AGENT_DIED
(-1) immédiatement, sans prendre en compte les gemmes déjà collectées.
2
FACULTÉ DES SCIENCES
DÉPARTEMENT D’INFORMATIQUE
3. Algorithmes de recherche
3.1. Minimax
Dans le fichier adversarial_search.py implémentez l’algorithme du minimax dans la fonction
minimax qui renvoie l’action à effectuer par l’agent 0 dans l’état donné. Cette fonction n’accepte que
des états pour lesquels c’est à l’agent 0 de jouer et lève une ValueError dans le cas contraire. On vous
suggère de diviser cet algorithme en une fonction _min et une fonction _max. N’oubliez pas qu’il peut
y avoir plus d’un adversaire.
3.2. Alpha-beta pruning
De manière similaire à la tâche précédente, implémentez l’algorithme 𝛼𝛽-pruning dans la fonction
alpha_beta. Cette fonction renvoie l’action à effectuer pour l’agent 0 dans l’état donné.
3.3. Expectimax
Dans minimax et 𝛼𝛽-pruning, nous avons supposé que l’adversaire agissait de manière optimale.
Cependant, ce n’est pas toujours le cas pour des humains. L’algorithme « expectimax » permet de
modéliser le comportement probabiliste d’humains qui pourraient prendre des choix sous-optimaux.
La nature d’expectimax demande que nous connaissions la probabilité que l’adversaire prenne chaque
action. Nous allons ici supposer que les autres agents prennent des actions uniformément aléatoires.
3.4. Meilleure évaluation
Comme dernier exercice, on vous demande d’implémenter une sous-classe de WorldMDP dans laquelle
la valeur d’un état est calculée de manière plus intelligente que simplement en considérant le score de
l’agent 0. Ecrivez cette sous-classe et vérifiez que pour une même carte, le nombre de noeuds étendus
est en effet plus bas qu’avec la fonction d’évaluation de base. Il n’y a pas de test pour cet exercice.
4. Rapport
On vous demande d’écrire un court rapport (environ 2 pages) sur deux sujets:
• Expliquez l’idée derrière votre fonction d’évaluation utilisée en Section 3.4
• Comparez le nombre d’états étendus dans différents algorithmes pour un même niveau. Pour ce faire,
inventez trois cartes (documenation dans le jupyter notebook et sur le wiki du git) que vous illustrez
dans le rapport. Pour chacune des cartes, reportez graphiquement le nombre d’états étendus dans la
recherche d’action pour
• minimax,
• minimax avec votre meilleure fonction d’évaluation,
• alpha_beta,
• alpha_beta avec la meilleure fonction d’évaluation.
Veillez à choisir une méthode de représentation graphique adaptée.
Remise
Le livrable de ce projet se présente sous la forme d’un fichier zip contenant les fichiers:
adversarial_search.py, world_mdp.py et votre rapport au format PDF.
Le travail est individuel et doit être rendu sur l’Université Virtuelle pour le 29/10/2023 à 23:59.
3
----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\map2023_10_29_13_55_47.png -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\map2023_10_29_13_56_42.png -----FACULTÉ DES SCIENCES
DÉPARTEMENT D’INFORMATIQUE
INFO-F-311: Intelligence Artificielle
Projet 2: Recherche adversariale
Yannick Molinghen Pascal Tribel Tom Lenaerts
1. Préambule
Dans ce projet, vous allez implémenter des techniques d’intelligence articifielle basées sur de la
recherche dans des graphes en considérant un ou plusieurs adversaires. On vous fournit des fichiers
de base pour le projet que vous pouvez les télécharger sur l’université virtuelle.
1.1. Rust
L’environnement dans lequel vous allez travailler est implémenté en Rust. Pour pouvoir compiler le
projet, vous aurez besoin d’installer le compilateur adapté avec la commande ci-dessous issue du site
officiel.
curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
Si vous utilisez Windows, vous pouvez utiliser Windows Subsystem for Linux (WSL) ou vous référer
à la procédure décrite dans la documentation.
1.2. Poetry
Le projet nécessite Python ≥ 3.10 et utilise Poetry pour gérer les dépendances Python et les environ-
nements virtuels. Ouvrez un terminal dans le répertoire du projet, installez Poetry puis installez les
mises dépendances du projet:
poetry shell
poetry install
Ces commandes vont automatiquement créer un environnement virtuel puis installer les dépendances
du projet.
1.3. Tests automatiques
Vous pouvez tester vos méthodes avec la commande pytest. Pour lancer uniquement les tests d’un
fichier en particulier, vous pouvez le donner en paramètre:
pytest tests/tests_bfs.py
Il y a un fichier de test pour chacune des tâches que vous devez remplir.
1.4. Fichiers existants
On vous donne les fichiers mdp.py, adversarial_search.py et world_mdp.py, mais vous ne devrez tra-
vailler que dans les deux derniers. mdp.py contient la déclaration de l’interface d’un Markov Decision
Process, adversarial_search.py contient les algorithmes de recherche que vous allez implémenter
tandis que world_mdp.py contient l’adaptation du lle.World en MDP dans lequel chaque agent agit
chacun à son tour.
Important: Vous pouvez modifier TOUT ce que vous voulez dans le code qui vous est donné à
l’exception des tests unitaires. Ce code vous est uniquement donné comme guide.
1
FACULTÉ DES SCIENCES
DÉPARTEMENT D’INFORMATIQUE
1.5. Laser Learning Environment
Comme lors du premier projet, vous allez utiliser la librairie lle illustrée dans la Figure 1 pour ce
projet. Il existe un jupyter notebook sur le git de du projet qui vous montre les fonctionnalités de l’en-
vironnement et qui vous permet de vous familiariser à son utilisation.
Figure 1: Rendu de l'environnement
Rapport de bugs: L’environnement LLE est encore récent, et quelques bugs pourraient subsister.
Dans le cas où vous pensez en avoir trouvé un, n’hésitez pas à le signaler par email à l’adresse
yannick.molinghen@ulb.be en joignant le code minimal nécessaire à le reproduire. Chaque pre-
mière personne à rapporter un bug (et qui est confirmé) donnera lieu à un point supplémentaire
avec un maximum de 2 points sur le projet.
2. Adaptation du World en MDP
Comme LLE est initialement un environnement cooperatif, vous allez adapter l’environnement de
sorte qu’il fonctionne comme un MDP compétitif dans lequel un agent agit à la fois. Votre but est
de maximiser le nombre de gemmes collectées par l’agent 0, les autres agents étant considérés
comme des adversaires. Pour ce faire, implémentez les classes WorldMDP et WorldState dans le fichier
world_mdp.py.
WorldState
Comme il s’agit d’un MDP à plusieurs agents et à tour par tour, chaque état doit retenir à quel agent
c’est le tour d’effectuer une action.
WorldMDP
• Après avoir fait reset(), c’est à l’agent 0 d’effectuer une action. Ainsi,
world.trasition(Action.NORTH) fera uniquement bouger l’agent 0 vers le nord tandis que tous les
autres resteront sur place. Ensuite, c’est à l’agent 1 de bouger et ainsi de suite.
• La méthode world.available_actions(state) renvoie les actions accessibles à l’agent courant.
• La valeur d’un état correspond à la somme des rewards obtenues par les actions de l’agent 0 (c’est-
à-dire les gemmes collectées + arriver sur une case de fin).
• Si l’agent 0 meurt lors d’une transition, la valeur de l’état tombe à la valeur lle.REWARD_AGENT_DIED
(-1) immédiatement, sans prendre en compte les gemmes déjà collectées.
2
FACULTÉ DES SCIENCES
DÉPARTEMENT D’INFORMATIQUE
3. Algorithmes de recherche
3.1. Minimax
Dans le fichier adversarial_search.py implémentez l’algorithme du minimax dans la fonction
minimax qui renvoie l’action à effectuer par l’agent 0 dans l’état donné. Cette fonction n’accepte que
des états pour lesquels c’est à l’agent 0 de jouer et lève une ValueError dans le cas contraire. On vous
suggère de diviser cet algorithme en une fonction _min et une fonction _max. N’oubliez pas qu’il peut
y avoir plus d’un adversaire.
3.2. Alpha-beta pruning
De manière similaire à la tâche précédente, implémentez l’algorithme 𝛼𝛽-pruning dans la fonction
alpha_beta. Cette fonction renvoie l’action à effectuer pour l’agent 0 dans l’état donné.
3.3. Expectimax
Dans minimax et 𝛼𝛽-pruning, nous avons supposé que l’adversaire agissait de manière optimale.
Cependant, ce n’est pas toujours le cas pour des humains. L’algorithme « expectimax » permet de
modéliser le comportement probabiliste d’humains qui pourraient prendre des choix sous-optimaux.
La nature d’expectimax demande que nous connaissions la probabilité que l’adversaire prenne chaque
action. Nous allons ici supposer que les autres agents prennent des actions uniformément aléatoires.
3.4. Meilleure évaluation
Comme dernier exercice, on vous demande d’implémenter une sous-classe de WorldMDP dans laquelle
la valeur d’un état est calculée de manière plus intelligente que simplement en considérant le score de
l’agent 0. Ecrivez cette sous-classe et vérifiez que pour une même carte, le nombre de noeuds étendus
est en effet plus bas qu’avec la fonction d’évaluation de base. Il n’y a pas de test pour cet exercice.
4. Rapport
On vous demande d’écrire un court rapport (environ 2 pages) sur deux sujets:
• Expliquez l’idée derrière votre fonction d’évaluation utilisée en Section 3.4
• Comparez le nombre d’états étendus dans différents algorithmes pour un même niveau. Pour ce faire,
inventez trois cartes (documenation dans le jupyter notebook et sur le wiki du git) que vous illustrez
dans le rapport. Pour chacune des cartes, reportez graphiquement le nombre d’états étendus dans la
recherche d’action pour
• minimax,
• minimax avec votre meilleure fonction d’évaluation,
• alpha_beta,
• alpha_beta avec la meilleure fonction d’évaluation.
Veillez à choisir une méthode de représentation graphique adaptée.
Remise
Le livrable de ce projet se présente sous la forme d’un fichier zip contenant les fichiers:
adversarial_search.py, world_mdp.py et votre rapport au format PDF.
Le travail est individuel et doit être rendu sur l’Université Virtuelle pour le 29/10/2023 à 23:59.
3
----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\map2023_10_29_13_56_42.png -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\pyproject.toml -----[tool.poetry]
name = "2-adversarial"
version = "0.1.0"
description = ""
authors = ["Yannick Molinghen <yannick.molinghen@ulb.be>"]
readme = "README.md"

[tool.poetry.dependencies]
python = ">=3.10, <3.13"
typing-extensions = "^4.8.0"
laser-learning-environment = "^0.1.5"
opencv-python = "^4.8.1.78"
rlenv = {git = "https://github.com/yamoling/rlenv"}
anytree = "^2.10.0"
graphviz = "^0.20.1"
cairosvg = "^2.7.1"
loguru = "^0.7.2"
scipy = "^1.11.3"
matplotlib = "^3.8.0"


[tool.poetry.dev-dependencies]
pytest = "^7.0"

[build-system]
requires = ["poetry-core"]
build-backend = "poetry.core.masonry.api"


[tool.pytest.ini_options]
pythonpath = ["src"]


[tool.ruff]
# Check https://beta.ruff.rs/docs/rules/ for all rules
fixable = ["ALL"]

# Do not fix imports automatically (it removes unused imports in __init__.py files)
# unfixable = ["F401"]
line-length = 140
----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\pyproject.toml -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\doc\in\Korf_Multi-player-Alpha-beta-Pruning.pdf -----Artificial Intelligence 48 (1991) 99-111 99
Elsevier
Research Note
Multi-player alpha-beta pruning
Richard E. Korf*
Computer Science Department, University of California, Los Angeles, Los Angeles,
CA 90024, USA
Received August 1989
Revised June 1990
Abstract
Korf, R.E., Multi-player alpha-beta pruning (Research Note), Artificial Intelligence 48
(1991) 99-111.
We consider the generalization of minimax search with alpha-beta pruning to non-coopera-
tive, perfect-information games with more than two players. The minimax algorithm was
generalized in 2 to the maxn algorithm applied to vectors of n-tuples representing the
evaluations for each of the players. If we assume an upper bound on the sum of the
evaluations for each player, and a lower bound on each individual evaluation, then shallow
alpha-beta pruning is possible, but not deep pruning. In the best case, the asymptotic
branching factor is reduced to (1 + 4bv'~b-"s~-3)/2.I n the average case, however, pruning does
not reduce the asymptotic branching factor. Thus, alpha-beta pruning is found to be
effective only in the special case of two-player games. In addition, we show that it is an
optimal directional algorithm for two players.
1. Introduction
Minimax search with alpha-beta pruning is the predominant algorithm
employed by two-player game programs 1, 3, 5. Figure 1 shows a game tree,
where squares represent maximizing nodes and circles correspond to minimiz-
ing nodes, along with its minimax value, bounds on interior nodes, and those
branches pruned by alpha-beta.
In this paper, we consider the generalization of alpha-beta pruning to
non-cooperative, perfect-information games with more than two players. For
example, Chinese Checkers can involve up to six different players moving
* Section 3 of this paper represents a more complete treatment of Section 3 of a paper by the
same author entitled, "Generalized game trees", that appeared in the Proceedings of the Eleventh
International Joint Conference on Artificial Intelligence ( IJCA1-89), Detroit, MI (1989).
0004-3702/91/$03.50 (~ 1991 -- Elsevier Science Publishers B.V.
100 R.E. Korf
/
11 _>15 _<3
111 9 a 15 16 3 b c
Fig. 1. Two-player alpha-beta pruning.
alternately. As another example, Othello can easily be extended to an arbitrary
number of players by having different colored pieces for each player, and
modifying the rules such that whenever a mixed row of opposing pieces is
flanked on both sides by two pieces of the same player, then all the pieces are
captured by the flanking player.
2. Maxn algorithm
Luckhardt and Irani 2 extended minimax to multi-player games, calling the
resulting algorithm max". For typographical convenience we refer to it as
maxn. They assume that the players alternate moves, that each player tries to
maximize his or her perceived return, and is indifferent to the returns of the
remaining players. At the frontier nodes, an evaluation function is applied that
returns an N-tuple of values, where N is the number of players, with each
component corresponding to the estimated merit of the position with respect to
one of the players. Then, the value of each interior node where player i is to
move is the entire N-tuple of the child for which the ith component is a
maximum. Figure 2 shows a maxn tree for three players, with the correspond-
ing maxn values.
(2,8,1)(1,7,2) (5,6,3) (6,5,4) (8,4,S) (7,3,6) (4,2,7) (3,1,8)
Fig. 2. Three-player maxn game tree.
Multi-player alpha-beta pruning 101
More formally, define M(x) to be the static heuristic value of node x, and
M(x, p) to be the backed-up maxn value of node x, given that player p is to
move at node x. Ms(x, p) is the component of M(x, p) that corresponds to the
return for player i. We can then define the maxn value of a node recursively as
follows:
M(x), if x is a frontier node,
M(x, p) = M(xi, p') , otherwise,
such that Mp(xi, p') = max Mp(Xi, p') where x i are the children of node x, p'
is the player that follows player p in the move order, and ties are broken in
favor of the leftmost node.
For example, an evaluation function for multi-player Othello might return
the number of pieces for each player on the board at any given point.
Minimax can be viewed as a special case of maxn for two players. The
evaluation function returns an ordered pair of x and -x, and each player
maximizes its component at its moves.
Luckhardt and Irani 2 observed that at nodes where player i is to move,
only the ith component of the children need be evaluated. At best, this can
produce a constant factor speedup, but it may be no less expensive to compute
all components than to compute only one. They correctly concluded that
without further assumptions on the values of the components, pruning of entire
branches is not possible with more than two players. Thus, they did not explore
such pruning any further.
They used the terms "shallow pruning" and "deep pruning" to refer to their
techniques of avoiding some partial evaluations. Since these terms had previ-
ously been used to describe actual tree pruning in the alpha-beta literature 1,
we will use the original meanings of both these terms, at the cost of inconsis-
tency with Luckhardt's and Irani's terminology 2.
3. Alpha-beta in multi-player games
If there is an upper bound on the sum of all components of a tuple, and
there is a lower bound on the values of each component, then actual tree
pruning is possible. The first condition is a weaker form of the standard
constant-sum assumption, which is in fact required for two-player alpha-beta
pruning. The second is equivalent to assuming a lower bound of zero on each
component, since any other lower bound can be shifted to zero by subtracting
it from every component. Most practical evaluation functions will satisfy both
these conditions, since violating them implies that the value of an individual
component can be unbounded in at least one direction. For example, in the
evaluation function described above for multi-player Othello, no player can
have less than zero pieces on the board, and the total number of pieces on the
102 R.E. Korf
board is the same for all nodes at the same level in the game tree, since exactly
one piece is added at each move.
3.1. Immediate pruning
The simplest kind of pruning possible under these assumptions occurs when
player i is to move, and the ith component of one of its children equals the
upper bound on the sum of all components. In that case, all remaining children
can be pruned, since no child's ith component can exceed the upper bound on
the sum. We will refer to this as immediate pruning. This is equivalent to
situations in the two-player case when a child of a Max node has a value of ~¢,
or a child of a Min node has a value of -~.
3.2. Shallow pruning
A more complex situation is called shallow pruning in the alpha-beta
literature. Figure 3 shows an example of shallow pruning in a three-player
game, where the upper bound on the sum of each component is 9, Note that in
this particular example, the sum of each component is exactly 9, but an upper
bound is all that is required. Evaluating node b results in a lower bound of 3 on
the first component of node a, since player one is to move at node a. This
implies an upper bound on each of the remaining components of 9-3 = 6.
Evaluating node g produces a lower bound of 7 on the second component of
node f, since player two is to move at node f. Similarly, this implies an upper
bound on the remaining components of 9- 7 = 2. Since the upper bound (2)
on the first component of node f is less than or equal to the lower bound on the
first component of node a (3), player one won't choose node f and its
remaining children can be pruned. Similarly, evaluating node i causes its
remaining brothers to be pruned. This is similar to the pruning in the left
subtree of Fig. 1.
The procedure Shallow takes a Node to be evaluated, the Player to move at
that node, and an upper Bound on the component of the player to move, and
returns an N-tuple that is the maxn value of the node. Sum is the global upper
bound on the sum of all components of an N-tuple, and all components are
assumed to be non-negative. Initially, Shallow is called with the root of the
(3,3,3) (4,2,3) (3,1,5) (1,7,1) (1,6,2)
Fig. 3. Shallow pruning in three-player game tree.
Multi-player alpha-beta pruning 103
tree, the player to move at the root, and Sum. Note that the shallow pruning
procedure includes immediate pruning as a special case.
Shallow(Node, Player, Bound)
IF Node is terminal, RETURN static value
Best = Shallow(first Child, next Player, Sum)
FOR each remaining Child
IF BestPlayer > = Bound, RETURN Best
Current = Shallow(Child, next Player, Sum - BestPlayer)
IF CurrentPlayer > BestPlayer, Best = Current
RETURN Best
3.3. Correctness of shallow pruning procedure
Here we establish the correctness of the shallow pruning procedure, in the
sense that it computes the maxn value defined in Section 2.
Theorem 1. If the evaluation of any position for any player is non-negative, and
the sum of all player's evaluations in any given position is less than or equal to
sum, and ties are broken in favor of the leftmost node, then Shallow(a, p, sum)
= M(a, p), for any node a and player p.
Proof. Since the only difference between the maxn procedure and the shallow
pruning procedure is that it doesn't examine certain nodes, it suffices to show
that the pruned nodes can have no effect on the maxn value of the root. There
are two types of pruning implemented in the shallow pruning procedure,
immediate pruning and shallow pruning.
Figure 4 shows the generic case of immediate pruning. Without loss of
generality, we assume that player one is to move at the root. In order for
immediate pruning to occur, Ml(b, 2) = sum, for some child b of node a. Since
all components are assumed to be non-negative, sum is the maximum possible
value of any individual component, and the remaining components must be
zero. Therefore sum = M~(b, 2) = max M~(a i, 2), where a i is a child of node a.
Thus, M(a, 1) = M(b, 2), and the remaining children of node a need not be
examined.
Figure 5 shows the generic case of shallow pruning. Again we assume
without loss of generality that player one is to move at the root, followed by
(sum, 0, 0 .... )
Fig. 4. Generic case of immediate pruning.
104 R.E. Korf
b ~ c/J~-<sum-y, _>y. ... )
x+y~sum d 3L3_J
(,y .... )
Fig. 5. Generic case of shallow pruning.
players two and three in turn. Ml(b, 2) = x, M2(d, 3) = y, and x + y >t sum, in
order for shallow pruning to occur. Since player two is to move at node c,
M2(c, 2) = max M2(c i, 3), where ci is a child of node c. Therefore, M2(c, 2)
M2(d, 3)= y. Since the sum of all components cannot exceed sum, and all
components are non-negative, M~(c, 2) ~< sum - y ~ x, since x + y >1 sum.
Since M l(b, 2) = x I> M 1( c, 2), and ties are broken in favor of leftmost nodes,
M(a, 1) ~ M(c, 2), and node c can have no effect on the maxn value of node a.
Therefore, the remaining children of node c can be pruned.
Since neither immediate nor shallow pruning eliminate nodes that can effect
the maxn value of the root node, and the shallow pruning procedure returns
the maxn value of the nodes it has examined, it correctly calculates the maxn
value of the root node. C
3.4. Failure of deep pruning
In a two-player game, alpha-beta pruning allows an additional type of
pruning known as deep pruning. For example, in Fig. 1, nodes b and c are
pruned based on bounds inherited from their great-great-grandparent, the root
in this case. In general, deep pruning refers to pruning a node based on a
bound inherited from its great-grandparent, or any more distant ancestor. In a
two-player game tree, it can only occur in trees of height four or greater.
Surprisingly, deep pruning does not generalize to more than two players.
Figure 6 illustrates the problem. Again, the upper bound on the sum of each
component is 9. Evaluating node b produces a lower bound of 5 on the first
component of node a and hence an upper bound of 9 - 5 = 4 on the remaining
components. Evaluating node e results in a lower bound of 5 on the third
component of node d and hence an upper bound of 9 - 5 = 4 on the remaining
components. Since the upper bound of 4 on the first component of node d is
less than the lower bound of 5 on the first component of node a, the value of
node d cannot become the value of node a. Thus, we are tempted to prune
node f.
With three players, however, the value of node fcould effect the value of the
root, depending on the value of node g. For example, if the value of node f
Multi-player alpha-beta pruning 105
4 (6, 1, 2)
Je
f
(2, 2, 5) (2, 3, 4) or (3, O, 6)
Fig. 6. Failure of deep pruning for three players.
were (2, 3, 4), the value of node d would be (2, 2,'5), the value of node c
would be (2, 2, 5), and the value of node a would be (5, 2, 2). On the other
hand, if the value of node f were (3, 0, 6), then the value of node d would be
(3, 0, 6), the value of node c would be (6, 1, 2), and the value of node a would
be (6, 1, 2). Thus, even though the value of node fcannot be the maxn value of
the root, it can affect it. Hence, it cannot be pruned.
3.5. Optimality of shallow pruning
Given the failure of deep pruning in this example, is there a more restricted
form of pruning that is valid, or is shallow pruning the best we can do? The
answer is the latter, as expressed by the following theorem:
Theorem 2. Every directional algorithm that computes the maxn value of a
game tree with more than two players must evaluate every terminal node
evaluated by shallow pruning under the same ordering.
A directional algorithm 4 is one in which the order of node evaluation is
independent of the value of the nodes, and once a node is pruned it can never
be revisited. For example, a strictly left-to-right algorithm is directional.
Proof sketch. Since the actual proof given below is somewhat tedious and not
very revealing, we first present here an overview and example of the argument.
The main idea is illustrated by the construction in Fig. 7, which shows a
3-player, 6-level tree. We assume that node n is evaluated by shallow pruning
but pruned by another algorithm. We then show that the value of every node
above it depends on the value of node n. The letters to the left of the path
from the root to node n represent the greatest lower bounds on the compo-
nents corresponding to the player to move at each node. Since by assumption
node n is evaluated by shallow pruning, it must be the case that for any two
106 R.E. Korf
(~a,,) -
" (cid:127)
(, b,)E (a, , )
(,, >c) E" "~ (a+, b, O)
E "~(0, b+, c)
(, ~>e, ) ~ "~ (d, O, c÷)
(,, ~f) "~ (d+, e, O)
(,,0) or (, O, f+) "~ (0, e+, f)
n
Fig. 7. Proof sketch of optimality of shallow pruning.
consecutive bounds x and y, x + y is strictly less than the global upper bound.
Since the decision to skip node n is made before any of the nodes to the right
of the path are examined, we are free to choose any values for these nodes
consistent with the global bounds. For this purpose, x + represents a value
greater than x by an arbitrarily small amount. The reader is encouraged to
assign each of the two alternative values to node n, and then compute the
maxn value of the root, to see that it is different in the two cases. The
propagation of values up the tree can be viewed as a "zipper" effect in the
sense that the original order of the "teeth" (nodes) at the bottom determines
the order of the teeth at the top, even though no individual tooth can move
very far. The formal proof below is by induction on the height of the tree and
generalizes the result to an arbitrary number of players greater than two.
Proof (see Fig. 8). Assume the converse. Namely, that there exists a direction-
al maxn algorithm A, and a minimax tree T with N players, with a leaf node X °
that is evaluated by shallow pruning but not by algorithm A. Since both
shallow pruning and algorithm A are directional, and by assumption visit nodes
in the same order, (re)order the nodes in T from left to right in the order they
are visited, without loss of generality. Let X i refer to the ancestor of node X °
at height i in the tree, on the unique path from the root to X °, and also to its
maxn value, depending on the context. Let X~j refer to the jth component of
the maxn value of node X i. Thus, X i = (Xio ..... X/i ..... X,i, ). Let p(i) be the
player to move at level i in the tree. Thus, Xei¢ i) is the maximum value of Wpi- (I il
among all the children W i i of X i.
Consider the state of the shallow pruning algorithm just before it evaluates
node X °. It consists of the path from the root to node X °, plus a set of bounds
a i, where c~i is equal to the maximum value of
Weic-iI)
among all the children
W i- 1 of X i to the left of X ~ ~. In other words, ai is the greatest lower bound so
far on X*pu). If at any level i, X i-1 has no brothers to its left, then a~ will simply
equal the global lower bound on each component. Without loss of generality,
Multi-player alpha-beta pruning 107
I'--, i1 ~- ~ (V:(:) = °ti + e'
yi-1 = 0)
p(i+2)
i-I
Xp(i) = 0 o._r
(Xp-(li) = U,i + 2e, xi'lp(i+1 ) =0)
Fig. 8. Inductive proof of optimality of shallow pruning.
let the global lower bound on each component be zero and the global upper
bound on the sum of all components be one. Any other bounds can always be
mapped to this range without affecting either algorithm since only relative
values matter.
The assumption that X ° is evaluated by shallow pruning constrains the values
of the ai. First, each of the ai must be strictly less than one. Otherwise, the
remaining children of X ~ would be immediately pruned, pruning X ° as well.
Furthermore, each lower bound a~ induces a corresponding upper bound of
1 - a e on the remaining components of X i. If this upper bound on X i is less
p(i-1)
than or equal to the lower bound of ai_ ~ on X 'p-~~s _~), then the remaining
children of X i would be pruned by shallow pruning, pruning X ° as well. Thus,
for alli1>l, 1-ai>a~_l, or a~+a~_~<l.
Let Y~ be a brother of X i to its right. When either algorithm is about to
consider X °, the values and even the existence of the Y~ nodes is unknown to
the algorithms. Thus, we are free to choose values for the Y~, consistent with
the global bounds, without affecting either algorithm's decision to evaluate or
prune X °. We are also free to choose a value for X ° as well, since the decision
to evaluate or prune it is made before its value is known. The rest of the proof
consists of carefully choosing values for the yi and X ° so that the value of X ~,
for all i, will depend on the value of X °. This will be done by induction on i.
Basis step: i = 0. Trivially, if i = 0, then the value of X ~ = X ° depends on X °.
However, to strengthen the induction hypothesis, we will choose two different
0 0
values for X °. In one case, let Xp(~)= 0, and in the other case, let Xp(~)=
a~ +2e, and X°(2~ = 0, where e is an arbitrarily small positive value. In
particular, choose e such that ai + a~_ Z + 3e < 1, for all i/> 1.
Induction step. Now assume that for all values up to and including i- 1,
i-1 i-1 i-I
Xp(i) O, or Xp(i) = a i -1- 2e and Xp(~+~) = O, depending on the value of X °.
=
1(18 R.E. Korf
Choose the value of Y' ~ as follows: yip-l ,) = ai + e and Y'p (i+l) = O{i~ I +2E and
y,-~ = 0, Note that the sum of the y i ~ components is ai + e + ~i, ~ + 2e +
pIi +2 )
0 = a i + ai+ t +3e< 1, since a: + ai+ 1 < 1, and e was chosen to satisfy this
property. Thus, this is a legal set of components for the value of Yi 1.
Now there are two cases to consider: In case I, Xif,<~ = 0. In that case, since
yi-I
Ypi ( i)I = ai + 6 ) oLi ~ (} = Xpi ( i)I , is the best child of X ~, and the value of X ~
equals the value of Y' ~. In particular, Xpio +~ ) = Y'~ ,1~~ +1~=o~+~ +2e and
Xpi( i+2) = Ypi (iI+ 2)= 0. In case II, Xpi o I) = ai + 2e and Xpi ~ +~ l ) =0. In that case,
since X i p~ ~-I = ai + 2e > ~i + e = Y ip-1, )> a , xi-I is the best child of X ~, and the
• i i 1
value of X i equals the value of X ~ i In particular, Xp~+!) = X j,,+ i i = 0. Thus,
i
in one case, Xp(i+l) =0, and in the other case, Xp(i+l)=OLi+l+26 and
XJp~+2) = 0. This is the induction hypothesis for i.
Thus, the induction hypothesis is true for i, given that it is true for i- 1.
Therefore, for such a tree of any height, the maxn value of the root depends on
X". But if algorithm A prunes X °, it cannot determine the maxn value of the
root. This contradicts our assumption that algorithm A works for any maxn
tree. Therefore, every directional algorithm that computes the maxn value of a
game tree with more than two players must evaluate every terminal node
evaluated by shallow pruning.
3.6. Best-case performance
How effective is shallow pruning in the best case? For simplicity, we will
exclude immediate pruning by assuming that no one component can equal the
upper bound on the sum. The best-case analysis of shallow pruning is indepen-
dent of the number of players and was done by Knuth and Moore 1 for two
players.
In order to evaluate a node in the best case, one child must be evaluated,
and then evaluating one grandchild of each remaining child will cause the
remaining grandchildren to be pruned (see Fig. 3). Thus, If F(d) is the number
of leaf nodes generated to evaluate a tree of depth d with branching factor b in
the best case, then F(d) = F(d - 1) + (b - 1)*F(d - 2). Since a tree of depth
zero is a single node, and a tree of depth one requires all children to be
evaluated, the initial conditions are F(0)= 1 and F(1)= b. Note that in a
binary tree, F(d) is the familiar Fibonacci sequence. The solution to the
general recurrence has an asymptotic branching factor of ½( 1 + x/Tb-- 3). For
large values of b, this approaches ~ which is the best-case performance of full
two-player alpha-beta pruning.
3.7. Average-case performance
Knuth and Moore 1 also determined that in the average case, the asymp-
totic branching factor of two-player shallow pruning is approximately b/log b.
They assumed independent, distinct leaf values.
Multi-player alpha-beta pruning 109
In the case of multiple players, however, our model of the evaluation
function must have a lower bound on each component and an upper bound on
their sum. For simplicity, assume that the lower bound is zero and that the sum
is exactly one. Thus, we need a way of randomly choosing n-tuples such that
each component is identically distributed between zero and one, and the sum
of all components is one. One way to do this is by cutting the zero-one interval
in n- 1 places, with each cut point independently and identically distributed
from zero to one, and using the n resulting segments as the components of the
n-tuple. Another way is to choose n values independently and identically
distributed from zero to one, and then divide each component by the sum of all
of them. Furthermore, we assume that each tuple is generated independently
of the others.
Under this average-case model, the asymptotic branching factor of shallow
pruning with more than two players is simply b, the brute-force branching
factor. The analysis relies on the minimax convergence theorem 4, which
holds for two-player minimax trees. This surprising phenomenon is that if the
leaf values are chosen independently from the same distribution, the variance
of the root value decreases with increasing height of the tree, and in the limit
of infinite height, the root value can be predicted with probability one. The
actual limiting value depends on the leaf distribution and also on which player
moves last in the tree, but the convergence does not. Based on empirical
studies, we conjecture that minimax convergence applies to maxn trees as well.
In order for pruning to take place, the lower bound on one component must
be greater than or equal to its upper bound, which equals one minus the lower
bound on another component. Thus, pruning only takes place when the sum of
the lower bounds on two different components is greater than or equal to one.
In order for this to occur in the limiting value, the values of the remaining
components must be zero, since the sum of the two components in question is
one. This cannot happen in the limiting value, assuming continuous terminal
values. Thus, while pruning occurs at low levels of the tree, at higher levels it
becomes increasingly rare, and in the limit of infinite depth, it disappears
entirely. Thus, the asymptotic branching factor is simply b.
The shallow pruning algorithm has been implemented and its efficiency
tested using the above model of independent and identically distributed cut
points. Not only does the effective branching factor converge to b, but the
convergence is rapid enough that even for small trees, the branching factor is
very close to b. Thus, in practice, alpha-beta is not effective for multi-player
trees.
In the case of two-player alpha-beta, the performance of the algorithm is
purely a function of the order of the terminal values and not the values
themselves. In particular, Knuth and Moore 1 show that for any set of
terminal values there is an ordering for which alpha-beta achieves its best-case
performance. For example, if all terminal values were equal, then alpha-beta
110 R.E. Korf
evaluates only the minimum number of nodes. In the multi-player case, the
situation is quite different. While node ordering can have an effect on the
performance of multi-player pruning, even under optimal ordering the actual
values can limit the effectiveness of the algorithm. In particular, if at a given
level of the tree all the values are identical and more than two components are
non-zero, which is what happens given maxn convergence, then no further
pruning is possible above that level in the tree. Thus, the above result, that
shallow pruning does not reduce the asymptotic branching factor in the average
case, does not depend on the ordering of nodes in the tree.
4. Optimality of alpha-beta
Since Theorem 1 shows that shallow pruning is an optimal directional
algorithm for game trees with more than two players, an obvious question is
what is the optimal directional algorithm for two-player minimax trees? The
answer is standard alpha-beta pruning. While far from surprising, this result
has not appeared in the literature, to our knowledge.
Theorem 3. Every directional minimax search algorithm must evaluate every
leaf node evaluated by alpha-beta under the same ordering.
Proof. Assume the converse. Namely, that there exists a directional minimax
algorithm A, and a minimax tree T, with a leaf node n that is evaluated by
alpha-beta but not by algorithm A. Since both algorithms are directional,
(re)order the nodes in T from left to right in the order they are visited.
Consider the state of the alpha-beta algorithm just before it evaluates node n.
It consists of the path from the root to node n, together with a set of lower
bounds a i on the MAX ancestors of n, and a set of upper bounds/3i on the
MIN ancestors of n. When the algorithm is called on node n, a is the maximum
of the a~, or the greatest lower bound on all of the MAX ancestors of n, and/3
is the minimum of the/3~, or the least upper bound on all of the MIN ancestors
of n, based on all nodes to the left of node n. Since by assumption node n is
evaluated by alpha-beta, a must be strictly less than/3, or node n would have
been pruned. Now, construct a new tree T' from T by removing all branches to
the right of the path from the root to node n, making node n the last frontier
node in a left-to-right traversal of T'. Choose a value x for node n in T' that is
greater than a and less than/3. Since x is strictly greater than the lower bound
on all its MAX ancestors in T', and strictly less than the upper bound on all its
MIN ancestors in T', x is the unique minimax value of tree T'. Since algorithm
A by assumption is a directional algorithm and decides to prune node n in T
based on the nodes to its left, it must also prune node n in T' because T and T'
are identical to the left of node n. But then algorithm A cannot correctly
Multi-player alpha-beta pruning 111
compute the minimax value of T', since it is uniquely determined by node n
which is pruned by A in T'. This contradicts our assumption that algorithm A is
a correct minimax search algorithm. Therefore, every directional minimax
algorithm must evaluate every leaf node evaluated by alpha-beta.
5. Conclusions
We considered the extension of alpha-beta pruning to games with more than
two players. Minimax was generalized to maxn by 2. If we assume that there
is a lower bound on each component of the evaluation function, and an upper
bound on the sum of all components, then shallow alpha-beta pruning is valid,
but not deep pruning. In the best case, this results in significant savings in
computation, but in the average case it does not reduce the asymptotic
branching factor. This implies that the effectiveness of alpha-beta is limited to
the case of two players. We also showed that alpha-beta is an optimal
directional algorithm for two-player games.
Acknowledgement
This research was supported by an NSF Presidential Young Investigator
Award, and NSF Grant IRI-8801939. Thanks to Chris Ferguson and Judea
Pearl for helpful discussions concerning this work, to the anonymous reviewers
for their comments, and to Valerie Aylett for drawing the figures.
References
1 D.E. Knuth and R.E. Moore, An analysis of alpha-beta pruning, Artif. lntell. 6 (1975)
293-326.
2 C.A. Luckhardt and K.B. Irani, An algorithmic solution of N-person games, in: Proceedings
AAAI-86, Philadelphia, PA (1986) 158-162.
3 A. Newell, J.C. Shaw and H.A. Simon, Chess playing programs and the problem of
complexity, IBM J. Res. Dev. 3 (1959) 320-335. Reprinted in: E.A. Feigenbaum and J.
Feldman, eds., Computers and Thought (McGraw-Hill, New York, 1963) 39-70.
4 J. Pearl, Heuristics (Addison-Wesley, Reading, MA, 1984).
5 C.E. Shannon, Programming a computer for playing chess, Philos. Mag. 41 (1950) 256-275.
----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\doc\in\Korf_Multi-player-Alpha-beta-Pruning.pdf -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\doc\out\Figure_1.png -----Artificial Intelligence 48 (1991) 99-111 99
Elsevier
Research Note
Multi-player alpha-beta pruning
Richard E. Korf*
Computer Science Department, University of California, Los Angeles, Los Angeles,
CA 90024, USA
Received August 1989
Revised June 1990
Abstract
Korf, R.E., Multi-player alpha-beta pruning (Research Note), Artificial Intelligence 48
(1991) 99-111.
We consider the generalization of minimax search with alpha-beta pruning to non-coopera-
tive, perfect-information games with more than two players. The minimax algorithm was
generalized in 2 to the maxn algorithm applied to vectors of n-tuples representing the
evaluations for each of the players. If we assume an upper bound on the sum of the
evaluations for each player, and a lower bound on each individual evaluation, then shallow
alpha-beta pruning is possible, but not deep pruning. In the best case, the asymptotic
branching factor is reduced to (1 + 4bv'~b-"s~-3)/2.I n the average case, however, pruning does
not reduce the asymptotic branching factor. Thus, alpha-beta pruning is found to be
effective only in the special case of two-player games. In addition, we show that it is an
optimal directional algorithm for two players.
1. Introduction
Minimax search with alpha-beta pruning is the predominant algorithm
employed by two-player game programs 1, 3, 5. Figure 1 shows a game tree,
where squares represent maximizing nodes and circles correspond to minimiz-
ing nodes, along with its minimax value, bounds on interior nodes, and those
branches pruned by alpha-beta.
In this paper, we consider the generalization of alpha-beta pruning to
non-cooperative, perfect-information games with more than two players. For
example, Chinese Checkers can involve up to six different players moving
* Section 3 of this paper represents a more complete treatment of Section 3 of a paper by the
same author entitled, "Generalized game trees", that appeared in the Proceedings of the Eleventh
International Joint Conference on Artificial Intelligence ( IJCA1-89), Detroit, MI (1989).
0004-3702/91/$03.50 (~ 1991 -- Elsevier Science Publishers B.V.
100 R.E. Korf
/
11 _>15 _<3
111 9 a 15 16 3 b c
Fig. 1. Two-player alpha-beta pruning.
alternately. As another example, Othello can easily be extended to an arbitrary
number of players by having different colored pieces for each player, and
modifying the rules such that whenever a mixed row of opposing pieces is
flanked on both sides by two pieces of the same player, then all the pieces are
captured by the flanking player.
2. Maxn algorithm
Luckhardt and Irani 2 extended minimax to multi-player games, calling the
resulting algorithm max". For typographical convenience we refer to it as
maxn. They assume that the players alternate moves, that each player tries to
maximize his or her perceived return, and is indifferent to the returns of the
remaining players. At the frontier nodes, an evaluation function is applied that
returns an N-tuple of values, where N is the number of players, with each
component corresponding to the estimated merit of the position with respect to
one of the players. Then, the value of each interior node where player i is to
move is the entire N-tuple of the child for which the ith component is a
maximum. Figure 2 shows a maxn tree for three players, with the correspond-
ing maxn values.
(2,8,1)(1,7,2) (5,6,3) (6,5,4) (8,4,S) (7,3,6) (4,2,7) (3,1,8)
Fig. 2. Three-player maxn game tree.
Multi-player alpha-beta pruning 101
More formally, define M(x) to be the static heuristic value of node x, and
M(x, p) to be the backed-up maxn value of node x, given that player p is to
move at node x. Ms(x, p) is the component of M(x, p) that corresponds to the
return for player i. We can then define the maxn value of a node recursively as
follows:
M(x), if x is a frontier node,
M(x, p) = M(xi, p') , otherwise,
such that Mp(xi, p') = max Mp(Xi, p') where x i are the children of node x, p'
is the player that follows player p in the move order, and ties are broken in
favor of the leftmost node.
For example, an evaluation function for multi-player Othello might return
the number of pieces for each player on the board at any given point.
Minimax can be viewed as a special case of maxn for two players. The
evaluation function returns an ordered pair of x and -x, and each player
maximizes its component at its moves.
Luckhardt and Irani 2 observed that at nodes where player i is to move,
only the ith component of the children need be evaluated. At best, this can
produce a constant factor speedup, but it may be no less expensive to compute
all components than to compute only one. They correctly concluded that
without further assumptions on the values of the components, pruning of entire
branches is not possible with more than two players. Thus, they did not explore
such pruning any further.
They used the terms "shallow pruning" and "deep pruning" to refer to their
techniques of avoiding some partial evaluations. Since these terms had previ-
ously been used to describe actual tree pruning in the alpha-beta literature 1,
we will use the original meanings of both these terms, at the cost of inconsis-
tency with Luckhardt's and Irani's terminology 2.
3. Alpha-beta in multi-player games
If there is an upper bound on the sum of all components of a tuple, and
there is a lower bound on the values of each component, then actual tree
pruning is possible. The first condition is a weaker form of the standard
constant-sum assumption, which is in fact required for two-player alpha-beta
pruning. The second is equivalent to assuming a lower bound of zero on each
component, since any other lower bound can be shifted to zero by subtracting
it from every component. Most practical evaluation functions will satisfy both
these conditions, since violating them implies that the value of an individual
component can be unbounded in at least one direction. For example, in the
evaluation function described above for multi-player Othello, no player can
have less than zero pieces on the board, and the total number of pieces on the
102 R.E. Korf
board is the same for all nodes at the same level in the game tree, since exactly
one piece is added at each move.
3.1. Immediate pruning
The simplest kind of pruning possible under these assumptions occurs when
player i is to move, and the ith component of one of its children equals the
upper bound on the sum of all components. In that case, all remaining children
can be pruned, since no child's ith component can exceed the upper bound on
the sum. We will refer to this as immediate pruning. This is equivalent to
situations in the two-player case when a child of a Max node has a value of ~¢,
or a child of a Min node has a value of -~.
3.2. Shallow pruning
A more complex situation is called shallow pruning in the alpha-beta
literature. Figure 3 shows an example of shallow pruning in a three-player
game, where the upper bound on the sum of each component is 9, Note that in
this particular example, the sum of each component is exactly 9, but an upper
bound is all that is required. Evaluating node b results in a lower bound of 3 on
the first component of node a, since player one is to move at node a. This
implies an upper bound on each of the remaining components of 9-3 = 6.
Evaluating node g produces a lower bound of 7 on the second component of
node f, since player two is to move at node f. Similarly, this implies an upper
bound on the remaining components of 9- 7 = 2. Since the upper bound (2)
on the first component of node f is less than or equal to the lower bound on the
first component of node a (3), player one won't choose node f and its
remaining children can be pruned. Similarly, evaluating node i causes its
remaining brothers to be pruned. This is similar to the pruning in the left
subtree of Fig. 1.
The procedure Shallow takes a Node to be evaluated, the Player to move at
that node, and an upper Bound on the component of the player to move, and
returns an N-tuple that is the maxn value of the node. Sum is the global upper
bound on the sum of all components of an N-tuple, and all components are
assumed to be non-negative. Initially, Shallow is called with the root of the
(3,3,3) (4,2,3) (3,1,5) (1,7,1) (1,6,2)
Fig. 3. Shallow pruning in three-player game tree.
Multi-player alpha-beta pruning 103
tree, the player to move at the root, and Sum. Note that the shallow pruning
procedure includes immediate pruning as a special case.
Shallow(Node, Player, Bound)
IF Node is terminal, RETURN static value
Best = Shallow(first Child, next Player, Sum)
FOR each remaining Child
IF BestPlayer > = Bound, RETURN Best
Current = Shallow(Child, next Player, Sum - BestPlayer)
IF CurrentPlayer > BestPlayer, Best = Current
RETURN Best
3.3. Correctness of shallow pruning procedure
Here we establish the correctness of the shallow pruning procedure, in the
sense that it computes the maxn value defined in Section 2.
Theorem 1. If the evaluation of any position for any player is non-negative, and
the sum of all player's evaluations in any given position is less than or equal to
sum, and ties are broken in favor of the leftmost node, then Shallow(a, p, sum)
= M(a, p), for any node a and player p.
Proof. Since the only difference between the maxn procedure and the shallow
pruning procedure is that it doesn't examine certain nodes, it suffices to show
that the pruned nodes can have no effect on the maxn value of the root. There
are two types of pruning implemented in the shallow pruning procedure,
immediate pruning and shallow pruning.
Figure 4 shows the generic case of immediate pruning. Without loss of
generality, we assume that player one is to move at the root. In order for
immediate pruning to occur, Ml(b, 2) = sum, for some child b of node a. Since
all components are assumed to be non-negative, sum is the maximum possible
value of any individual component, and the remaining components must be
zero. Therefore sum = M~(b, 2) = max M~(a i, 2), where a i is a child of node a.
Thus, M(a, 1) = M(b, 2), and the remaining children of node a need not be
examined.
Figure 5 shows the generic case of shallow pruning. Again we assume
without loss of generality that player one is to move at the root, followed by
(sum, 0, 0 .... )
Fig. 4. Generic case of immediate pruning.
104 R.E. Korf
b ~ c/J~-<sum-y, _>y. ... )
x+y~sum d 3L3_J
(,y .... )
Fig. 5. Generic case of shallow pruning.
players two and three in turn. Ml(b, 2) = x, M2(d, 3) = y, and x + y >t sum, in
order for shallow pruning to occur. Since player two is to move at node c,
M2(c, 2) = max M2(c i, 3), where ci is a child of node c. Therefore, M2(c, 2)
M2(d, 3)= y. Since the sum of all components cannot exceed sum, and all
components are non-negative, M~(c, 2) ~< sum - y ~ x, since x + y >1 sum.
Since M l(b, 2) = x I> M 1( c, 2), and ties are broken in favor of leftmost nodes,
M(a, 1) ~ M(c, 2), and node c can have no effect on the maxn value of node a.
Therefore, the remaining children of node c can be pruned.
Since neither immediate nor shallow pruning eliminate nodes that can effect
the maxn value of the root node, and the shallow pruning procedure returns
the maxn value of the nodes it has examined, it correctly calculates the maxn
value of the root node. C
3.4. Failure of deep pruning
In a two-player game, alpha-beta pruning allows an additional type of
pruning known as deep pruning. For example, in Fig. 1, nodes b and c are
pruned based on bounds inherited from their great-great-grandparent, the root
in this case. In general, deep pruning refers to pruning a node based on a
bound inherited from its great-grandparent, or any more distant ancestor. In a
two-player game tree, it can only occur in trees of height four or greater.
Surprisingly, deep pruning does not generalize to more than two players.
Figure 6 illustrates the problem. Again, the upper bound on the sum of each
component is 9. Evaluating node b produces a lower bound of 5 on the first
component of node a and hence an upper bound of 9 - 5 = 4 on the remaining
components. Evaluating node e results in a lower bound of 5 on the third
component of node d and hence an upper bound of 9 - 5 = 4 on the remaining
components. Since the upper bound of 4 on the first component of node d is
less than the lower bound of 5 on the first component of node a, the value of
node d cannot become the value of node a. Thus, we are tempted to prune
node f.
With three players, however, the value of node fcould effect the value of the
root, depending on the value of node g. For example, if the value of node f
Multi-player alpha-beta pruning 105
4 (6, 1, 2)
Je
f
(2, 2, 5) (2, 3, 4) or (3, O, 6)
Fig. 6. Failure of deep pruning for three players.
were (2, 3, 4), the value of node d would be (2, 2,'5), the value of node c
would be (2, 2, 5), and the value of node a would be (5, 2, 2). On the other
hand, if the value of node f were (3, 0, 6), then the value of node d would be
(3, 0, 6), the value of node c would be (6, 1, 2), and the value of node a would
be (6, 1, 2). Thus, even though the value of node fcannot be the maxn value of
the root, it can affect it. Hence, it cannot be pruned.
3.5. Optimality of shallow pruning
Given the failure of deep pruning in this example, is there a more restricted
form of pruning that is valid, or is shallow pruning the best we can do? The
answer is the latter, as expressed by the following theorem:
Theorem 2. Every directional algorithm that computes the maxn value of a
game tree with more than two players must evaluate every terminal node
evaluated by shallow pruning under the same ordering.
A directional algorithm 4 is one in which the order of node evaluation is
independent of the value of the nodes, and once a node is pruned it can never
be revisited. For example, a strictly left-to-right algorithm is directional.
Proof sketch. Since the actual proof given below is somewhat tedious and not
very revealing, we first present here an overview and example of the argument.
The main idea is illustrated by the construction in Fig. 7, which shows a
3-player, 6-level tree. We assume that node n is evaluated by shallow pruning
but pruned by another algorithm. We then show that the value of every node
above it depends on the value of node n. The letters to the left of the path
from the root to node n represent the greatest lower bounds on the compo-
nents corresponding to the player to move at each node. Since by assumption
node n is evaluated by shallow pruning, it must be the case that for any two
106 R.E. Korf
(~a,,) -
" (cid:127)
(, b,)E (a, , )
(,, >c) E" "~ (a+, b, O)
E "~(0, b+, c)
(, ~>e, ) ~ "~ (d, O, c÷)
(,, ~f) "~ (d+, e, O)
(,,0) or (, O, f+) "~ (0, e+, f)
n
Fig. 7. Proof sketch of optimality of shallow pruning.
consecutive bounds x and y, x + y is strictly less than the global upper bound.
Since the decision to skip node n is made before any of the nodes to the right
of the path are examined, we are free to choose any values for these nodes
consistent with the global bounds. For this purpose, x + represents a value
greater than x by an arbitrarily small amount. The reader is encouraged to
assign each of the two alternative values to node n, and then compute the
maxn value of the root, to see that it is different in the two cases. The
propagation of values up the tree can be viewed as a "zipper" effect in the
sense that the original order of the "teeth" (nodes) at the bottom determines
the order of the teeth at the top, even though no individual tooth can move
very far. The formal proof below is by induction on the height of the tree and
generalizes the result to an arbitrary number of players greater than two.
Proof (see Fig. 8). Assume the converse. Namely, that there exists a direction-
al maxn algorithm A, and a minimax tree T with N players, with a leaf node X °
that is evaluated by shallow pruning but not by algorithm A. Since both
shallow pruning and algorithm A are directional, and by assumption visit nodes
in the same order, (re)order the nodes in T from left to right in the order they
are visited, without loss of generality. Let X i refer to the ancestor of node X °
at height i in the tree, on the unique path from the root to X °, and also to its
maxn value, depending on the context. Let X~j refer to the jth component of
the maxn value of node X i. Thus, X i = (Xio ..... X/i ..... X,i, ). Let p(i) be the
player to move at level i in the tree. Thus, Xei¢ i) is the maximum value of Wpi- (I il
among all the children W i i of X i.
Consider the state of the shallow pruning algorithm just before it evaluates
node X °. It consists of the path from the root to node X °, plus a set of bounds
a i, where c~i is equal to the maximum value of
Weic-iI)
among all the children
W i- 1 of X i to the left of X ~ ~. In other words, ai is the greatest lower bound so
far on X*pu). If at any level i, X i-1 has no brothers to its left, then a~ will simply
equal the global lower bound on each component. Without loss of generality,
Multi-player alpha-beta pruning 107
I'--, i1 ~- ~ (V:(:) = °ti + e'
yi-1 = 0)
p(i+2)
i-I
Xp(i) = 0 o._r
(Xp-(li) = U,i + 2e, xi'lp(i+1 ) =0)
Fig. 8. Inductive proof of optimality of shallow pruning.
let the global lower bound on each component be zero and the global upper
bound on the sum of all components be one. Any other bounds can always be
mapped to this range without affecting either algorithm since only relative
values matter.
The assumption that X ° is evaluated by shallow pruning constrains the values
of the ai. First, each of the ai must be strictly less than one. Otherwise, the
remaining children of X ~ would be immediately pruned, pruning X ° as well.
Furthermore, each lower bound a~ induces a corresponding upper bound of
1 - a e on the remaining components of X i. If this upper bound on X i is less
p(i-1)
than or equal to the lower bound of ai_ ~ on X 'p-~~s _~), then the remaining
children of X i would be pruned by shallow pruning, pruning X ° as well. Thus,
for alli1>l, 1-ai>a~_l, or a~+a~_~<l.
Let Y~ be a brother of X i to its right. When either algorithm is about to
consider X °, the values and even the existence of the Y~ nodes is unknown to
the algorithms. Thus, we are free to choose values for the Y~, consistent with
the global bounds, without affecting either algorithm's decision to evaluate or
prune X °. We are also free to choose a value for X ° as well, since the decision
to evaluate or prune it is made before its value is known. The rest of the proof
consists of carefully choosing values for the yi and X ° so that the value of X ~,
for all i, will depend on the value of X °. This will be done by induction on i.
Basis step: i = 0. Trivially, if i = 0, then the value of X ~ = X ° depends on X °.
However, to strengthen the induction hypothesis, we will choose two different
0 0
values for X °. In one case, let Xp(~)= 0, and in the other case, let Xp(~)=
a~ +2e, and X°(2~ = 0, where e is an arbitrarily small positive value. In
particular, choose e such that ai + a~_ Z + 3e < 1, for all i/> 1.
Induction step. Now assume that for all values up to and including i- 1,
i-1 i-1 i-I
Xp(i) O, or Xp(i) = a i -1- 2e and Xp(~+~) = O, depending on the value of X °.
=
1(18 R.E. Korf
Choose the value of Y' ~ as follows: yip-l ,) = ai + e and Y'p (i+l) = O{i~ I +2E and
y,-~ = 0, Note that the sum of the y i ~ components is ai + e + ~i, ~ + 2e +
pIi +2 )
0 = a i + ai+ t +3e< 1, since a: + ai+ 1 < 1, and e was chosen to satisfy this
property. Thus, this is a legal set of components for the value of Yi 1.
Now there are two cases to consider: In case I, Xif,<~ = 0. In that case, since
yi-I
Ypi ( i)I = ai + 6 ) oLi ~ (} = Xpi ( i)I , is the best child of X ~, and the value of X ~
equals the value of Y' ~. In particular, Xpio +~ ) = Y'~ ,1~~ +1~=o~+~ +2e and
Xpi( i+2) = Ypi (iI+ 2)= 0. In case II, Xpi o I) = ai + 2e and Xpi ~ +~ l ) =0. In that case,
since X i p~ ~-I = ai + 2e > ~i + e = Y ip-1, )> a , xi-I is the best child of X ~, and the
• i i 1
value of X i equals the value of X ~ i In particular, Xp~+!) = X j,,+ i i = 0. Thus,
i
in one case, Xp(i+l) =0, and in the other case, Xp(i+l)=OLi+l+26 and
XJp~+2) = 0. This is the induction hypothesis for i.
Thus, the induction hypothesis is true for i, given that it is true for i- 1.
Therefore, for such a tree of any height, the maxn value of the root depends on
X". But if algorithm A prunes X °, it cannot determine the maxn value of the
root. This contradicts our assumption that algorithm A works for any maxn
tree. Therefore, every directional algorithm that computes the maxn value of a
game tree with more than two players must evaluate every terminal node
evaluated by shallow pruning.
3.6. Best-case performance
How effective is shallow pruning in the best case? For simplicity, we will
exclude immediate pruning by assuming that no one component can equal the
upper bound on the sum. The best-case analysis of shallow pruning is indepen-
dent of the number of players and was done by Knuth and Moore 1 for two
players.
In order to evaluate a node in the best case, one child must be evaluated,
and then evaluating one grandchild of each remaining child will cause the
remaining grandchildren to be pruned (see Fig. 3). Thus, If F(d) is the number
of leaf nodes generated to evaluate a tree of depth d with branching factor b in
the best case, then F(d) = F(d - 1) + (b - 1)*F(d - 2). Since a tree of depth
zero is a single node, and a tree of depth one requires all children to be
evaluated, the initial conditions are F(0)= 1 and F(1)= b. Note that in a
binary tree, F(d) is the familiar Fibonacci sequence. The solution to the
general recurrence has an asymptotic branching factor of ½( 1 + x/Tb-- 3). For
large values of b, this approaches ~ which is the best-case performance of full
two-player alpha-beta pruning.
3.7. Average-case performance
Knuth and Moore 1 also determined that in the average case, the asymp-
totic branching factor of two-player shallow pruning is approximately b/log b.
They assumed independent, distinct leaf values.
Multi-player alpha-beta pruning 109
In the case of multiple players, however, our model of the evaluation
function must have a lower bound on each component and an upper bound on
their sum. For simplicity, assume that the lower bound is zero and that the sum
is exactly one. Thus, we need a way of randomly choosing n-tuples such that
each component is identically distributed between zero and one, and the sum
of all components is one. One way to do this is by cutting the zero-one interval
in n- 1 places, with each cut point independently and identically distributed
from zero to one, and using the n resulting segments as the components of the
n-tuple. Another way is to choose n values independently and identically
distributed from zero to one, and then divide each component by the sum of all
of them. Furthermore, we assume that each tuple is generated independently
of the others.
Under this average-case model, the asymptotic branching factor of shallow
pruning with more than two players is simply b, the brute-force branching
factor. The analysis relies on the minimax convergence theorem 4, which
holds for two-player minimax trees. This surprising phenomenon is that if the
leaf values are chosen independently from the same distribution, the variance
of the root value decreases with increasing height of the tree, and in the limit
of infinite height, the root value can be predicted with probability one. The
actual limiting value depends on the leaf distribution and also on which player
moves last in the tree, but the convergence does not. Based on empirical
studies, we conjecture that minimax convergence applies to maxn trees as well.
In order for pruning to take place, the lower bound on one component must
be greater than or equal to its upper bound, which equals one minus the lower
bound on another component. Thus, pruning only takes place when the sum of
the lower bounds on two different components is greater than or equal to one.
In order for this to occur in the limiting value, the values of the remaining
components must be zero, since the sum of the two components in question is
one. This cannot happen in the limiting value, assuming continuous terminal
values. Thus, while pruning occurs at low levels of the tree, at higher levels it
becomes increasingly rare, and in the limit of infinite depth, it disappears
entirely. Thus, the asymptotic branching factor is simply b.
The shallow pruning algorithm has been implemented and its efficiency
tested using the above model of independent and identically distributed cut
points. Not only does the effective branching factor converge to b, but the
convergence is rapid enough that even for small trees, the branching factor is
very close to b. Thus, in practice, alpha-beta is not effective for multi-player
trees.
In the case of two-player alpha-beta, the performance of the algorithm is
purely a function of the order of the terminal values and not the values
themselves. In particular, Knuth and Moore 1 show that for any set of
terminal values there is an ordering for which alpha-beta achieves its best-case
performance. For example, if all terminal values were equal, then alpha-beta
110 R.E. Korf
evaluates only the minimum number of nodes. In the multi-player case, the
situation is quite different. While node ordering can have an effect on the
performance of multi-player pruning, even under optimal ordering the actual
values can limit the effectiveness of the algorithm. In particular, if at a given
level of the tree all the values are identical and more than two components are
non-zero, which is what happens given maxn convergence, then no further
pruning is possible above that level in the tree. Thus, the above result, that
shallow pruning does not reduce the asymptotic branching factor in the average
case, does not depend on the ordering of nodes in the tree.
4. Optimality of alpha-beta
Since Theorem 1 shows that shallow pruning is an optimal directional
algorithm for game trees with more than two players, an obvious question is
what is the optimal directional algorithm for two-player minimax trees? The
answer is standard alpha-beta pruning. While far from surprising, this result
has not appeared in the literature, to our knowledge.
Theorem 3. Every directional minimax search algorithm must evaluate every
leaf node evaluated by alpha-beta under the same ordering.
Proof. Assume the converse. Namely, that there exists a directional minimax
algorithm A, and a minimax tree T, with a leaf node n that is evaluated by
alpha-beta but not by algorithm A. Since both algorithms are directional,
(re)order the nodes in T from left to right in the order they are visited.
Consider the state of the alpha-beta algorithm just before it evaluates node n.
It consists of the path from the root to node n, together with a set of lower
bounds a i on the MAX ancestors of n, and a set of upper bounds/3i on the
MIN ancestors of n. When the algorithm is called on node n, a is the maximum
of the a~, or the greatest lower bound on all of the MAX ancestors of n, and/3
is the minimum of the/3~, or the least upper bound on all of the MIN ancestors
of n, based on all nodes to the left of node n. Since by assumption node n is
evaluated by alpha-beta, a must be strictly less than/3, or node n would have
been pruned. Now, construct a new tree T' from T by removing all branches to
the right of the path from the root to node n, making node n the last frontier
node in a left-to-right traversal of T'. Choose a value x for node n in T' that is
greater than a and less than/3. Since x is strictly greater than the lower bound
on all its MAX ancestors in T', and strictly less than the upper bound on all its
MIN ancestors in T', x is the unique minimax value of tree T'. Since algorithm
A by assumption is a directional algorithm and decides to prune node n in T
based on the nodes to its left, it must also prune node n in T' because T and T'
are identical to the left of node n. But then algorithm A cannot correctly
Multi-player alpha-beta pruning 111
compute the minimax value of T', since it is uniquely determined by node n
which is pruned by A in T'. This contradicts our assumption that algorithm A is
a correct minimax search algorithm. Therefore, every directional minimax
algorithm must evaluate every leaf node evaluated by alpha-beta.
5. Conclusions
We considered the extension of alpha-beta pruning to games with more than
two players. Minimax was generalized to maxn by 2. If we assume that there
is a lower bound on each component of the evaluation function, and an upper
bound on the sum of all components, then shallow alpha-beta pruning is valid,
but not deep pruning. In the best case, this results in significant savings in
computation, but in the average case it does not reduce the asymptotic
branching factor. This implies that the effectiveness of alpha-beta is limited to
the case of two players. We also showed that alpha-beta is an optimal
directional algorithm for two-player games.
Acknowledgement
This research was supported by an NSF Presidential Young Investigator
Award, and NSF Grant IRI-8801939. Thanks to Chris Ferguson and Judea
Pearl for helpful discussions concerning this work, to the anonymous reviewers
for their comments, and to Valerie Aylett for drawing the figures.
References
1 D.E. Knuth and R.E. Moore, An analysis of alpha-beta pruning, Artif. lntell. 6 (1975)
293-326.
2 C.A. Luckhardt and K.B. Irani, An algorithmic solution of N-person games, in: Proceedings
AAAI-86, Philadelphia, PA (1986) 158-162.
3 A. Newell, J.C. Shaw and H.A. Simon, Chess playing programs and the problem of
complexity, IBM J. Res. Dev. 3 (1959) 320-335. Reprinted in: E.A. Feigenbaum and J.
Feldman, eds., Computers and Thought (McGraw-Hill, New York, 1963) 39-70.
4 J. Pearl, Heuristics (Addison-Wesley, Reading, MA, 1984).
5 C.E. Shannon, Programming a computer for playing chess, Philos. Mag. 41 (1950) 256-275.
----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\doc\out\Figure_1.png -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\doc\out\report.pdf -----INFO-F-311: Artificial Intelligence - Project 2:
Recherche adversariale
Bourgeois No´e
2023 October 29
Contents
1 Introduction 2
2 Better evaluation function 2
2.1 get available actions ordered . . . . . . . . . . . . . . . . . . 2
2.2 Method: transition . . . . . . . . . . . . . . . . . . . . . . . . . 2
3 Results 3
3.1 Fewer Nodes with Better Evaluation . . . . . . . . . . . . . . . . 3
3.1.1 First Map . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
3.1.2 Second Map . . . . . . . . . . . . . . . . . . . . . . . . . . 4
3.1.3 Third Map . . . . . . . . . . . . . . . . . . . . . . . . . . 5
3.2 Fourth Map . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
3.3 Fifth Map . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
4 Discussion 8
4.1 Limitations of High-Level Heuristics . . . . . . . . . . . . . . . . 8
4.2 Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
5 ChatGPT Usage 8
1
1 Introduction
This report outlines the application of adversarial search techniques in graph
search problems. For references, please refer to the project instructions and the
project 1.
2 Better evaluation function
2.1 get available actions ordered
Algorithm Time Complexity
Minimax O(bm)
Alpha-Beta O(bm/2) to O(bm)
Alpha-Beta with Perfect Ordering O(bm/2)
IntheBetterValueFunctionclass,thismethodorderstheavailableactions
based on certain conditions:
• Moves the action STAY to the end of the list.
• If not all gems are collected, it prioritizes actions that lead to a gem.
• If the agent’s path intersects with dangerous lasers, it de-prioritizes such
actions.
2.2 Method: transition
In this method, the value of a state is changed based on multiple factors:
• The distance of agents to gems and exit points.
• Whether all gems are collected or not.
2
Figure 1: Map 1
3 Results
3.1 Fewer Nodes with Better Evaluation
3.1.1 First Map
Here we can see that, compared to alpha beta, the number of nodes is re-
ducedby6. Additionally,comparedtominimax,thenumberofnodesisreduced
by 11. This is what we expected, because we have a better evaluation function.
3
Figure 2: Map 1 results
3.1.2 Second Map
Again, we can see that, compared to alpha beta, the number of nodes is
reduced and compared to minimax, the number of nodes is reduced by factor 2.
4
Figure 3: First map’s Better Evaluation Markov Decisional Process Tree
3.1.3 Third Map
Here again, with a slightly bigger map and whith a much bigger difference
with factors of 2 and 7.
5
Figure 4: Map 2
3.2 Fourth Map
This case shows that better evaluation function does not always mean fewer
nodes.
6
Figure 5: Map 2 results
3.3 Fifth Map
Again, better evaluation function is not the best.
7
Figure 6: Map 3
4 Discussion
4.1 Limitations of High-Level Heuristics
While the use of high-level heuristics in action ordering generally improves per-
formance, it is not always fine enough for every case. As a result, in some
cases, the BetterValueFunction may not actually result in fewer nodes being
expanded compared to basic evaluation functions.
4.2 Future Work
Recent advancements in adversarial search algorithms have started to incorpo-
rate machine learning techniques to dynamically adapt the heuristics used for
action selection. This represents a potential avenue for further improving the
performance of our algorithms.
5 ChatGPT Usage
The project was made with ChatGPT.
8
Figure 7: Map 3 results
Figure 8: Map 4
9
Figure 9: Map 4 results
Figure 10: Map 5
10
Figure 11: Map 5 results
11
----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\doc\out\report.pdf -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\doc\out\media\20231029-134819.png -----INFO-F-311: Artificial Intelligence - Project 2:
Recherche adversariale
Bourgeois No´e
2023 October 29
Contents
1 Introduction 2
2 Better evaluation function 2
2.1 get available actions ordered . . . . . . . . . . . . . . . . . . 2
2.2 Method: transition . . . . . . . . . . . . . . . . . . . . . . . . . 2
3 Results 3
3.1 Fewer Nodes with Better Evaluation . . . . . . . . . . . . . . . . 3
3.1.1 First Map . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
3.1.2 Second Map . . . . . . . . . . . . . . . . . . . . . . . . . . 4
3.1.3 Third Map . . . . . . . . . . . . . . . . . . . . . . . . . . 5
3.2 Fourth Map . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
3.3 Fifth Map . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
4 Discussion 8
4.1 Limitations of High-Level Heuristics . . . . . . . . . . . . . . . . 8
4.2 Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
5 ChatGPT Usage 8
1
1 Introduction
This report outlines the application of adversarial search techniques in graph
search problems. For references, please refer to the project instructions and the
project 1.
2 Better evaluation function
2.1 get available actions ordered
Algorithm Time Complexity
Minimax O(bm)
Alpha-Beta O(bm/2) to O(bm)
Alpha-Beta with Perfect Ordering O(bm/2)
IntheBetterValueFunctionclass,thismethodorderstheavailableactions
based on certain conditions:
• Moves the action STAY to the end of the list.
• If not all gems are collected, it prioritizes actions that lead to a gem.
• If the agent’s path intersects with dangerous lasers, it de-prioritizes such
actions.
2.2 Method: transition
In this method, the value of a state is changed based on multiple factors:
• The distance of agents to gems and exit points.
• Whether all gems are collected or not.
2
Figure 1: Map 1
3 Results
3.1 Fewer Nodes with Better Evaluation
3.1.1 First Map
Here we can see that, compared to alpha beta, the number of nodes is re-
ducedby6. Additionally,comparedtominimax,thenumberofnodesisreduced
by 11. This is what we expected, because we have a better evaluation function.
3
Figure 2: Map 1 results
3.1.2 Second Map
Again, we can see that, compared to alpha beta, the number of nodes is
reduced and compared to minimax, the number of nodes is reduced by factor 2.
4
Figure 3: First map’s Better Evaluation Markov Decisional Process Tree
3.1.3 Third Map
Here again, with a slightly bigger map and whith a much bigger difference
with factors of 2 and 7.
5
Figure 4: Map 2
3.2 Fourth Map
This case shows that better evaluation function does not always mean fewer
nodes.
6
Figure 5: Map 2 results
3.3 Fifth Map
Again, better evaluation function is not the best.
7
Figure 6: Map 3
4 Discussion
4.1 Limitations of High-Level Heuristics
While the use of high-level heuristics in action ordering generally improves per-
formance, it is not always fine enough for every case. As a result, in some
cases, the BetterValueFunction may not actually result in fewer nodes being
expanded compared to basic evaluation functions.
4.2 Future Work
Recent advancements in adversarial search algorithms have started to incorpo-
rate machine learning techniques to dynamically adapt the heuristics used for
action selection. This represents a potential avenue for further improving the
performance of our algorithms.
5 ChatGPT Usage
The project was made with ChatGPT.
8
Figure 7: Map 3 results
Figure 8: Map 4
9
Figure 9: Map 4 results
Figure 10: Map 5
10
Figure 11: Map 5 results
11
----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\doc\out\media\20231029-134819.png -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\doc\out\media\20231029-135613.png -----INFO-F-311: Artificial Intelligence - Project 2:
Recherche adversariale
Bourgeois No´e
2023 October 29
Contents
1 Introduction 2
2 Better evaluation function 2
2.1 get available actions ordered . . . . . . . . . . . . . . . . . . 2
2.2 Method: transition . . . . . . . . . . . . . . . . . . . . . . . . . 2
3 Results 3
3.1 Fewer Nodes with Better Evaluation . . . . . . . . . . . . . . . . 3
3.1.1 First Map . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
3.1.2 Second Map . . . . . . . . . . . . . . . . . . . . . . . . . . 4
3.1.3 Third Map . . . . . . . . . . . . . . . . . . . . . . . . . . 5
3.2 Fourth Map . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
3.3 Fifth Map . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
4 Discussion 8
4.1 Limitations of High-Level Heuristics . . . . . . . . . . . . . . . . 8
4.2 Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
5 ChatGPT Usage 8
1
1 Introduction
This report outlines the application of adversarial search techniques in graph
search problems. For references, please refer to the project instructions and the
project 1.
2 Better evaluation function
2.1 get available actions ordered
Algorithm Time Complexity
Minimax O(bm)
Alpha-Beta O(bm/2) to O(bm)
Alpha-Beta with Perfect Ordering O(bm/2)
IntheBetterValueFunctionclass,thismethodorderstheavailableactions
based on certain conditions:
• Moves the action STAY to the end of the list.
• If not all gems are collected, it prioritizes actions that lead to a gem.
• If the agent’s path intersects with dangerous lasers, it de-prioritizes such
actions.
2.2 Method: transition
In this method, the value of a state is changed based on multiple factors:
• The distance of agents to gems and exit points.
• Whether all gems are collected or not.
2
Figure 1: Map 1
3 Results
3.1 Fewer Nodes with Better Evaluation
3.1.1 First Map
Here we can see that, compared to alpha beta, the number of nodes is re-
ducedby6. Additionally,comparedtominimax,thenumberofnodesisreduced
by 11. This is what we expected, because we have a better evaluation function.
3
Figure 2: Map 1 results
3.1.2 Second Map
Again, we can see that, compared to alpha beta, the number of nodes is
reduced and compared to minimax, the number of nodes is reduced by factor 2.
4
Figure 3: First map’s Better Evaluation Markov Decisional Process Tree
3.1.3 Third Map
Here again, with a slightly bigger map and whith a much bigger difference
with factors of 2 and 7.
5
Figure 4: Map 2
3.2 Fourth Map
This case shows that better evaluation function does not always mean fewer
nodes.
6
Figure 5: Map 2 results
3.3 Fifth Map
Again, better evaluation function is not the best.
7
Figure 6: Map 3
4 Discussion
4.1 Limitations of High-Level Heuristics
While the use of high-level heuristics in action ordering generally improves per-
formance, it is not always fine enough for every case. As a result, in some
cases, the BetterValueFunction may not actually result in fewer nodes being
expanded compared to basic evaluation functions.
4.2 Future Work
Recent advancements in adversarial search algorithms have started to incorpo-
rate machine learning techniques to dynamically adapt the heuristics used for
action selection. This represents a potential avenue for further improving the
performance of our algorithms.
5 ChatGPT Usage
The project was made with ChatGPT.
8
Figure 7: Map 3 results
Figure 8: Map 4
9
Figure 9: Map 4 results
Figure 10: Map 5
10
Figure 11: Map 5 results
11
----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\doc\out\media\20231029-135613.png -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\doc\out\media\map2023_10_29_13_48_15.png -----INFO-F-311: Artificial Intelligence - Project 2:
Recherche adversariale
Bourgeois No´e
2023 October 29
Contents
1 Introduction 2
2 Better evaluation function 2
2.1 get available actions ordered . . . . . . . . . . . . . . . . . . 2
2.2 Method: transition . . . . . . . . . . . . . . . . . . . . . . . . . 2
3 Results 3
3.1 Fewer Nodes with Better Evaluation . . . . . . . . . . . . . . . . 3
3.1.1 First Map . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
3.1.2 Second Map . . . . . . . . . . . . . . . . . . . . . . . . . . 4
3.1.3 Third Map . . . . . . . . . . . . . . . . . . . . . . . . . . 5
3.2 Fourth Map . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
3.3 Fifth Map . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
4 Discussion 8
4.1 Limitations of High-Level Heuristics . . . . . . . . . . . . . . . . 8
4.2 Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
5 ChatGPT Usage 8
1
1 Introduction
This report outlines the application of adversarial search techniques in graph
search problems. For references, please refer to the project instructions and the
project 1.
2 Better evaluation function
2.1 get available actions ordered
Algorithm Time Complexity
Minimax O(bm)
Alpha-Beta O(bm/2) to O(bm)
Alpha-Beta with Perfect Ordering O(bm/2)
IntheBetterValueFunctionclass,thismethodorderstheavailableactions
based on certain conditions:
• Moves the action STAY to the end of the list.
• If not all gems are collected, it prioritizes actions that lead to a gem.
• If the agent’s path intersects with dangerous lasers, it de-prioritizes such
actions.
2.2 Method: transition
In this method, the value of a state is changed based on multiple factors:
• The distance of agents to gems and exit points.
• Whether all gems are collected or not.
2
Figure 1: Map 1
3 Results
3.1 Fewer Nodes with Better Evaluation
3.1.1 First Map
Here we can see that, compared to alpha beta, the number of nodes is re-
ducedby6. Additionally,comparedtominimax,thenumberofnodesisreduced
by 11. This is what we expected, because we have a better evaluation function.
3
Figure 2: Map 1 results
3.1.2 Second Map
Again, we can see that, compared to alpha beta, the number of nodes is
reduced and compared to minimax, the number of nodes is reduced by factor 2.
4
Figure 3: First map’s Better Evaluation Markov Decisional Process Tree
3.1.3 Third Map
Here again, with a slightly bigger map and whith a much bigger difference
with factors of 2 and 7.
5
Figure 4: Map 2
3.2 Fourth Map
This case shows that better evaluation function does not always mean fewer
nodes.
6
Figure 5: Map 2 results
3.3 Fifth Map
Again, better evaluation function is not the best.
7
Figure 6: Map 3
4 Discussion
4.1 Limitations of High-Level Heuristics
While the use of high-level heuristics in action ordering generally improves per-
formance, it is not always fine enough for every case. As a result, in some
cases, the BetterValueFunction may not actually result in fewer nodes being
expanded compared to basic evaluation functions.
4.2 Future Work
Recent advancements in adversarial search algorithms have started to incorpo-
rate machine learning techniques to dynamically adapt the heuristics used for
action selection. This represents a potential avenue for further improving the
performance of our algorithms.
5 ChatGPT Usage
The project was made with ChatGPT.
8
Figure 7: Map 3 results
Figure 8: Map 4
9
Figure 9: Map 4 results
Figure 10: Map 5
10
Figure 11: Map 5 results
11
----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\doc\out\media\map2023_10_29_13_48_15.png -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\doc\out\media\map2023_10_29_13_49_35.png -----INFO-F-311: Artificial Intelligence - Project 2:
Recherche adversariale
Bourgeois No´e
2023 October 29
Contents
1 Introduction 2
2 Better evaluation function 2
2.1 get available actions ordered . . . . . . . . . . . . . . . . . . 2
2.2 Method: transition . . . . . . . . . . . . . . . . . . . . . . . . . 2
3 Results 3
3.1 Fewer Nodes with Better Evaluation . . . . . . . . . . . . . . . . 3
3.1.1 First Map . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
3.1.2 Second Map . . . . . . . . . . . . . . . . . . . . . . . . . . 4
3.1.3 Third Map . . . . . . . . . . . . . . . . . . . . . . . . . . 5
3.2 Fourth Map . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
3.3 Fifth Map . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
4 Discussion 8
4.1 Limitations of High-Level Heuristics . . . . . . . . . . . . . . . . 8
4.2 Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
5 ChatGPT Usage 8
1
1 Introduction
This report outlines the application of adversarial search techniques in graph
search problems. For references, please refer to the project instructions and the
project 1.
2 Better evaluation function
2.1 get available actions ordered
Algorithm Time Complexity
Minimax O(bm)
Alpha-Beta O(bm/2) to O(bm)
Alpha-Beta with Perfect Ordering O(bm/2)
IntheBetterValueFunctionclass,thismethodorderstheavailableactions
based on certain conditions:
• Moves the action STAY to the end of the list.
• If not all gems are collected, it prioritizes actions that lead to a gem.
• If the agent’s path intersects with dangerous lasers, it de-prioritizes such
actions.
2.2 Method: transition
In this method, the value of a state is changed based on multiple factors:
• The distance of agents to gems and exit points.
• Whether all gems are collected or not.
2
Figure 1: Map 1
3 Results
3.1 Fewer Nodes with Better Evaluation
3.1.1 First Map
Here we can see that, compared to alpha beta, the number of nodes is re-
ducedby6. Additionally,comparedtominimax,thenumberofnodesisreduced
by 11. This is what we expected, because we have a better evaluation function.
3
Figure 2: Map 1 results
3.1.2 Second Map
Again, we can see that, compared to alpha beta, the number of nodes is
reduced and compared to minimax, the number of nodes is reduced by factor 2.
4
Figure 3: First map’s Better Evaluation Markov Decisional Process Tree
3.1.3 Third Map
Here again, with a slightly bigger map and whith a much bigger difference
with factors of 2 and 7.
5
Figure 4: Map 2
3.2 Fourth Map
This case shows that better evaluation function does not always mean fewer
nodes.
6
Figure 5: Map 2 results
3.3 Fifth Map
Again, better evaluation function is not the best.
7
Figure 6: Map 3
4 Discussion
4.1 Limitations of High-Level Heuristics
While the use of high-level heuristics in action ordering generally improves per-
formance, it is not always fine enough for every case. As a result, in some
cases, the BetterValueFunction may not actually result in fewer nodes being
expanded compared to basic evaluation functions.
4.2 Future Work
Recent advancements in adversarial search algorithms have started to incorpo-
rate machine learning techniques to dynamically adapt the heuristics used for
action selection. This represents a potential avenue for further improving the
performance of our algorithms.
5 ChatGPT Usage
The project was made with ChatGPT.
8
Figure 7: Map 3 results
Figure 8: Map 4
9
Figure 9: Map 4 results
Figure 10: Map 5
10
Figure 11: Map 5 results
11
----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\doc\out\media\map2023_10_29_13_49_35.png -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\doc\out\media\map2023_10_29_13_53_25.png -----INFO-F-311: Artificial Intelligence - Project 2:
Recherche adversariale
Bourgeois No´e
2023 October 29
Contents
1 Introduction 2
2 Better evaluation function 2
2.1 get available actions ordered . . . . . . . . . . . . . . . . . . 2
2.2 Method: transition . . . . . . . . . . . . . . . . . . . . . . . . . 2
3 Results 3
3.1 Fewer Nodes with Better Evaluation . . . . . . . . . . . . . . . . 3
3.1.1 First Map . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
3.1.2 Second Map . . . . . . . . . . . . . . . . . . . . . . . . . . 4
3.1.3 Third Map . . . . . . . . . . . . . . . . . . . . . . . . . . 5
3.2 Fourth Map . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
3.3 Fifth Map . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
4 Discussion 8
4.1 Limitations of High-Level Heuristics . . . . . . . . . . . . . . . . 8
4.2 Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
5 ChatGPT Usage 8
1
1 Introduction
This report outlines the application of adversarial search techniques in graph
search problems. For references, please refer to the project instructions and the
project 1.
2 Better evaluation function
2.1 get available actions ordered
Algorithm Time Complexity
Minimax O(bm)
Alpha-Beta O(bm/2) to O(bm)
Alpha-Beta with Perfect Ordering O(bm/2)
IntheBetterValueFunctionclass,thismethodorderstheavailableactions
based on certain conditions:
• Moves the action STAY to the end of the list.
• If not all gems are collected, it prioritizes actions that lead to a gem.
• If the agent’s path intersects with dangerous lasers, it de-prioritizes such
actions.
2.2 Method: transition
In this method, the value of a state is changed based on multiple factors:
• The distance of agents to gems and exit points.
• Whether all gems are collected or not.
2
Figure 1: Map 1
3 Results
3.1 Fewer Nodes with Better Evaluation
3.1.1 First Map
Here we can see that, compared to alpha beta, the number of nodes is re-
ducedby6. Additionally,comparedtominimax,thenumberofnodesisreduced
by 11. This is what we expected, because we have a better evaluation function.
3
Figure 2: Map 1 results
3.1.2 Second Map
Again, we can see that, compared to alpha beta, the number of nodes is
reduced and compared to minimax, the number of nodes is reduced by factor 2.
4
Figure 3: First map’s Better Evaluation Markov Decisional Process Tree
3.1.3 Third Map
Here again, with a slightly bigger map and whith a much bigger difference
with factors of 2 and 7.
5
Figure 4: Map 2
3.2 Fourth Map
This case shows that better evaluation function does not always mean fewer
nodes.
6
Figure 5: Map 2 results
3.3 Fifth Map
Again, better evaluation function is not the best.
7
Figure 6: Map 3
4 Discussion
4.1 Limitations of High-Level Heuristics
While the use of high-level heuristics in action ordering generally improves per-
formance, it is not always fine enough for every case. As a result, in some
cases, the BetterValueFunction may not actually result in fewer nodes being
expanded compared to basic evaluation functions.
4.2 Future Work
Recent advancements in adversarial search algorithms have started to incorpo-
rate machine learning techniques to dynamically adapt the heuristics used for
action selection. This represents a potential avenue for further improving the
performance of our algorithms.
5 ChatGPT Usage
The project was made with ChatGPT.
8
Figure 7: Map 3 results
Figure 8: Map 4
9
Figure 9: Map 4 results
Figure 10: Map 5
10
Figure 11: Map 5 results
11
----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\doc\out\media\map2023_10_29_13_53_25.png -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\doc\out\media\map2023_10_29_13_55_47.png -----INFO-F-311: Artificial Intelligence - Project 2:
Recherche adversariale
Bourgeois No´e
2023 October 29
Contents
1 Introduction 2
2 Better evaluation function 2
2.1 get available actions ordered . . . . . . . . . . . . . . . . . . 2
2.2 Method: transition . . . . . . . . . . . . . . . . . . . . . . . . . 2
3 Results 3
3.1 Fewer Nodes with Better Evaluation . . . . . . . . . . . . . . . . 3
3.1.1 First Map . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
3.1.2 Second Map . . . . . . . . . . . . . . . . . . . . . . . . . . 4
3.1.3 Third Map . . . . . . . . . . . . . . . . . . . . . . . . . . 5
3.2 Fourth Map . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
3.3 Fifth Map . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
4 Discussion 8
4.1 Limitations of High-Level Heuristics . . . . . . . . . . . . . . . . 8
4.2 Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
5 ChatGPT Usage 8
1
1 Introduction
This report outlines the application of adversarial search techniques in graph
search problems. For references, please refer to the project instructions and the
project 1.
2 Better evaluation function
2.1 get available actions ordered
Algorithm Time Complexity
Minimax O(bm)
Alpha-Beta O(bm/2) to O(bm)
Alpha-Beta with Perfect Ordering O(bm/2)
IntheBetterValueFunctionclass,thismethodorderstheavailableactions
based on certain conditions:
• Moves the action STAY to the end of the list.
• If not all gems are collected, it prioritizes actions that lead to a gem.
• If the agent’s path intersects with dangerous lasers, it de-prioritizes such
actions.
2.2 Method: transition
In this method, the value of a state is changed based on multiple factors:
• The distance of agents to gems and exit points.
• Whether all gems are collected or not.
2
Figure 1: Map 1
3 Results
3.1 Fewer Nodes with Better Evaluation
3.1.1 First Map
Here we can see that, compared to alpha beta, the number of nodes is re-
ducedby6. Additionally,comparedtominimax,thenumberofnodesisreduced
by 11. This is what we expected, because we have a better evaluation function.
3
Figure 2: Map 1 results
3.1.2 Second Map
Again, we can see that, compared to alpha beta, the number of nodes is
reduced and compared to minimax, the number of nodes is reduced by factor 2.
4
Figure 3: First map’s Better Evaluation Markov Decisional Process Tree
3.1.3 Third Map
Here again, with a slightly bigger map and whith a much bigger difference
with factors of 2 and 7.
5
Figure 4: Map 2
3.2 Fourth Map
This case shows that better evaluation function does not always mean fewer
nodes.
6
Figure 5: Map 2 results
3.3 Fifth Map
Again, better evaluation function is not the best.
7
Figure 6: Map 3
4 Discussion
4.1 Limitations of High-Level Heuristics
While the use of high-level heuristics in action ordering generally improves per-
formance, it is not always fine enough for every case. As a result, in some
cases, the BetterValueFunction may not actually result in fewer nodes being
expanded compared to basic evaluation functions.
4.2 Future Work
Recent advancements in adversarial search algorithms have started to incorpo-
rate machine learning techniques to dynamically adapt the heuristics used for
action selection. This represents a potential avenue for further improving the
performance of our algorithms.
5 ChatGPT Usage
The project was made with ChatGPT.
8
Figure 7: Map 3 results
Figure 8: Map 4
9
Figure 9: Map 4 results
Figure 10: Map 5
10
Figure 11: Map 5 results
11
----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\doc\out\media\map2023_10_29_13_55_47.png -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\doc\out\media\map2023_10_29_13_56_42.png -----INFO-F-311: Artificial Intelligence - Project 2:
Recherche adversariale
Bourgeois No´e
2023 October 29
Contents
1 Introduction 2
2 Better evaluation function 2
2.1 get available actions ordered . . . . . . . . . . . . . . . . . . 2
2.2 Method: transition . . . . . . . . . . . . . . . . . . . . . . . . . 2
3 Results 3
3.1 Fewer Nodes with Better Evaluation . . . . . . . . . . . . . . . . 3
3.1.1 First Map . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
3.1.2 Second Map . . . . . . . . . . . . . . . . . . . . . . . . . . 4
3.1.3 Third Map . . . . . . . . . . . . . . . . . . . . . . . . . . 5
3.2 Fourth Map . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
3.3 Fifth Map . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
4 Discussion 8
4.1 Limitations of High-Level Heuristics . . . . . . . . . . . . . . . . 8
4.2 Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
5 ChatGPT Usage 8
1
1 Introduction
This report outlines the application of adversarial search techniques in graph
search problems. For references, please refer to the project instructions and the
project 1.
2 Better evaluation function
2.1 get available actions ordered
Algorithm Time Complexity
Minimax O(bm)
Alpha-Beta O(bm/2) to O(bm)
Alpha-Beta with Perfect Ordering O(bm/2)
IntheBetterValueFunctionclass,thismethodorderstheavailableactions
based on certain conditions:
• Moves the action STAY to the end of the list.
• If not all gems are collected, it prioritizes actions that lead to a gem.
• If the agent’s path intersects with dangerous lasers, it de-prioritizes such
actions.
2.2 Method: transition
In this method, the value of a state is changed based on multiple factors:
• The distance of agents to gems and exit points.
• Whether all gems are collected or not.
2
Figure 1: Map 1
3 Results
3.1 Fewer Nodes with Better Evaluation
3.1.1 First Map
Here we can see that, compared to alpha beta, the number of nodes is re-
ducedby6. Additionally,comparedtominimax,thenumberofnodesisreduced
by 11. This is what we expected, because we have a better evaluation function.
3
Figure 2: Map 1 results
3.1.2 Second Map
Again, we can see that, compared to alpha beta, the number of nodes is
reduced and compared to minimax, the number of nodes is reduced by factor 2.
4
Figure 3: First map’s Better Evaluation Markov Decisional Process Tree
3.1.3 Third Map
Here again, with a slightly bigger map and whith a much bigger difference
with factors of 2 and 7.
5
Figure 4: Map 2
3.2 Fourth Map
This case shows that better evaluation function does not always mean fewer
nodes.
6
Figure 5: Map 2 results
3.3 Fifth Map
Again, better evaluation function is not the best.
7
Figure 6: Map 3
4 Discussion
4.1 Limitations of High-Level Heuristics
While the use of high-level heuristics in action ordering generally improves per-
formance, it is not always fine enough for every case. As a result, in some
cases, the BetterValueFunction may not actually result in fewer nodes being
expanded compared to basic evaluation functions.
4.2 Future Work
Recent advancements in adversarial search algorithms have started to incorpo-
rate machine learning techniques to dynamically adapt the heuristics used for
action selection. This represents a potential avenue for further improving the
performance of our algorithms.
5 ChatGPT Usage
The project was made with ChatGPT.
8
Figure 7: Map 3 results
Figure 8: Map 4
9
Figure 9: Map 4 results
Figure 10: Map 5
10
Figure 11: Map 5 results
11
----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\doc\out\media\map2023_10_29_13_56_42.png -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\doc\out\media\maps.zip -----INFO-F-311: Artificial Intelligence - Project 2:
Recherche adversariale
Bourgeois No´e
2023 October 29
Contents
1 Introduction 2
2 Better evaluation function 2
2.1 get available actions ordered . . . . . . . . . . . . . . . . . . 2
2.2 Method: transition . . . . . . . . . . . . . . . . . . . . . . . . . 2
3 Results 3
3.1 Fewer Nodes with Better Evaluation . . . . . . . . . . . . . . . . 3
3.1.1 First Map . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
3.1.2 Second Map . . . . . . . . . . . . . . . . . . . . . . . . . . 4
3.1.3 Third Map . . . . . . . . . . . . . . . . . . . . . . . . . . 5
3.2 Fourth Map . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
3.3 Fifth Map . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
4 Discussion 8
4.1 Limitations of High-Level Heuristics . . . . . . . . . . . . . . . . 8
4.2 Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
5 ChatGPT Usage 8
1
1 Introduction
This report outlines the application of adversarial search techniques in graph
search problems. For references, please refer to the project instructions and the
project 1.
2 Better evaluation function
2.1 get available actions ordered
Algorithm Time Complexity
Minimax O(bm)
Alpha-Beta O(bm/2) to O(bm)
Alpha-Beta with Perfect Ordering O(bm/2)
IntheBetterValueFunctionclass,thismethodorderstheavailableactions
based on certain conditions:
• Moves the action STAY to the end of the list.
• If not all gems are collected, it prioritizes actions that lead to a gem.
• If the agent’s path intersects with dangerous lasers, it de-prioritizes such
actions.
2.2 Method: transition
In this method, the value of a state is changed based on multiple factors:
• The distance of agents to gems and exit points.
• Whether all gems are collected or not.
2
Figure 1: Map 1
3 Results
3.1 Fewer Nodes with Better Evaluation
3.1.1 First Map
Here we can see that, compared to alpha beta, the number of nodes is re-
ducedby6. Additionally,comparedtominimax,thenumberofnodesisreduced
by 11. This is what we expected, because we have a better evaluation function.
3
Figure 2: Map 1 results
3.1.2 Second Map
Again, we can see that, compared to alpha beta, the number of nodes is
reduced and compared to minimax, the number of nodes is reduced by factor 2.
4
Figure 3: First map’s Better Evaluation Markov Decisional Process Tree
3.1.3 Third Map
Here again, with a slightly bigger map and whith a much bigger difference
with factors of 2 and 7.
5
Figure 4: Map 2
3.2 Fourth Map
This case shows that better evaluation function does not always mean fewer
nodes.
6
Figure 5: Map 2 results
3.3 Fifth Map
Again, better evaluation function is not the best.
7
Figure 6: Map 3
4 Discussion
4.1 Limitations of High-Level Heuristics
While the use of high-level heuristics in action ordering generally improves per-
formance, it is not always fine enough for every case. As a result, in some
cases, the BetterValueFunction may not actually result in fewer nodes being
expanded compared to basic evaluation functions.
4.2 Future Work
Recent advancements in adversarial search algorithms have started to incorpo-
rate machine learning techniques to dynamically adapt the heuristics used for
action selection. This represents a potential avenue for further improving the
performance of our algorithms.
5 ChatGPT Usage
The project was made with ChatGPT.
8
Figure 7: Map 3 results
Figure 8: Map 4
9
Figure 9: Map 4 results
Figure 10: Map 5
10
Figure 11: Map 5 results
11
----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\doc\out\media\maps.zip -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\doc\out\media\map\map2023_10_29_13_48_15.png -----INFO-F-311: Artificial Intelligence - Project 2:
Recherche adversariale
Bourgeois No´e
2023 October 29
Contents
1 Introduction 2
2 Better evaluation function 2
2.1 get available actions ordered . . . . . . . . . . . . . . . . . . 2
2.2 Method: transition . . . . . . . . . . . . . . . . . . . . . . . . . 2
3 Results 3
3.1 Fewer Nodes with Better Evaluation . . . . . . . . . . . . . . . . 3
3.1.1 First Map . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
3.1.2 Second Map . . . . . . . . . . . . . . . . . . . . . . . . . . 4
3.1.3 Third Map . . . . . . . . . . . . . . . . . . . . . . . . . . 5
3.2 Fourth Map . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
3.3 Fifth Map . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
4 Discussion 8
4.1 Limitations of High-Level Heuristics . . . . . . . . . . . . . . . . 8
4.2 Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
5 ChatGPT Usage 8
1
1 Introduction
This report outlines the application of adversarial search techniques in graph
search problems. For references, please refer to the project instructions and the
project 1.
2 Better evaluation function
2.1 get available actions ordered
Algorithm Time Complexity
Minimax O(bm)
Alpha-Beta O(bm/2) to O(bm)
Alpha-Beta with Perfect Ordering O(bm/2)
IntheBetterValueFunctionclass,thismethodorderstheavailableactions
based on certain conditions:
• Moves the action STAY to the end of the list.
• If not all gems are collected, it prioritizes actions that lead to a gem.
• If the agent’s path intersects with dangerous lasers, it de-prioritizes such
actions.
2.2 Method: transition
In this method, the value of a state is changed based on multiple factors:
• The distance of agents to gems and exit points.
• Whether all gems are collected or not.
2
Figure 1: Map 1
3 Results
3.1 Fewer Nodes with Better Evaluation
3.1.1 First Map
Here we can see that, compared to alpha beta, the number of nodes is re-
ducedby6. Additionally,comparedtominimax,thenumberofnodesisreduced
by 11. This is what we expected, because we have a better evaluation function.
3
Figure 2: Map 1 results
3.1.2 Second Map
Again, we can see that, compared to alpha beta, the number of nodes is
reduced and compared to minimax, the number of nodes is reduced by factor 2.
4
Figure 3: First map’s Better Evaluation Markov Decisional Process Tree
3.1.3 Third Map
Here again, with a slightly bigger map and whith a much bigger difference
with factors of 2 and 7.
5
Figure 4: Map 2
3.2 Fourth Map
This case shows that better evaluation function does not always mean fewer
nodes.
6
Figure 5: Map 2 results
3.3 Fifth Map
Again, better evaluation function is not the best.
7
Figure 6: Map 3
4 Discussion
4.1 Limitations of High-Level Heuristics
While the use of high-level heuristics in action ordering generally improves per-
formance, it is not always fine enough for every case. As a result, in some
cases, the BetterValueFunction may not actually result in fewer nodes being
expanded compared to basic evaluation functions.
4.2 Future Work
Recent advancements in adversarial search algorithms have started to incorpo-
rate machine learning techniques to dynamically adapt the heuristics used for
action selection. This represents a potential avenue for further improving the
performance of our algorithms.
5 ChatGPT Usage
The project was made with ChatGPT.
8
Figure 7: Map 3 results
Figure 8: Map 4
9
Figure 9: Map 4 results
Figure 10: Map 5
10
Figure 11: Map 5 results
11
----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\doc\out\media\map\map2023_10_29_13_48_15.png -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\doc\out\media\map\map2023_10_29_13_49_35.png -----INFO-F-311: Artificial Intelligence - Project 2:
Recherche adversariale
Bourgeois No´e
2023 October 29
Contents
1 Introduction 2
2 Better evaluation function 2
2.1 get available actions ordered . . . . . . . . . . . . . . . . . . 2
2.2 Method: transition . . . . . . . . . . . . . . . . . . . . . . . . . 2
3 Results 3
3.1 Fewer Nodes with Better Evaluation . . . . . . . . . . . . . . . . 3
3.1.1 First Map . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
3.1.2 Second Map . . . . . . . . . . . . . . . . . . . . . . . . . . 4
3.1.3 Third Map . . . . . . . . . . . . . . . . . . . . . . . . . . 5
3.2 Fourth Map . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
3.3 Fifth Map . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
4 Discussion 8
4.1 Limitations of High-Level Heuristics . . . . . . . . . . . . . . . . 8
4.2 Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
5 ChatGPT Usage 8
1
1 Introduction
This report outlines the application of adversarial search techniques in graph
search problems. For references, please refer to the project instructions and the
project 1.
2 Better evaluation function
2.1 get available actions ordered
Algorithm Time Complexity
Minimax O(bm)
Alpha-Beta O(bm/2) to O(bm)
Alpha-Beta with Perfect Ordering O(bm/2)
IntheBetterValueFunctionclass,thismethodorderstheavailableactions
based on certain conditions:
• Moves the action STAY to the end of the list.
• If not all gems are collected, it prioritizes actions that lead to a gem.
• If the agent’s path intersects with dangerous lasers, it de-prioritizes such
actions.
2.2 Method: transition
In this method, the value of a state is changed based on multiple factors:
• The distance of agents to gems and exit points.
• Whether all gems are collected or not.
2
Figure 1: Map 1
3 Results
3.1 Fewer Nodes with Better Evaluation
3.1.1 First Map
Here we can see that, compared to alpha beta, the number of nodes is re-
ducedby6. Additionally,comparedtominimax,thenumberofnodesisreduced
by 11. This is what we expected, because we have a better evaluation function.
3
Figure 2: Map 1 results
3.1.2 Second Map
Again, we can see that, compared to alpha beta, the number of nodes is
reduced and compared to minimax, the number of nodes is reduced by factor 2.
4
Figure 3: First map’s Better Evaluation Markov Decisional Process Tree
3.1.3 Third Map
Here again, with a slightly bigger map and whith a much bigger difference
with factors of 2 and 7.
5
Figure 4: Map 2
3.2 Fourth Map
This case shows that better evaluation function does not always mean fewer
nodes.
6
Figure 5: Map 2 results
3.3 Fifth Map
Again, better evaluation function is not the best.
7
Figure 6: Map 3
4 Discussion
4.1 Limitations of High-Level Heuristics
While the use of high-level heuristics in action ordering generally improves per-
formance, it is not always fine enough for every case. As a result, in some
cases, the BetterValueFunction may not actually result in fewer nodes being
expanded compared to basic evaluation functions.
4.2 Future Work
Recent advancements in adversarial search algorithms have started to incorpo-
rate machine learning techniques to dynamically adapt the heuristics used for
action selection. This represents a potential avenue for further improving the
performance of our algorithms.
5 ChatGPT Usage
The project was made with ChatGPT.
8
Figure 7: Map 3 results
Figure 8: Map 4
9
Figure 9: Map 4 results
Figure 10: Map 5
10
Figure 11: Map 5 results
11
----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\doc\out\media\map\map2023_10_29_13_49_35.png -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\doc\out\media\map\map2023_10_29_13_53_25.png -----INFO-F-311: Artificial Intelligence - Project 2:
Recherche adversariale
Bourgeois No´e
2023 October 29
Contents
1 Introduction 2
2 Better evaluation function 2
2.1 get available actions ordered . . . . . . . . . . . . . . . . . . 2
2.2 Method: transition . . . . . . . . . . . . . . . . . . . . . . . . . 2
3 Results 3
3.1 Fewer Nodes with Better Evaluation . . . . . . . . . . . . . . . . 3
3.1.1 First Map . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
3.1.2 Second Map . . . . . . . . . . . . . . . . . . . . . . . . . . 4
3.1.3 Third Map . . . . . . . . . . . . . . . . . . . . . . . . . . 5
3.2 Fourth Map . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
3.3 Fifth Map . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
4 Discussion 8
4.1 Limitations of High-Level Heuristics . . . . . . . . . . . . . . . . 8
4.2 Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
5 ChatGPT Usage 8
1
1 Introduction
This report outlines the application of adversarial search techniques in graph
search problems. For references, please refer to the project instructions and the
project 1.
2 Better evaluation function
2.1 get available actions ordered
Algorithm Time Complexity
Minimax O(bm)
Alpha-Beta O(bm/2) to O(bm)
Alpha-Beta with Perfect Ordering O(bm/2)
IntheBetterValueFunctionclass,thismethodorderstheavailableactions
based on certain conditions:
• Moves the action STAY to the end of the list.
• If not all gems are collected, it prioritizes actions that lead to a gem.
• If the agent’s path intersects with dangerous lasers, it de-prioritizes such
actions.
2.2 Method: transition
In this method, the value of a state is changed based on multiple factors:
• The distance of agents to gems and exit points.
• Whether all gems are collected or not.
2
Figure 1: Map 1
3 Results
3.1 Fewer Nodes with Better Evaluation
3.1.1 First Map
Here we can see that, compared to alpha beta, the number of nodes is re-
ducedby6. Additionally,comparedtominimax,thenumberofnodesisreduced
by 11. This is what we expected, because we have a better evaluation function.
3
Figure 2: Map 1 results
3.1.2 Second Map
Again, we can see that, compared to alpha beta, the number of nodes is
reduced and compared to minimax, the number of nodes is reduced by factor 2.
4
Figure 3: First map’s Better Evaluation Markov Decisional Process Tree
3.1.3 Third Map
Here again, with a slightly bigger map and whith a much bigger difference
with factors of 2 and 7.
5
Figure 4: Map 2
3.2 Fourth Map
This case shows that better evaluation function does not always mean fewer
nodes.
6
Figure 5: Map 2 results
3.3 Fifth Map
Again, better evaluation function is not the best.
7
Figure 6: Map 3
4 Discussion
4.1 Limitations of High-Level Heuristics
While the use of high-level heuristics in action ordering generally improves per-
formance, it is not always fine enough for every case. As a result, in some
cases, the BetterValueFunction may not actually result in fewer nodes being
expanded compared to basic evaluation functions.
4.2 Future Work
Recent advancements in adversarial search algorithms have started to incorpo-
rate machine learning techniques to dynamically adapt the heuristics used for
action selection. This represents a potential avenue for further improving the
performance of our algorithms.
5 ChatGPT Usage
The project was made with ChatGPT.
8
Figure 7: Map 3 results
Figure 8: Map 4
9
Figure 9: Map 4 results
Figure 10: Map 5
10
Figure 11: Map 5 results
11
----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\doc\out\media\map\map2023_10_29_13_53_25.png -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\doc\out\media\map\map2023_10_29_13_55_47.png -----INFO-F-311: Artificial Intelligence - Project 2:
Recherche adversariale
Bourgeois No´e
2023 October 29
Contents
1 Introduction 2
2 Better evaluation function 2
2.1 get available actions ordered . . . . . . . . . . . . . . . . . . 2
2.2 Method: transition . . . . . . . . . . . . . . . . . . . . . . . . . 2
3 Results 3
3.1 Fewer Nodes with Better Evaluation . . . . . . . . . . . . . . . . 3
3.1.1 First Map . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
3.1.2 Second Map . . . . . . . . . . . . . . . . . . . . . . . . . . 4
3.1.3 Third Map . . . . . . . . . . . . . . . . . . . . . . . . . . 5
3.2 Fourth Map . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
3.3 Fifth Map . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
4 Discussion 8
4.1 Limitations of High-Level Heuristics . . . . . . . . . . . . . . . . 8
4.2 Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
5 ChatGPT Usage 8
1
1 Introduction
This report outlines the application of adversarial search techniques in graph
search problems. For references, please refer to the project instructions and the
project 1.
2 Better evaluation function
2.1 get available actions ordered
Algorithm Time Complexity
Minimax O(bm)
Alpha-Beta O(bm/2) to O(bm)
Alpha-Beta with Perfect Ordering O(bm/2)
IntheBetterValueFunctionclass,thismethodorderstheavailableactions
based on certain conditions:
• Moves the action STAY to the end of the list.
• If not all gems are collected, it prioritizes actions that lead to a gem.
• If the agent’s path intersects with dangerous lasers, it de-prioritizes such
actions.
2.2 Method: transition
In this method, the value of a state is changed based on multiple factors:
• The distance of agents to gems and exit points.
• Whether all gems are collected or not.
2
Figure 1: Map 1
3 Results
3.1 Fewer Nodes with Better Evaluation
3.1.1 First Map
Here we can see that, compared to alpha beta, the number of nodes is re-
ducedby6. Additionally,comparedtominimax,thenumberofnodesisreduced
by 11. This is what we expected, because we have a better evaluation function.
3
Figure 2: Map 1 results
3.1.2 Second Map
Again, we can see that, compared to alpha beta, the number of nodes is
reduced and compared to minimax, the number of nodes is reduced by factor 2.
4
Figure 3: First map’s Better Evaluation Markov Decisional Process Tree
3.1.3 Third Map
Here again, with a slightly bigger map and whith a much bigger difference
with factors of 2 and 7.
5
Figure 4: Map 2
3.2 Fourth Map
This case shows that better evaluation function does not always mean fewer
nodes.
6
Figure 5: Map 2 results
3.3 Fifth Map
Again, better evaluation function is not the best.
7
Figure 6: Map 3
4 Discussion
4.1 Limitations of High-Level Heuristics
While the use of high-level heuristics in action ordering generally improves per-
formance, it is not always fine enough for every case. As a result, in some
cases, the BetterValueFunction may not actually result in fewer nodes being
expanded compared to basic evaluation functions.
4.2 Future Work
Recent advancements in adversarial search algorithms have started to incorpo-
rate machine learning techniques to dynamically adapt the heuristics used for
action selection. This represents a potential avenue for further improving the
performance of our algorithms.
5 ChatGPT Usage
The project was made with ChatGPT.
8
Figure 7: Map 3 results
Figure 8: Map 4
9
Figure 9: Map 4 results
Figure 10: Map 5
10
Figure 11: Map 5 results
11
----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\doc\out\media\map\map2023_10_29_13_55_47.png -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\doc\out\media\map\map2023_10_29_13_56_42.png -----INFO-F-311: Artificial Intelligence - Project 2:
Recherche adversariale
Bourgeois No´e
2023 October 29
Contents
1 Introduction 2
2 Better evaluation function 2
2.1 get available actions ordered . . . . . . . . . . . . . . . . . . 2
2.2 Method: transition . . . . . . . . . . . . . . . . . . . . . . . . . 2
3 Results 3
3.1 Fewer Nodes with Better Evaluation . . . . . . . . . . . . . . . . 3
3.1.1 First Map . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
3.1.2 Second Map . . . . . . . . . . . . . . . . . . . . . . . . . . 4
3.1.3 Third Map . . . . . . . . . . . . . . . . . . . . . . . . . . 5
3.2 Fourth Map . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
3.3 Fifth Map . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
4 Discussion 8
4.1 Limitations of High-Level Heuristics . . . . . . . . . . . . . . . . 8
4.2 Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
5 ChatGPT Usage 8
1
1 Introduction
This report outlines the application of adversarial search techniques in graph
search problems. For references, please refer to the project instructions and the
project 1.
2 Better evaluation function
2.1 get available actions ordered
Algorithm Time Complexity
Minimax O(bm)
Alpha-Beta O(bm/2) to O(bm)
Alpha-Beta with Perfect Ordering O(bm/2)
IntheBetterValueFunctionclass,thismethodorderstheavailableactions
based on certain conditions:
• Moves the action STAY to the end of the list.
• If not all gems are collected, it prioritizes actions that lead to a gem.
• If the agent’s path intersects with dangerous lasers, it de-prioritizes such
actions.
2.2 Method: transition
In this method, the value of a state is changed based on multiple factors:
• The distance of agents to gems and exit points.
• Whether all gems are collected or not.
2
Figure 1: Map 1
3 Results
3.1 Fewer Nodes with Better Evaluation
3.1.1 First Map
Here we can see that, compared to alpha beta, the number of nodes is re-
ducedby6. Additionally,comparedtominimax,thenumberofnodesisreduced
by 11. This is what we expected, because we have a better evaluation function.
3
Figure 2: Map 1 results
3.1.2 Second Map
Again, we can see that, compared to alpha beta, the number of nodes is
reduced and compared to minimax, the number of nodes is reduced by factor 2.
4
Figure 3: First map’s Better Evaluation Markov Decisional Process Tree
3.1.3 Third Map
Here again, with a slightly bigger map and whith a much bigger difference
with factors of 2 and 7.
5
Figure 4: Map 2
3.2 Fourth Map
This case shows that better evaluation function does not always mean fewer
nodes.
6
Figure 5: Map 2 results
3.3 Fifth Map
Again, better evaluation function is not the best.
7
Figure 6: Map 3
4 Discussion
4.1 Limitations of High-Level Heuristics
While the use of high-level heuristics in action ordering generally improves per-
formance, it is not always fine enough for every case. As a result, in some
cases, the BetterValueFunction may not actually result in fewer nodes being
expanded compared to basic evaluation functions.
4.2 Future Work
Recent advancements in adversarial search algorithms have started to incorpo-
rate machine learning techniques to dynamically adapt the heuristics used for
action selection. This represents a potential avenue for further improving the
performance of our algorithms.
5 ChatGPT Usage
The project was made with ChatGPT.
8
Figure 7: Map 3 results
Figure 8: Map 4
9
Figure 9: Map 4 results
Figure 10: Map 5
10
Figure 11: Map 5 results
11
----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\doc\out\media\map\map2023_10_29_13_56_42.png -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\graphs\map2023_10_29_13_48_15.png -----INFO-F-311: Artificial Intelligence - Project 2:
Recherche adversariale
Bourgeois No´e
2023 October 29
Contents
1 Introduction 2
2 Better evaluation function 2
2.1 get available actions ordered . . . . . . . . . . . . . . . . . . 2
2.2 Method: transition . . . . . . . . . . . . . . . . . . . . . . . . . 2
3 Results 3
3.1 Fewer Nodes with Better Evaluation . . . . . . . . . . . . . . . . 3
3.1.1 First Map . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
3.1.2 Second Map . . . . . . . . . . . . . . . . . . . . . . . . . . 4
3.1.3 Third Map . . . . . . . . . . . . . . . . . . . . . . . . . . 5
3.2 Fourth Map . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
3.3 Fifth Map . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
4 Discussion 8
4.1 Limitations of High-Level Heuristics . . . . . . . . . . . . . . . . 8
4.2 Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
5 ChatGPT Usage 8
1
1 Introduction
This report outlines the application of adversarial search techniques in graph
search problems. For references, please refer to the project instructions and the
project 1.
2 Better evaluation function
2.1 get available actions ordered
Algorithm Time Complexity
Minimax O(bm)
Alpha-Beta O(bm/2) to O(bm)
Alpha-Beta with Perfect Ordering O(bm/2)
IntheBetterValueFunctionclass,thismethodorderstheavailableactions
based on certain conditions:
• Moves the action STAY to the end of the list.
• If not all gems are collected, it prioritizes actions that lead to a gem.
• If the agent’s path intersects with dangerous lasers, it de-prioritizes such
actions.
2.2 Method: transition
In this method, the value of a state is changed based on multiple factors:
• The distance of agents to gems and exit points.
• Whether all gems are collected or not.
2
Figure 1: Map 1
3 Results
3.1 Fewer Nodes with Better Evaluation
3.1.1 First Map
Here we can see that, compared to alpha beta, the number of nodes is re-
ducedby6. Additionally,comparedtominimax,thenumberofnodesisreduced
by 11. This is what we expected, because we have a better evaluation function.
3
Figure 2: Map 1 results
3.1.2 Second Map
Again, we can see that, compared to alpha beta, the number of nodes is
reduced and compared to minimax, the number of nodes is reduced by factor 2.
4
Figure 3: First map’s Better Evaluation Markov Decisional Process Tree
3.1.3 Third Map
Here again, with a slightly bigger map and whith a much bigger difference
with factors of 2 and 7.
5
Figure 4: Map 2
3.2 Fourth Map
This case shows that better evaluation function does not always mean fewer
nodes.
6
Figure 5: Map 2 results
3.3 Fifth Map
Again, better evaluation function is not the best.
7
Figure 6: Map 3
4 Discussion
4.1 Limitations of High-Level Heuristics
While the use of high-level heuristics in action ordering generally improves per-
formance, it is not always fine enough for every case. As a result, in some
cases, the BetterValueFunction may not actually result in fewer nodes being
expanded compared to basic evaluation functions.
4.2 Future Work
Recent advancements in adversarial search algorithms have started to incorpo-
rate machine learning techniques to dynamically adapt the heuristics used for
action selection. This represents a potential avenue for further improving the
performance of our algorithms.
5 ChatGPT Usage
The project was made with ChatGPT.
8
Figure 7: Map 3 results
Figure 8: Map 4
9
Figure 9: Map 4 results
Figure 10: Map 5
10
Figure 11: Map 5 results
11
----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\graphs\map2023_10_29_13_48_15.png -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\graphs\map2023_10_29_13_49_35.png -----INFO-F-311: Artificial Intelligence - Project 2:
Recherche adversariale
Bourgeois No´e
2023 October 29
Contents
1 Introduction 2
2 Better evaluation function 2
2.1 get available actions ordered . . . . . . . . . . . . . . . . . . 2
2.2 Method: transition . . . . . . . . . . . . . . . . . . . . . . . . . 2
3 Results 3
3.1 Fewer Nodes with Better Evaluation . . . . . . . . . . . . . . . . 3
3.1.1 First Map . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
3.1.2 Second Map . . . . . . . . . . . . . . . . . . . . . . . . . . 4
3.1.3 Third Map . . . . . . . . . . . . . . . . . . . . . . . . . . 5
3.2 Fourth Map . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
3.3 Fifth Map . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
4 Discussion 8
4.1 Limitations of High-Level Heuristics . . . . . . . . . . . . . . . . 8
4.2 Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
5 ChatGPT Usage 8
1
1 Introduction
This report outlines the application of adversarial search techniques in graph
search problems. For references, please refer to the project instructions and the
project 1.
2 Better evaluation function
2.1 get available actions ordered
Algorithm Time Complexity
Minimax O(bm)
Alpha-Beta O(bm/2) to O(bm)
Alpha-Beta with Perfect Ordering O(bm/2)
IntheBetterValueFunctionclass,thismethodorderstheavailableactions
based on certain conditions:
• Moves the action STAY to the end of the list.
• If not all gems are collected, it prioritizes actions that lead to a gem.
• If the agent’s path intersects with dangerous lasers, it de-prioritizes such
actions.
2.2 Method: transition
In this method, the value of a state is changed based on multiple factors:
• The distance of agents to gems and exit points.
• Whether all gems are collected or not.
2
Figure 1: Map 1
3 Results
3.1 Fewer Nodes with Better Evaluation
3.1.1 First Map
Here we can see that, compared to alpha beta, the number of nodes is re-
ducedby6. Additionally,comparedtominimax,thenumberofnodesisreduced
by 11. This is what we expected, because we have a better evaluation function.
3
Figure 2: Map 1 results
3.1.2 Second Map
Again, we can see that, compared to alpha beta, the number of nodes is
reduced and compared to minimax, the number of nodes is reduced by factor 2.
4
Figure 3: First map’s Better Evaluation Markov Decisional Process Tree
3.1.3 Third Map
Here again, with a slightly bigger map and whith a much bigger difference
with factors of 2 and 7.
5
Figure 4: Map 2
3.2 Fourth Map
This case shows that better evaluation function does not always mean fewer
nodes.
6
Figure 5: Map 2 results
3.3 Fifth Map
Again, better evaluation function is not the best.
7
Figure 6: Map 3
4 Discussion
4.1 Limitations of High-Level Heuristics
While the use of high-level heuristics in action ordering generally improves per-
formance, it is not always fine enough for every case. As a result, in some
cases, the BetterValueFunction may not actually result in fewer nodes being
expanded compared to basic evaluation functions.
4.2 Future Work
Recent advancements in adversarial search algorithms have started to incorpo-
rate machine learning techniques to dynamically adapt the heuristics used for
action selection. This represents a potential avenue for further improving the
performance of our algorithms.
5 ChatGPT Usage
The project was made with ChatGPT.
8
Figure 7: Map 3 results
Figure 8: Map 4
9
Figure 9: Map 4 results
Figure 10: Map 5
10
Figure 11: Map 5 results
11
----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\graphs\map2023_10_29_13_49_35.png -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\graphs\map2023_10_29_13_53_25.png -----INFO-F-311: Artificial Intelligence - Project 2:
Recherche adversariale
Bourgeois No´e
2023 October 29
Contents
1 Introduction 2
2 Better evaluation function 2
2.1 get available actions ordered . . . . . . . . . . . . . . . . . . 2
2.2 Method: transition . . . . . . . . . . . . . . . . . . . . . . . . . 2
3 Results 3
3.1 Fewer Nodes with Better Evaluation . . . . . . . . . . . . . . . . 3
3.1.1 First Map . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
3.1.2 Second Map . . . . . . . . . . . . . . . . . . . . . . . . . . 4
3.1.3 Third Map . . . . . . . . . . . . . . . . . . . . . . . . . . 5
3.2 Fourth Map . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
3.3 Fifth Map . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
4 Discussion 8
4.1 Limitations of High-Level Heuristics . . . . . . . . . . . . . . . . 8
4.2 Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
5 ChatGPT Usage 8
1
1 Introduction
This report outlines the application of adversarial search techniques in graph
search problems. For references, please refer to the project instructions and the
project 1.
2 Better evaluation function
2.1 get available actions ordered
Algorithm Time Complexity
Minimax O(bm)
Alpha-Beta O(bm/2) to O(bm)
Alpha-Beta with Perfect Ordering O(bm/2)
IntheBetterValueFunctionclass,thismethodorderstheavailableactions
based on certain conditions:
• Moves the action STAY to the end of the list.
• If not all gems are collected, it prioritizes actions that lead to a gem.
• If the agent’s path intersects with dangerous lasers, it de-prioritizes such
actions.
2.2 Method: transition
In this method, the value of a state is changed based on multiple factors:
• The distance of agents to gems and exit points.
• Whether all gems are collected or not.
2
Figure 1: Map 1
3 Results
3.1 Fewer Nodes with Better Evaluation
3.1.1 First Map
Here we can see that, compared to alpha beta, the number of nodes is re-
ducedby6. Additionally,comparedtominimax,thenumberofnodesisreduced
by 11. This is what we expected, because we have a better evaluation function.
3
Figure 2: Map 1 results
3.1.2 Second Map
Again, we can see that, compared to alpha beta, the number of nodes is
reduced and compared to minimax, the number of nodes is reduced by factor 2.
4
Figure 3: First map’s Better Evaluation Markov Decisional Process Tree
3.1.3 Third Map
Here again, with a slightly bigger map and whith a much bigger difference
with factors of 2 and 7.
5
Figure 4: Map 2
3.2 Fourth Map
This case shows that better evaluation function does not always mean fewer
nodes.
6
Figure 5: Map 2 results
3.3 Fifth Map
Again, better evaluation function is not the best.
7
Figure 6: Map 3
4 Discussion
4.1 Limitations of High-Level Heuristics
While the use of high-level heuristics in action ordering generally improves per-
formance, it is not always fine enough for every case. As a result, in some
cases, the BetterValueFunction may not actually result in fewer nodes being
expanded compared to basic evaluation functions.
4.2 Future Work
Recent advancements in adversarial search algorithms have started to incorpo-
rate machine learning techniques to dynamically adapt the heuristics used for
action selection. This represents a potential avenue for further improving the
performance of our algorithms.
5 ChatGPT Usage
The project was made with ChatGPT.
8
Figure 7: Map 3 results
Figure 8: Map 4
9
Figure 9: Map 4 results
Figure 10: Map 5
10
Figure 11: Map 5 results
11
----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\graphs\map2023_10_29_13_53_25.png -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\graphs\map2023_10_29_13_55_47.png -----INFO-F-311: Artificial Intelligence - Project 2:
Recherche adversariale
Bourgeois No´e
2023 October 29
Contents
1 Introduction 2
2 Better evaluation function 2
2.1 get available actions ordered . . . . . . . . . . . . . . . . . . 2
2.2 Method: transition . . . . . . . . . . . . . . . . . . . . . . . . . 2
3 Results 3
3.1 Fewer Nodes with Better Evaluation . . . . . . . . . . . . . . . . 3
3.1.1 First Map . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
3.1.2 Second Map . . . . . . . . . . . . . . . . . . . . . . . . . . 4
3.1.3 Third Map . . . . . . . . . . . . . . . . . . . . . . . . . . 5
3.2 Fourth Map . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
3.3 Fifth Map . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
4 Discussion 8
4.1 Limitations of High-Level Heuristics . . . . . . . . . . . . . . . . 8
4.2 Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
5 ChatGPT Usage 8
1
1 Introduction
This report outlines the application of adversarial search techniques in graph
search problems. For references, please refer to the project instructions and the
project 1.
2 Better evaluation function
2.1 get available actions ordered
Algorithm Time Complexity
Minimax O(bm)
Alpha-Beta O(bm/2) to O(bm)
Alpha-Beta with Perfect Ordering O(bm/2)
IntheBetterValueFunctionclass,thismethodorderstheavailableactions
based on certain conditions:
• Moves the action STAY to the end of the list.
• If not all gems are collected, it prioritizes actions that lead to a gem.
• If the agent’s path intersects with dangerous lasers, it de-prioritizes such
actions.
2.2 Method: transition
In this method, the value of a state is changed based on multiple factors:
• The distance of agents to gems and exit points.
• Whether all gems are collected or not.
2
Figure 1: Map 1
3 Results
3.1 Fewer Nodes with Better Evaluation
3.1.1 First Map
Here we can see that, compared to alpha beta, the number of nodes is re-
ducedby6. Additionally,comparedtominimax,thenumberofnodesisreduced
by 11. This is what we expected, because we have a better evaluation function.
3
Figure 2: Map 1 results
3.1.2 Second Map
Again, we can see that, compared to alpha beta, the number of nodes is
reduced and compared to minimax, the number of nodes is reduced by factor 2.
4
Figure 3: First map’s Better Evaluation Markov Decisional Process Tree
3.1.3 Third Map
Here again, with a slightly bigger map and whith a much bigger difference
with factors of 2 and 7.
5
Figure 4: Map 2
3.2 Fourth Map
This case shows that better evaluation function does not always mean fewer
nodes.
6
Figure 5: Map 2 results
3.3 Fifth Map
Again, better evaluation function is not the best.
7
Figure 6: Map 3
4 Discussion
4.1 Limitations of High-Level Heuristics
While the use of high-level heuristics in action ordering generally improves per-
formance, it is not always fine enough for every case. As a result, in some
cases, the BetterValueFunction may not actually result in fewer nodes being
expanded compared to basic evaluation functions.
4.2 Future Work
Recent advancements in adversarial search algorithms have started to incorpo-
rate machine learning techniques to dynamically adapt the heuristics used for
action selection. This represents a potential avenue for further improving the
performance of our algorithms.
5 ChatGPT Usage
The project was made with ChatGPT.
8
Figure 7: Map 3 results
Figure 8: Map 4
9
Figure 9: Map 4 results
Figure 10: Map 5
10
Figure 11: Map 5 results
11
----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\graphs\map2023_10_29_13_55_47.png -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\graphs\map2023_10_29_13_56_42.png -----INFO-F-311: Artificial Intelligence - Project 2:
Recherche adversariale
Bourgeois No´e
2023 October 29
Contents
1 Introduction 2
2 Better evaluation function 2
2.1 get available actions ordered . . . . . . . . . . . . . . . . . . 2
2.2 Method: transition . . . . . . . . . . . . . . . . . . . . . . . . . 2
3 Results 3
3.1 Fewer Nodes with Better Evaluation . . . . . . . . . . . . . . . . 3
3.1.1 First Map . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
3.1.2 Second Map . . . . . . . . . . . . . . . . . . . . . . . . . . 4
3.1.3 Third Map . . . . . . . . . . . . . . . . . . . . . . . . . . 5
3.2 Fourth Map . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
3.3 Fifth Map . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
4 Discussion 8
4.1 Limitations of High-Level Heuristics . . . . . . . . . . . . . . . . 8
4.2 Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
5 ChatGPT Usage 8
1
1 Introduction
This report outlines the application of adversarial search techniques in graph
search problems. For references, please refer to the project instructions and the
project 1.
2 Better evaluation function
2.1 get available actions ordered
Algorithm Time Complexity
Minimax O(bm)
Alpha-Beta O(bm/2) to O(bm)
Alpha-Beta with Perfect Ordering O(bm/2)
IntheBetterValueFunctionclass,thismethodorderstheavailableactions
based on certain conditions:
• Moves the action STAY to the end of the list.
• If not all gems are collected, it prioritizes actions that lead to a gem.
• If the agent’s path intersects with dangerous lasers, it de-prioritizes such
actions.
2.2 Method: transition
In this method, the value of a state is changed based on multiple factors:
• The distance of agents to gems and exit points.
• Whether all gems are collected or not.
2
Figure 1: Map 1
3 Results
3.1 Fewer Nodes with Better Evaluation
3.1.1 First Map
Here we can see that, compared to alpha beta, the number of nodes is re-
ducedby6. Additionally,comparedtominimax,thenumberofnodesisreduced
by 11. This is what we expected, because we have a better evaluation function.
3
Figure 2: Map 1 results
3.1.2 Second Map
Again, we can see that, compared to alpha beta, the number of nodes is
reduced and compared to minimax, the number of nodes is reduced by factor 2.
4
Figure 3: First map’s Better Evaluation Markov Decisional Process Tree
3.1.3 Third Map
Here again, with a slightly bigger map and whith a much bigger difference
with factors of 2 and 7.
5
Figure 4: Map 2
3.2 Fourth Map
This case shows that better evaluation function does not always mean fewer
nodes.
6
Figure 5: Map 2 results
3.3 Fifth Map
Again, better evaluation function is not the best.
7
Figure 6: Map 3
4 Discussion
4.1 Limitations of High-Level Heuristics
While the use of high-level heuristics in action ordering generally improves per-
formance, it is not always fine enough for every case. As a result, in some
cases, the BetterValueFunction may not actually result in fewer nodes being
expanded compared to basic evaluation functions.
4.2 Future Work
Recent advancements in adversarial search algorithms have started to incorpo-
rate machine learning techniques to dynamically adapt the heuristics used for
action selection. This represents a potential avenue for further improving the
performance of our algorithms.
5 ChatGPT Usage
The project was made with ChatGPT.
8
Figure 7: Map 3 results
Figure 8: Map 4
9
Figure 9: Map 4 results
Figure 10: Map 5
10
Figure 11: Map 5 results
11
----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\graphs\map2023_10_29_13_56_42.png -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\graphs\maps.zip -----INFO-F-311: Artificial Intelligence - Project 2:
Recherche adversariale
Bourgeois No´e
2023 October 29
Contents
1 Introduction 2
2 Better evaluation function 2
2.1 get available actions ordered . . . . . . . . . . . . . . . . . . 2
2.2 Method: transition . . . . . . . . . . . . . . . . . . . . . . . . . 2
3 Results 3
3.1 Fewer Nodes with Better Evaluation . . . . . . . . . . . . . . . . 3
3.1.1 First Map . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
3.1.2 Second Map . . . . . . . . . . . . . . . . . . . . . . . . . . 4
3.1.3 Third Map . . . . . . . . . . . . . . . . . . . . . . . . . . 5
3.2 Fourth Map . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
3.3 Fifth Map . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
4 Discussion 8
4.1 Limitations of High-Level Heuristics . . . . . . . . . . . . . . . . 8
4.2 Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
5 ChatGPT Usage 8
1
1 Introduction
This report outlines the application of adversarial search techniques in graph
search problems. For references, please refer to the project instructions and the
project 1.
2 Better evaluation function
2.1 get available actions ordered
Algorithm Time Complexity
Minimax O(bm)
Alpha-Beta O(bm/2) to O(bm)
Alpha-Beta with Perfect Ordering O(bm/2)
IntheBetterValueFunctionclass,thismethodorderstheavailableactions
based on certain conditions:
• Moves the action STAY to the end of the list.
• If not all gems are collected, it prioritizes actions that lead to a gem.
• If the agent’s path intersects with dangerous lasers, it de-prioritizes such
actions.
2.2 Method: transition
In this method, the value of a state is changed based on multiple factors:
• The distance of agents to gems and exit points.
• Whether all gems are collected or not.
2
Figure 1: Map 1
3 Results
3.1 Fewer Nodes with Better Evaluation
3.1.1 First Map
Here we can see that, compared to alpha beta, the number of nodes is re-
ducedby6. Additionally,comparedtominimax,thenumberofnodesisreduced
by 11. This is what we expected, because we have a better evaluation function.
3
Figure 2: Map 1 results
3.1.2 Second Map
Again, we can see that, compared to alpha beta, the number of nodes is
reduced and compared to minimax, the number of nodes is reduced by factor 2.
4
Figure 3: First map’s Better Evaluation Markov Decisional Process Tree
3.1.3 Third Map
Here again, with a slightly bigger map and whith a much bigger difference
with factors of 2 and 7.
5
Figure 4: Map 2
3.2 Fourth Map
This case shows that better evaluation function does not always mean fewer
nodes.
6
Figure 5: Map 2 results
3.3 Fifth Map
Again, better evaluation function is not the best.
7
Figure 6: Map 3
4 Discussion
4.1 Limitations of High-Level Heuristics
While the use of high-level heuristics in action ordering generally improves per-
formance, it is not always fine enough for every case. As a result, in some
cases, the BetterValueFunction may not actually result in fewer nodes being
expanded compared to basic evaluation functions.
4.2 Future Work
Recent advancements in adversarial search algorithms have started to incorpo-
rate machine learning techniques to dynamically adapt the heuristics used for
action selection. This represents a potential avenue for further improving the
performance of our algorithms.
5 ChatGPT Usage
The project was made with ChatGPT.
8
Figure 7: Map 3 results
Figure 8: Map 4
9
Figure 9: Map 4 results
Figure 10: Map 5
10
Figure 11: Map 5 results
11
----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\graphs\maps.zip -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\graph_search\priority_queue.py -----import heapq
import json
from typing import Dict, Generic, TypeVar
# from typing_extensions import deprecated

T = TypeVar("T")


class PriorityQueue(Generic[T]):
    """
    Implements a priority queue data structure. Each inserted item
    has a priority associated with it and the client is usually interested
    in quick retrieval of the lowest-priority item in the queue. This
    data structure allows O(1) access to the lowest-priority item.

    Credits: Berkley AI Pacman Project
    """

    def __init__(self):
        self.heap: list[T] = []
        self.count = 0

    def push(self, item: T, priority: float):
        entry = (priority, self.count, item)
        heapq.heappush(self.heap, entry)
        self.count += 1

    def pop(self) -> T:
        """pop the item with the minimum priority"""
        (_, _, item) = heapq.heappop(self.heap)
        return item
    
    def pop_maximum_priority(self) -> T:
        """pop the item with the maximum priority"""
        (_, _, item) = heapq.heappop(self.heap)
        return item

    # @deprecated
    # def isEmpty(self):
    #     return len(self.heap) == 0

    def is_empty(self):
        return len(self.heap) == 0

    def update(self, item: T, priority: float):
        # If item already in priority queue with higher priority, update its priority and rebuild the heap.
        # If item already in priority queue with equal or lower priority, do nothing.
        # If item not in priority queue, do the same thing as self.push.
        for index, (p, c, i) in enumerate(self.heap):
            if i == item:
                if p <= priority:
                    break
                del self.heap[index]
                self.heap.append((priority, c, item))
                heapq.heapify(self.heap)
                break
        else:
            self.push(item, priority)

class PriorityQueueOptimized(Generic[T]): #todo: 2x slower than PriorityQueue at least
    """heapq default minheap
    
    heapify to rebuild the heap, which is an 
    O(n) operation optimized using a dictionary 
    to keep track of the heap indices for each item, allowing to 
    update the heap in 
    O(logn) time."""
    def __init__(self):
        self.heap = []
        # self.entry_finder: Dict[T, int] = {}  
        self.entry_finder: Dict[str, int] = {}  # String keys for unhashable types
        self.count = 0

    # PriorityQueueOptimized iterable form
    def __iter__(self):
        return iter(self.heap)
    
    def _stringify(self, item: T) -> str:
        # return json.dumps(item)
        return str(id(item))
    
    def serialize(self, world_state: T) -> tuple:
        return (tuple(world_state.agents_positions), tuple(world_state.gems_collected))

    def push(self, item: T, priority: float):
        # serialized_item = self.serialize(item)
        # entry = (priority, self.count, serialized_item)
        # self.entry_finder[serialized_item] = len(self.heap)  # Keep track of the index
        entry = (priority, self.count, item)
        key = self._stringify(item)
        self.entry_finder[key] = len(self.heap)
        heapq.heappush(self.heap, entry)
        self.count += 1

    def pop(self) -> T:
        while self.heap:
            _, _, item = heapq.heappop(self.heap)
            key = self._stringify(item)
            # key = self.serialize(item)
            if key in self.entry_finder:
                del self.entry_finder[key]
                return item
        raise KeyError('pop from an empty priority queue')

    def is_empty(self):
        return len(self.heap) == 0

    def update(self, item: T, priority: float):
        key = self._stringify(item)
        # key = self.serialize(item)

        if key in self.entry_finder:
            index = self.entry_finder[key]
            _, _, existing_item = self.heap[index]
            # if existing_item is not key:
            if existing_item is not item:
                return
            self.heap[index] = (priority, self.count, item)
            heapq._siftup(self.heap, index)
            heapq._siftdown(self.heap, 0, index)
            self.count += 1
        else:
            self.push(item, priority)

----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\graph_search\priority_queue.py -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\graph_search\problem.py -----from abc import ABC, abstractmethod
import copy
from itertools import product
from scipy.optimize import linear_sum_assignment
import numpy as np
from typing import Tuple, Iterable, Generic, TypeVar
from lle import Position, World, Action, WorldState

T = TypeVar('T', bound=WorldState)  # Declare the generic type variable with a default bound

def get_distance(coord1, coord2):
    """Returns the distance between two coordinates"""
    x1, y1 = coord1
    x2, y2 = coord2
    return abs(x1 - x2) + abs(y1 - y2)

def min_distance_position(position : Tuple[int, int]
                          , positions: list[Tuple[int, int]] 
                            ) -> Tuple[Tuple[int, int], float]:
    """Returns the position in positions that is closest to position"""
    min_distance = float("inf")
    min_position = None
    for pos in positions:
        distance = 0
        distance = get_distance(position, pos)
        if distance < min_distance:
            min_distance = distance
            min_position = pos
    return min_position, min_distance

def balanced_multi_salesmen_greedy_tsp(remaining_cities: list[Tuple[int, int]]
                                       , num_salesmen: int
                                       , start_cities: list[Tuple[int, int]]
                                       , finish_cities: list[Tuple[int, int]]
                                       ) -> Tuple[dict[str, list[Tuple[int, int]]], dict[str, float], float]: 
    #todo: calculate the distance between the last city and the finish city one time at problem creation
    """Given a list of cities coordinates, returns a list of cities visited by each agent
    in the order that minimizes the total distance traveled.
    """
    routes = {f"agent_{i+1}": [start_cities[i]] for i in range(num_salesmen)}
    distances = {f"agent_{i+1}": 0.0 for i in range(num_salesmen)}

    while remaining_cities:
        for agent in routes.keys():
            if not remaining_cities:
                break
            current_city = routes[agent][-1]
            nearest_city, nearest_distance = min_distance_position(routes[agent][-1], remaining_cities)
            distances[agent] += nearest_distance
            routes[agent].append(nearest_city)
            remaining_cities.remove(nearest_city)

    for agent in routes.keys():
        current_city = routes[agent][-1]
        finish_city, final_distance = min_distance_position(current_city, finish_cities)
        distances[agent] += final_distance
        routes[agent].append(finish_city)
        
    total_distance = sum(distances.values())
    return routes, distances, total_distance

def serialize(world_state: WorldState
              ,objectives_reached: list[Position] = None
              ) -> tuple:
    """Serialize the given world state.
    Args:
        world_state: the world state to serialize.
    Returns:
        A tuple that represents the given world state.
    """
    if objectives_reached:
        return (tuple(world_state.agents_positions), tuple(world_state.gems_collected), tuple(objectives_reached))
    else:
        return (tuple(world_state.agents_positions), tuple(world_state.gems_collected))

def was(state: WorldState
        , objectives_reached: list[Position]
        , visited: set) -> bool:
    return serialize(state, objectives_reached) in visited

def min_distance_pairing(list_1
                             , list_2):
        # Create a cost matrix
        cost_matrix = np.zeros((len(list_1), len(list_2)))
        for i, point1 in enumerate(list_1):
            for j, point2 in enumerate(list_2):
                cost_matrix[i, j] = ((point1[0] - point2[0]) ** 2 + (point1[1] - point2[1]) ** 2) ** 0.5
        # Hungarian algorithm:
        # from cost_matrix, it does the pairing by minimizing the total distance
        row_ind, col_ind = linear_sum_assignment(cost_matrix)
        
        # Extract the paired points, their distances, and the minimum total distance
        paired_points = []
        distances = []
        min_total_distance = 0
        for i, j in zip(row_ind, col_ind):
            paired_points.append((list_1[i], list_2[j]))
            distances.append(cost_matrix[i, j])
            min_total_distance += cost_matrix[i, j]
        print("paired_points", paired_points)
        print("distances", distances)
        
        return paired_points, distances, min_total_distance

class SearchProblem(ABC, Generic[T]):
    """
    A Search Problem is a problem that can be solved by a search algorithm.

    The generic parameter T is the type of the problem state, 
    which must inherit from WorldState.
    """
    def __init__(self, world: World):
        self.world = world
        world.reset()
        self.initial_state = world.get_state()
        self.objectives = []

        self.path_size = 0
        self.nodes_expanded = 0

    @abstractmethod
    def is_goal_state(self, problem_state: T) -> bool:
        """Whether the given state is the goal state"""
        
    @abstractmethod
    def get_successors(self, state: T) -> Iterable[Tuple[T, Tuple[Action, ...], float]]:
        """
        Yield all possible states that can be reached from the given world state.
        Returns
            - the new problem state
            - the joint action that was taken to reach it
            - the cost of taking the action
        """

    def heuristic(self, problem_state: T) -> float:
        return 0.0
class SimpleSearchProblem(SearchProblem[T], Generic[T]):  # Use Generic[T] to make the class generic

    def no_duplicate_in(self, agents_positions: list[Position]) -> bool:
        """Whether each agent is on a different position."""
        agents_positions_set = set(agents_positions)  
        # Check if the number of agents on exits is equal to the total number of agents
        # and if each agent is on a different exit
        result = len(agents_positions) == len(agents_positions_set)
        return result
    
    def agents_each_on_different_exit_pos(self, state: WorldState) -> bool:
        """Whether each agent is on a different exit position."""
        agent_positions = set(state.agents_positions)  
        exit_positions = set(self.world.exit_pos)  
        # Intersect the sets to find agents that are on exit positions
        agents_on_exits = agent_positions.intersection(exit_positions)
        # Check if the number of agents on exits is equal to the total number of agents
        # and if each agent is on a different exit
        return len(agents_on_exits) == len(agent_positions) # and len(agents_on_exits) == len(exit_positions)

    def is_goal_state(self, state: WorldState) -> bool:
        """Whether the given 
        SimpleStateProblem state is the 
        SimpleSearchProblem goal state.
        True if all agents are on exit tiles
        """
        return self.agents_each_on_different_exit_pos(state)
    
    def agent_position_after_action(self, agent_pos: Position, action: Action) -> Position:
        """The position of an agent after applying the given action."""
        # print("agent_position_after_action()")
        # print("agent_pos", agent_pos)
        # print("action", action)
        agent_pos_after_action = None
        # Apply the action to the agent's position
        if action == Action.NORTH:
            agent_pos_after_action = (agent_pos[0] - 1, agent_pos[1])
        elif action == Action.SOUTH:
            agent_pos_after_action = (agent_pos[0] + 1, agent_pos[1])
        elif action == Action.WEST:
            agent_pos_after_action = (agent_pos[0], agent_pos[1] - 1)
        elif action == Action.EAST:
            agent_pos_after_action = (agent_pos[0], agent_pos[1] + 1)
        elif action == Action.STAY:
            agent_pos_after_action = (agent_pos[0], agent_pos[1])
        else:
            raise ValueError("Invalid action")
        return agent_pos_after_action

    def are_valid_joint_actions(self, state: WorldState, joint_actions: Tuple[Action, ...]) -> bool:
        """Whether the given joint actions are valid.
        an action is valid if it is available for an agent 
        and if it does not lead the agent to be on the same position as another agent"""
        # print("are_valid_joint_actions()")
        # print("state", state)
        # print("joint_actions", joint_actions)
        # print("state.agents_positions", state.agents_positions)
        # # calculate agent positions after applying the joint action
        agents_positions_after_joint_actions = []
        for i, agent_pos in enumerate(state.agents_positions):
            agent_pos_after_action = self.agent_position_after_action(agent_pos, joint_actions[i])
            agents_positions_after_joint_actions.append(agent_pos_after_action)
        return self.no_duplicate_in(agents_positions_after_joint_actions)

    
    def get_valid_joint_actions(self
                                , state: WorldState
                                , available_actions: Tuple[Tuple[Action, ...], ...]) -> Iterable[Tuple[Action, ...]]:
        """Yield all possible joint actions that can be taken from the given state.
        Hint: you can use `self.world.available_actions()` to get the available actions for each agent.
        """
        # print("available_actions", available_actions)
        # cartesian product of the agents' actions
        for joint_actions in product(*available_actions):
            # print("joint_actions", joint_actions)
           
            if self.are_valid_joint_actions(state, joint_actions):
                yield joint_actions
    
    def get_successor_state(self
                            , state: WorldState
                            , joint_actions: Tuple[Action, ...]) -> WorldState:
        """The successor state of the given state after applying the given joint actions."""
        self.world.set_state(state)
        self.world.step(list(joint_actions))
        successor_state = self.world.get_state()
        # print("successor_state", successor_state)
        return successor_state

    def get_successors(self
                       , state: WorldState
                       , visited: set = None
                       , objectives_reached_before_successor: list[Position] = None
                       ):
        # - N'oubliez pas de jeter un oeil aux méthodes de la classe World (set_state, done, step, available_actions, ...)
        # - Vous aurez aussi peut-être besoin de `from itertools import product`
        """Yield all possible states that can be reached from the given world state."""
        if visited is None: # for tests
            visited = set()
        self.nodes_expanded += 1
        real_state = self.world.get_state()
        self.world.set_state(state)
        # For each possible joint actions set (i.e. cartesian product of the agents' actions)
        available_actions = self.world.available_actions()
        valid_joint_actions = self.get_valid_joint_actions(state, available_actions)
        i = 0
        for joint_actions in valid_joint_actions:
            i += 1
            objectives_reached_by_successor = objectives_reached_before_successor
            try:
                successor_state = self.get_successor_state(state, joint_actions)
            except ValueError:
                # print("ValueError: World is done, cannot step anymore")
                continue
            if isinstance(self, CornerSearchProblem):
                objectives_reached_by_successor = self.update_corners_reached(copy.deepcopy(objectives_reached_before_successor)
                                                              , joint_actions
                                                              , successor_state.agents_positions
                                                              )
            elif isinstance(self, GemSearchProblem):
                objectives_reached_by_successor = self.update_gems_collected(copy.deepcopy(objectives_reached_before_successor)
                                                              , joint_actions
                                                              , successor_state.agents_positions
                                                              )
            if was(successor_state
                    , objectives_reached_by_successor
                   , visited):
                continue
            if isinstance(self, CornerSearchProblem):
                # Compute the cost of the new state
                cost = self.heuristic(successor_state, objectives_reached_by_successor)
                yield successor_state, joint_actions, cost, objectives_reached_by_successor
            elif isinstance(self, GemSearchProblem):
                # Compute the cost of the new state
                cost = self.heuristic(successor_state, objectives_reached_by_successor)
                yield successor_state, joint_actions, cost, objectives_reached_by_successor
            elif not isinstance(self, CornerSearchProblem) and not isinstance(self, GemSearchProblem):
                # Compute the cost of the new state
                cost = self.heuristic(successor_state)
                yield successor_state, joint_actions, cost #todo must not change for test
        self.world.set_state(real_state)

    def manhattan_distance(self, pos1: Position, pos2: Position) -> float:
        """The Manhattan distance between two positions"""
        return abs(pos1[0] - pos2[0]) + abs(pos1[1] - pos2[1])

    def average_manhattan_distance_from_agents_to_exits(self, state: WorldState) -> float:
        """The average Manhattan distance from each agent to each exit
        divided by the number of agents"""
        agents_positions = state.agents_positions
        exit_positions = self.world.exit_pos
        # For each agent, compute its Manhattan distance to each exit
        total_distance = 0
        for agent_pos in agents_positions:
            for exit_pos in exit_positions:
                total_distance += self.manhattan_distance(agent_pos, exit_pos)
        # Divide the total distance by the number of agents
        average_distance = total_distance / len(agents_positions)
        return average_distance
    
    def heuristic(self
                  , state: WorldState
                  , last_actions: Tuple[Action, ...] = None
                  ) -> float:
        """Manhattan distance for each agent to the closest exit"""
        agent_positions = self.world.agents_positions
        exit_positions = self.world.exit_pos
        total_distance = self.average_manhattan_distance_from_agents_to_exits(state)
        # problem:  if agent has to get away from exit to get around a wall to reach the exit, a star chooses for him to stay in place
        # tie breaking with the last actions 
        # a last action STAY agent (if he had other options, but we don't take that into account here for simplicity) could have moved
        # so he actually lost 1 turn
        # for each action, if it was STAY
        # and if the agent is not on an exit
        # , add 1 to the total distance
        if last_actions:
            for i, action in enumerate(last_actions):
                if action == Action.STAY and agent_positions[i] not in exit_positions:
                    total_distance += 1
        return total_distance

class CornerProblemState:
    def __init__(self, world_state: WorldState):
        self.agents_positions = world_state.agents_positions
        self.gems_collected = world_state.gems_collected
        self.world_state = world_state

class CornerSearchProblem(SimpleSearchProblem[WorldState]):
    """Problème qui consiste à passer par les quatre coins du World 
    puis d’atteindre une sortie."""
    def __init__(self, world: World):
        super().__init__(world)
        self.corners = [(0, 0), (0, world.width - 1), (world.height - 1, 0), (world.height - 1, world.width - 1)]
        self.initial_state = world.get_state()

    def update_corners_reached(self
                                 , corners_reached: list[Position]
                                    , joint_actions: Tuple[Action, ...]
                                    , agent_positions: list[Position]) -> list[Position]:
        """Update the list of corners reached"""

        for action in joint_actions:
            if action != Action.STAY:
                agent_position = agent_positions[joint_actions.index(action)]
                if agent_position not in corners_reached and agent_position in self.corners:
                    corners_reached.append(agent_position)

        return corners_reached

    def all_corners_reached(self
                            , state
                            , corners_reached: list[Position]) -> bool:
        """Whether all corners are reached"""
        return len(corners_reached) == len(self.corners)

    def is_goal_state(self
                      , state: WorldState
                      , corners_reached: list[Position]) -> bool:
        """Whether the given state is the goal state.
        True if all corners are reached and all agents are on exit tiles
        """
        return self.all_corners_reached(state, corners_reached) and SimpleSearchProblem.is_goal_state(self, state)

    def heuristic(self
                  , state: WorldState
                  , corners_reached: list[Position]
                  ) -> float:
        """"""
        agents_positions = state.agents_positions
        corners_to_reach = [corner for corner in self.corners if corner not in corners_reached]

        cost = balanced_multi_salesmen_greedy_tsp(corners_to_reach
                                                  , len(agents_positions)
                                                  , agents_positions
                                                  , self.world.exit_pos
                                                  )[2]
        return cost

class GemProblemState:
    """The state of the GemSearchProblem"""
    def __init__(self, world_state: WorldState):
        self.agents_positions = world_state.agents_positions
        self.gems_collected = world_state.gems_collected
        self.world_state = world_state

class GemSearchProblem(SimpleSearchProblem[WorldState]):
    """Modéliez le problème qui consiste à collecter toutes les gemmes de l’environnement 
    puis à rejoindre les cases de sortie"""
    def __init__(self, world: World):
        super().__init__(world)
        self.initial_state = world.get_state()

    def update_gems_collected(self
                                , gems_collected: list[Position]
                                , joint_actions: Tuple[Action, ...]
                                , agent_positions: list[Position]) -> list[Position]:
        """Update the list of gems collected"""
        for action in joint_actions:
            if action != Action.STAY:
                agent_position = agent_positions[joint_actions.index(action)]
                if agent_position not in gems_collected and agent_position in [pos for pos, gem in self.world.gems]:
                    gems_collected.append(agent_position)
        return gems_collected

    def all_gems_collected(self, state):
        return sum(state.gems_collected) == self.world.n_gems

    def is_goal_state(self
                      , state
                      ) -> bool:
        return self.all_gems_collected(state) and super().is_goal_state(state)

    def heuristic(self
                  , state: WorldState
                  , gems_collected
                  ) -> float:
        """The distance of each agent to each uncollected gem and to the closest exit
        when all gems are collected, the distance of each agent to the closest exit"""
        gems_to_collect = [gem[0] for gem in self.world.gems if not gem[0] in gems_collected]

        cost = balanced_multi_salesmen_greedy_tsp(gems_to_collect
                                                  , len(state.agents_positions)
                                                  , state.agents_positions
                                                  , self.world.exit_pos
                                                  )[2]
        return cost

    ----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\graph_search\problem.py -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\graph_search\search.py -----from dataclasses import dataclass
from typing import Optional
from lle import Action, World, WorldState

from problem import CornerSearchProblem, GemSearchProblem, SearchProblem, SimpleSearchProblem, serialize, was
from priority_queue import PriorityQueue
# import sys
# import auto_indent
# from utils import print_items

# sys.stdout = auto_indent.AutoIndent(sys.stdout)

@dataclass
class Solution:
    actions: list[tuple[Action]]

    @property
    def n_steps(self) -> int:
        return len(self.actions)

def is_empty(data_structure) -> bool:
    """Returns True if data_structure is empty, False otherwise"""
    if isinstance(data_structure, list):
        return len(data_structure) == 0
    elif isinstance(data_structure, set):
        return len(data_structure) == 0
    elif isinstance(data_structure, PriorityQueue):
        return data_structure.is_empty()

def check_goal_state(problem: SearchProblem
                     , current_state: WorldState
                     , actions: list[tuple[Action]]
                     , objectives_reached = None
                     ) -> bool:
    # Check if the current state is the goal state
    if isinstance(problem, CornerSearchProblem):
        current_state_is_goal_state = problem.is_goal_state(current_state, objectives_reached)
    else:
        current_state_is_goal_state = problem.is_goal_state(current_state)
    if current_state_is_goal_state:
        # print("Solution found!")
        print("nodes expanded: ", problem.nodes_expanded)
        # print("actions: ", actions)
        problem.path_size = len(actions)
        print( "n_steps: ", len(actions))
        return Solution(actions)

def get_initial_objectives_reached(problem: SearchProblem
                                        , initial_state: WorldState
                                        ) -> list[tuple[int, int]]:
    objectives_reached = []
    if isinstance(problem, CornerSearchProblem):
        for corner in problem.corners:
            if corner in initial_state.agents_positions:
                objectives_reached.append(corner)
    return objectives_reached
    
def tree_search(problem: SearchProblem, mode: str) -> Optional[Solution]:
    """Tree search algorithm.
    Args:
        problem: the problem to solve.
        mode: the search mode to use:
            - "dfs": Depth-First Search
            - "bfs": Breadth-First Search
            - "astar": A* Search
    Returns:
        A solution to the problem, or None if no solution exists.
    """
    initial_state = problem.initial_state
    actions = []
    cost = 0
    objectives_reached = get_initial_objectives_reached(problem, initial_state)
    if mode == "astar":
        data_structure = PriorityQueue() 
        if isinstance(problem, CornerSearchProblem) or isinstance(problem, GemSearchProblem):
            data_structure.push((initial_state
                                 , actions
                                 , objectives_reached)
                                , cost)
        else:
            data_structure.push((initial_state
                                 , actions
                                 )
                                , cost)
    else:
        data_structure = [(initial_state
                           , actions)]  #  to keep track of states
    visited = set()  # Set to keep track of visited states
    while not is_empty(data_structure):
        # Pop the top state from the data_structure
        if mode == "bfs":
            current_state, actions = data_structure.pop(0)
        else:
            if isinstance(problem, CornerSearchProblem) or isinstance(problem, GemSearchProblem):
                current_state, actions, objectives_reached = data_structure.pop()
            else:
                current_state, actions = data_structure.pop()
        # Check if the current state is in the visited set
        if was(current_state
               , objectives_reached
               , visited):
            continue
        solution = None
        if isinstance(problem, CornerSearchProblem):
            solution = check_goal_state(problem
                            , current_state
                            , actions
                            , objectives_reached)
        else:
            solution = check_goal_state(problem
                            , current_state
                            , actions
                            , None)
        if solution:
            return solution
        current_state_hashable = serialize(current_state, objectives_reached)
        visited.add(current_state_hashable)
        # Add successors to data_structure
        if isinstance(problem, CornerSearchProblem) or isinstance(problem, GemSearchProblem):
            successors = problem.get_successors(current_state
                                                ,visited
                                                ,objectives_reached)
        else:
            successors = problem.get_successors(current_state
                                            ,visited)
        for successor_tuple in successors:  
            if isinstance(problem, CornerSearchProblem) or isinstance(problem, GemSearchProblem):
                successor, successor_actions, cost, objectives_reached = successor_tuple
            else:
                successor, successor_actions, cost = successor_tuple
                objectives_reached = None
            new_actions = actions + [successor_actions]
            if mode == "astar":
                if isinstance(problem, CornerSearchProblem) or isinstance(problem, GemSearchProblem):
                    successor_cost = problem.heuristic(successor, objectives_reached)
                    total_cost = cost + successor_cost
                    data_structure.push((successor
                                         , new_actions
                                         , objectives_reached)
                                        , total_cost)
                else:
                    successor_cost = problem.heuristic(successor, successor_actions)
                    total_cost = cost + successor_cost
                    data_structure.push((successor
                                         , new_actions)
                                        , total_cost)
            else:
                data_structure.append((successor, new_actions))
    return None

def dfs(problem: SearchProblem) -> Optional[Solution]:
    """Depth-First Search"""
    return tree_search(problem, "dfs")

def bfs(problem: SearchProblem) -> Optional[Solution]:
    """Breadth-First Search"""
    return tree_search(problem, "bfs")

def astar(problem: SearchProblem) -> Optional[Solution]:
    """A* Search"""
    return tree_search(problem, "astar")

if __name__ == "__main__":
    world = World.from_file("cartes/corners")
    problem = CornerSearchProblem(world)
    solution = astar(problem)
    world.reset()
    corners = set([(0, 0), (0, world.width - 1), (world.height - 1, 0), (world.height - 1, world.width - 1)])
    for action in solution.actions:
        world.step(action)
        agent_pos = world.agents_positions[0]
        if agent_pos in corners:
            corners.remove(agent_pos)
    assert len(corners) == 0, f"The agent did not reach these corners: {corners}"----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\graph_search\search.py -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\graph_search\travel_sales_man.py -----# Adjusted code based on user input
import heapq
from typing import List, Tuple
import numpy as np
from priority_queue import PriorityQueueOptimized
from utils import min_distance_position

# Adapted Greedy Algorithm with Balanced Total Distances
def balanced_multi_salesmen_greedy_tsp(remaining_cities: List[Tuple[int, int]]
                                       , num_salesmen: int
                                       , start_cities: List[Tuple[int, int]]
                                       , finish_cities: List[Tuple[int, int]]):
    """Given a list of cities coordinates, returns a list of cities visited by each salesman
    in the order that minimizes the total distance traveled.
    """
    
    routes = {f"Salesman_{i+1}": [start_cities[i]] for i in range(num_salesmen)}
    distances = {f"Salesman_{i+1}": 0.0 for i in range(num_salesmen)}

    while remaining_cities:
        for salesman in routes.keys():
            if not remaining_cities:
                break
            
            current_city = routes[salesman][-1]
            nearest_city, nearest_distance = min_distance_position(routes[salesman][-1], remaining_cities)
            distances[salesman] += nearest_distance
            routes[salesman].append(nearest_city)
            remaining_cities.remove(nearest_city)

    for salesman in routes.keys():
        current_city = routes[salesman][-1]
        finish_city, final_distance = min_distance_position(current_city, finish_cities)
        distances[salesman] += final_distance
        routes[salesman].append(finish_city)
        
    total_distance = sum(distances.values())
    return routes, distances, total_distance


# Greedy Algorithm with Min Heap (Priority Queue)
def greedy_tsp_heap(cities_to_visit
                    , start_city=None
                    , finish_city=None):
    """Given a dict of cities and their coordinates, returns a list of cities
    visited in the order that minimizes the total distance traveled.
    Route: A→C→B→D→E→A
    Total Distance: 12.35
    Elapsed Time: 0.0000703 seconds"""

    road = [start_city]
    current_city = start_city
    total_distance = 0.0

    def distance(city1, city2):
        x1, y1 = cities_to_visit[city1]
        x2, y2 = cities_to_visit[city2]
        return np.sqrt((x2 - x1)**2 + (y2 - y1)**2)

    while cities_to_visit:
        heap = [(distance(current_city, city), city) for city in cities_to_visit]
        heapq.heapify(heap)
        nearest_distance, nearest_city = heapq.heappop(heap)
        total_distance += nearest_distance
        road.append(nearest_city)
        current_city = nearest_city
        cities_to_visit.remove(nearest_city)

    total_distance += distance(current_city, finish_city)
    road.append(finish_city)

    return road, total_distance


def greedy_tsp_optimized_pq(cities):
    """Given a dict of cities and their coordinates, returns a list of cities
    visited in the order that minimizes the total distance traveled.
    Elapsed Time: 0.0002046 seconds"""
    start_city = list(cities.keys())[0]
    road = [start_city]
    cities_to_visit = list(set(cities.keys()) - set(road))
    current_city = start_city
    total_distance = 0.0

    def distance(city1, city2):
        x1, y1 = cities[city1]
        x2, y2 = cities[city2]
        return np.sqrt((x2 - x1)**2 + (y2 - y1)**2)

    pq = PriorityQueueOptimized()

    while cities_to_visit:
        for city in cities_to_visit:
            pq.push(city, distance(current_city, city))
        
        nearest_city = pq.pop()
        nearest_distance = distance(current_city, nearest_city)
        total_distance += nearest_distance
        road.append(nearest_city)
        current_city = nearest_city
        cities_to_visit.remove(nearest_city)

    total_distance += distance(current_city, start_city)
    road.append(start_city)

    return road, total_distance----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\graph_search\travel_sales_man.py -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\src\adversarial_search.py -----from typing import List, Tuple
from lle import Action
from mdp import MDP, S, A
from world_mdp import BetterValueFunction, WorldMDP

WORLD_STEP_NOT_POSSIBLE_ERROR = "There is no more step to take"

def transition(mdp: MDP[A, S], state: S, action: A, depth: int = 0) -> S:
    """Returns the state reached by performing the given action in the given state."""
    if isinstance(mdp, BetterValueFunction):
        new_state = mdp.transition(state, action, depth)
    else:
        new_state = mdp.transition(state, action)
    if isinstance(mdp, WorldMDP):
        if mdp.was_visited(new_state):
            raise ValueError("was visited")
        mdp.add_to_visited(new_state)
    return new_state
            
        
def _max(mdp: MDP[A, S], state: S, max_depth: int, depth: int = 0) -> Tuple[float, A]:
    """Returns the value of the state and the action that maximizes it."""
    if mdp.is_final(state) or depth == max_depth :
        return state.value, None
    best_value = float('-inf')
    best_action = None
    mdp_available_actions = mdp.available_actions(state)
    for action in mdp_available_actions:
        try:
            new_state = transition(mdp, state, action, depth)
      
            if new_state.current_agent == 0:
                value, _ = _max(mdp, new_state, max_depth, depth + 1)
            else:
                value = _min(mdp, new_state, max_depth, depth + 1)
            if value > best_value:
                best_value = value
                best_action = action
            if isinstance(mdp, WorldMDP):
                mdp.remove_from_visited(new_state)
        except ValueError:
            # print(WORLD_STEP_NOT_POSSIBLE_ERROR)
            pass
    return best_value, best_action

def _min(mdp: MDP[A, S], state: S, max_depth: int, depth: int = 0) -> float:
    """Returns the worst value of the state."""
    if mdp.is_final(state) or depth == max_depth:
        return state.value
    worst_value = float('inf')
    for action in mdp.available_actions(state):
        try:
            new_state = transition(mdp, state, action, depth)
        
            if new_state.current_agent == 0:
                value, _ = _max(mdp, new_state, max_depth, depth + 1)
            else:
                value = _min(mdp, new_state, max_depth, depth)
            worst_value = min(worst_value, value)
            if isinstance(mdp, WorldMDP):
                mdp.remove_from_visited(new_state)
        except ValueError:
            # print(WORLD_STEP_NOT_POSSIBLE_ERROR)
            pass
        
    return worst_value

def minimax(mdp: MDP[A, S], state: S, max_depth: int) -> A:
    """Returns the action to be performed by Agent 0 in the given state. 
    This function only accepts 
    states where it's Agent 0's turn to play 
    and raises a ValueError otherwise. 
    Don't forget that there may be more than one opponent"""
    if state.current_agent != 0:
        raise ValueError("It's not Agent 0's turn to play")
    value, action = _max(mdp, state, max_depth, 0)
    return action

def _alpha_beta_max(mdp: MDP[A, S], state: S, alpha: float, beta: float, max_depth: int, depth: int = 0) -> Tuple[float, A, float, float]:
    if mdp.is_final(state) or depth == max_depth:
        return state.value, None
    best_value = float('-inf')
    best_action = None
    available_actions = mdp.available_actions(state)
    for action in available_actions:
        try:
            new_state = transition(mdp, state, action, depth)
    
            if new_state.current_agent == 0:
                value, _ = _alpha_beta_max(mdp, new_state, alpha, beta, max_depth, depth + 1)
            else:
                value = _alpha_beta_min(mdp, new_state, alpha, beta, max_depth, depth + 1)
            if value > best_value:
                best_value = value
                best_action = action
            if isinstance(mdp, WorldMDP):
                mdp.remove_from_visited(new_state)
            if beta <= best_value:  # Beta cutoff
                return best_value, best_action
            alpha = max(alpha, best_value)  # Update alpha after cutoff: fail hard
        except ValueError:
            # print(WORLD_STEP_NOT_POSSIBLE_ERROR)
            pass

    return best_value, best_action

def _alpha_beta_min(mdp: MDP[A, S], state: S, alpha: float, beta: float, max_depth: int, depth: int = 0) -> Tuple[float, A, float, float]:
    if mdp.is_final(state) or depth == max_depth:
        return state.value
    worst_value = float('inf')
    available_actions = mdp.available_actions(state)
    for action in available_actions:
        try:
            new_state = transition(mdp, state, action, depth)
    
            if new_state.current_agent == 0:
                value, _ = _alpha_beta_max(mdp, new_state, alpha, beta, max_depth, depth + 1)
            else:
                value = _alpha_beta_min(mdp, new_state, alpha, beta, max_depth, depth)
            worst_value = min(worst_value, value)
            if isinstance(mdp, WorldMDP):
                mdp.remove_from_visited(new_state)
            if worst_value <= alpha:  # Alpha cutoff
                return worst_value
            beta = min(beta, worst_value)  # Update beta after cutoff: fail hard
        except ValueError:
            # print(WORLD_STEP_NOT_POSSIBLE_ERROR)
            pass
    return worst_value

def alpha_beta(mdp: MDP[A, S]
               , state: S
               , max_depth: int) -> A: # todo good node ordering reduces time complexity to O(b^m/2)
    """The alpha-beta pruning algorithm 
    is an improvement over 
    minimax 
    that allows for pruning of the search tree."""
    # todo In maxn (Luckhardt and Irani, 1986), 
    # the extension of minimax to multi-player games
    # , pruning is not as successful.
    if state.current_agent != 0:
        raise ValueError("It's not Agent 0's turn to play")
    alpha = float('-inf')
    beta = float('inf')
    value, action = _alpha_beta_max(mdp, 
                                    state, 
                                    alpha, 
                                    beta, 
                                    max_depth, 
                                    0)
    return action

def _expectimax_max(mdp: MDP[A, S], 
                    state: S, 
                    max_depth: int, 
                    depth: int = 0
                    ) -> Tuple[float, A]:
    if mdp.is_final(state) or depth == max_depth:
        return state.value, None
    best_value = float('-inf')
    best_action = None
    for action in mdp.available_actions(state):
        try:
            new_state = transition(mdp, 
                                   state, 
                                   action, 
                                   depth
                                    )

            if new_state.current_agent == 0:
                value, _ = _expectimax_max(mdp, 
                                        new_state, 
                                        max_depth, 
                                        depth + 1
                                        )
            else:
                value = _expectimax_exp(mdp, new_state, max_depth, depth + 1)
            if value > best_value:
                best_value = value
                best_action = action
            if isinstance(mdp, WorldMDP):
                mdp.remove_from_visited(new_state)
        except ValueError:
            # print(WORLD_STEP_NOT_POSSIBLE_ERROR)
            pass
    return best_value, best_action

def _expectimax_exp(mdp: MDP[A, S],
                     state: S, 
                     max_depth: int, 
                     depth: int = 0
                    ) -> float:
    """Returns the expected value of the state.
    The expected value of a state is
    the average value of the state
    after all possible actions are performed.
    """
    if mdp.is_final(state) or depth == max_depth:
        return state.value
    total_value = 0
    num_actions = len(mdp.available_actions(state))
    for action in mdp.available_actions(state):
        try:
            new_state = transition(mdp, state, action, depth)
    
            if new_state.current_agent == 0:
                value, _ = _expectimax_max(mdp, new_state, max_depth, depth + 1)
            else:
                value = _expectimax_exp(mdp, new_state, max_depth, depth)
            total_value += value
            if isinstance(mdp, WorldMDP):
                mdp.remove_from_visited(new_state)
        except ValueError:
            # print(WORLD_STEP_NOT_POSSIBLE_ERROR)
            pass
    expected_value = total_value / num_actions if num_actions != 0 else 0
    return expected_value

def expectimax(mdp: MDP[A, S], 
               state: S, 
               max_depth: int
               ) -> Action:
    """ The 'expectimax' algorithm allows for 
    modeling the probabilistic behavior of humans 
    who might make suboptimal choices. 
    The nature of expectimax requires that we know 
    the probability that the opponent will take each action. 
    Here, we will assume that 
    the other agents take actions that are uniformly random."""
    if state.current_agent != 0:
        raise ValueError("It's not Agent 0's turn to play")
    _, action = _expectimax_max(mdp, state, max_depth, 0)
    return action
----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\src\adversarial_search.py -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\src\latextest.py -----import matplotlib.pyplot as plt

plt.rcParams['text.usetex'] = True
plt.rcParams['text.latex.preamble'] = r'\usepackage{amsmath}'

plt.figure()
plt.title(r'Test $\text{with\;\;\;spaces}$')
plt.show()
----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\src\latextest.py -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\src\map_display.py -----import cv2
from lle import World


map2023_10_29_13_48_15 = """.   G   S1 
L0E S0  .  
X   @   X  
"""
results2023_10_29_13_48_15 = {
"depth":  3 ,
"minimax":  27 ,
"minimax_with_better_value_function":  27 ,
"alpha_beta":  22 ,
"alpha_beta_with_better_value_function":  16 ,
"expectimax":  27 ,
"action_minimax":  " North " ,
"action_minimax_with_better_value_function":  " North " ,
"action_alpha_beta":  " North " ,
"action_alpha_beta_with_better_value_function":  " North " ,
"action_expectimax":  " North " ,
}

map2023_10_29_13_49_35 = """.  S0 S2 X 
.  .  G  . 
.  @  .  G 
.  .  S1 . 
.  .  X  X 
"""
results2023_10_29_13_49_35 = {
"depth":  4 ,
"minimax":  2706 ,
"minimax_with_better_value_function":  2706 ,
"alpha_beta":  312 ,
"alpha_beta_with_better_value_function":  366 ,
"expectimax":  2706 ,
"action_minimax":  " Stay " ,
"action_minimax_with_better_value_function":  " Stay " ,
"action_alpha_beta":  " Stay " ,
"action_alpha_beta_with_better_value_function":  " South " ,
"action_expectimax":  " South " ,
}

map2023_10_29_13_53_25 = """G  .  .  .  .  G 
X  .  G  S2 S0 G 
.  .  G  X  S1 X 
.  .  G  .  .  . 
"""
results2023_10_29_13_53_25 = {
"depth":  4 ,
"minimax":  3160 ,
"minimax_with_better_value_function":  3160 ,
"alpha_beta":  929 ,
"alpha_beta_with_better_value_function":  436 ,
"expectimax":  3160 ,
"action_minimax":  " East " ,
"action_minimax_with_better_value_function":  " East " ,
"action_alpha_beta":  " East " ,
"action_alpha_beta_with_better_value_function":  " East " ,
"action_expectimax":  " East " ,
}

map2023_10_29_13_55_47 = """.   .   L0S .   .   S2 
X   L1S .   S1  .   .  
X   .   .   @   S0  X  
"""
results2023_10_29_13_55_47 = {
"depth":  4 ,
"minimax":  701 ,
"minimax_with_better_value_function":  701 ,
"alpha_beta":  297 ,
"alpha_beta_with_better_value_function":  385 ,
"expectimax":  701 ,
"action_minimax":  " Stay " ,
"action_minimax_with_better_value_function":  " North " ,
"action_alpha_beta":  " Stay " ,
"action_alpha_beta_with_better_value_function":  " North " ,
"action_expectimax":  " Stay " ,
}

map2023_10_29_13_56_42 = """.  X  G 
@  @  S0
.  .  . 
.  .  . 
.  X  S1
"""

def display(map_str, map_name):
    world_instance = World(map_str)
    img = world_instance.get_image()
    cv2.imshow("Visualisation", img)
    cv2.waitKey(0) # Attend que l'utilisateur appuie sur 'enter'
    cv2.waitKey(1) # continue l'exécution du code
    #save img
    cv2.imwrite(map_name, img)

maps_names = ["map2023_10_29_13_48_15", "map2023_10_29_13_49_35", "map2023_10_29_13_53_25", "map2023_10_29_13_55_47", "map2023_10_29_13_56_42"]
map_names_png = ["map2023_10_29_13_48_15.png", "map2023_10_29_13_49_35.png", "map2023_10_29_13_53_25.png", "map2023_10_29_13_55_47.png", "map2023_10_29_13_56_42.png"]
maps = [map2023_10_29_13_48_15, map2023_10_29_13_49_35, map2023_10_29_13_53_25, map2023_10_29_13_55_47, map2023_10_29_13_56_42]
for map_str, map_name in zip(maps, map_names_png):
    display(map_str, map_name)----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\src\map_display.py -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\src\map_generator.py -----import random


class MapGenerator:
    def __init__(self
                 , map_width = 9
                 , map_height = 9
                 , map_type = "random"
                 ):
        self.map_width = map_width
        self.map_height = map_height
        self.map_type = map_type
        self.map = []
        self.map = self.generate_map_str()
    

    def generate_map_str(self):
        # Generate map based on map_type
        random_params = self.generate_random_params()
        random_map = self.generate_random_map(**random_params)
        # random_map_str = self.map_to_str(random_map)
        matrix_layout = self.matrix_to_layout(random_map)
        return matrix_layout        
        
    def generate_random_map(self
                               , rows=9
                               , cols=9
                               , num_agents=1
                               , num_gems=0
                               , num_lasers=0
                               ):
        """
        Generate a random map of dimensions (rows x cols) with given elements.
        
        Parameters:
        - rows: Number of rows
        - cols: Number of columns
        - num_agents: Number of agents (default is 1)
        - num_gems: Number of gems (default is 0)
        - num_lasers: Number of lasers (default is 0)
        
        Returns:
        - A list of lists representing the map
        """
        # if rows*cols < num_agents*2 + num_gems + num_lasers:
        #     rows = cols = num_agents*2 + num_gems + num_lasers
        # Initialize the map with floor tiles '.' and walls '@'
        map_grid = [['.' for _ in range(cols)] for _ in range(rows)]
        
        
        
        # Place agents' start positions
        for i in range(num_agents):
            placed = False
            while not placed:
                r, c = random.randint(0, rows-1), random.randint(0, cols-1)
                if map_grid[r][c] == '.':
                    map_grid[r][c] = f'S{i}'
                    placed = True
        
        # Place exits (same number as agents)
        for _ in range(num_agents):
            placed = False
            while not placed:
                r, c = random.randint(0, rows-1), random.randint(0, cols-1)
                if map_grid[r][c] == '.':
                    map_grid[r][c] = 'X'
                    placed = True
        
        # Place gems
        for _ in range(num_gems):
            for _ in range(3):
                r, c = random.randint(0, rows-1), random.randint(0, cols-1)
                if map_grid[r][c] == '.':
                    map_grid[r][c] = 'G'
                    break
        
        # Place lasers
        for i in range(num_lasers):
            for _ in range(3):
                r, c = random.randint(0, rows-1), random.randint(0, cols-1)
                if map_grid[r][c] == '.':
                    direction = random.choice(['N', 'S', 'E', 'W'])
                    map_grid[r][c] = f'L{i}{direction}'
                    break
        # Randomly place walls (20% of the map)
        for _ in range((rows * cols) // 5):
            r, c = random.randint(0, rows-1), random.randint(0, cols-1)
            if map_grid[r][c] == '.':
                map_grid[r][c] = '@'
        
        return map_grid

    # Function to generate random but reasonable parameters for the map
    def generate_random_params(self
                            #    , max_rows=4
                               , max_rows=6
                            #    , max_rows=9
                            #    , max_rows=5
                            #    , max_cols=4
                               , max_cols=6
                            #    , max_cols=9
                            #    , max_cols=5
                               , max_agents=3
                               , max_gems=6
                               , max_lasers=3):
        """
        Generate random but reasonable parameters for the map.
        
        Parameters:
        - max_rows: Maximum number of rows (default is 9)
        - max_cols: Maximum number of columns (default is 9)
        - max_agents: Maximum number of agents (default is 4)
        - max_gems: Maximum number of gems (default is 6)
        - max_lasers: Maximum number of lasers (default is 3)
        
        Returns:
        - A dictionary containing the generated parameters
        """
        rows_min = 3
        # rows_min = 5
        cols_min = 3
        # cols_min = 5
        rows = random.randint(rows_min
                              , max_rows)
        cols = random.randint(cols_min
                              , max_cols)
        max_agents = min(max_agents, rows*cols//2)
        num_agents = random.randint(2, max_agents)
        print("rows: ", rows)
        print("cols: ", cols)
        print("num_agents: ", num_agents)

        params = {
            "rows": rows,
            "cols": cols,
            "num_agents": num_agents,
            "num_gems": random.randint(0, max_gems),
            "num_lasers": random.randint(0, max_lasers)
        }
        
        return params

    def map_to_string(self
                      , map_grid):
        """
        Convert a given map into a string.
        
        Parameters:
        - map_grid: A list of lists representing the map.
        
        Returns:
        - A string representing the map.
        """
        return "\n".join(" ".join(row) for row in map_grid)
    
    def matrix_to_layout(self
                         ,matrix):
        """
        Convert a given matrix into a layout.
        
        Parameters:
        matrix (list of list of str): A matrix representing the layout.

        Returns:
        list of str: Each string represents a row in the layout.
        """
        # Determine the maximum length of any element in the matrix for alignment
        max_len = max(len(str(item)) for row in matrix for item in row)
        
        layout = """"""
        # layout += "\n"

        for row in matrix:
            # Align the elements by padding with spaces
            aligned_row = " ".join(str(item).ljust(max_len) for item in row)
            layout += aligned_row + "\n"
            
        return layout

----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\src\map_generator.py -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\src\mdp.py -----from typing import TypeVar, Generic
from abc import abstractmethod, ABC
from dataclasses import dataclass


@dataclass
class State(ABC):
    """
    State in an adversarial MDP.
    It must somehow know whose agent's turn it is.
    """

    value: float
    current_agent: int


A = TypeVar("A")
S = TypeVar("S", bound=State)


class MDP(ABC, Generic[A, S]):
    """Adversarial Markov Decision Process"""

    def __init__(self):
        super().__init__()
        self.n_expanded_states = 0

    @abstractmethod
    def reset(self) -> S:
        """Reset the MDP to its initial state and returns it."""

    @abstractmethod
    def available_actions(self, state: S) -> list[A]:
        """Returns the list of available actions for the current agent from the given state."""

    @abstractmethod
    def transition(self, state: S, action: A) -> S:
        """Returns the next state and the reward."""

    @abstractmethod
    def is_final(self, state: S) -> bool:
        """Returns whether the given state is final."""
----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\src\mdp.py -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\src\stock_tree.py -----import copy
import datetime
import os
import sys
from typing import List, Tuple

from lle import Action, World
from mdp import MDP, S, A

from world_mdp import BetterValueFunction, WorldMDP
from anytree import Node, RenderTree
from anytree.exporter import UniqueDotExporter


def stock_tree(mdp: MDP[A, S]
               , algorithm: str
                ) -> None:
    """Stocks the tree in a png file"""
    if isinstance(mdp, WorldMDP):
        if not os.path.exists('tree/current/'+algorithm):
            os.makedirs('tree/current/'+algorithm)
        UniqueDotExporter(mdp.root).to_picture("tree/current/"+algorithm+".png")
        print("tree stocked in tree/current/"+algorithm+".png")

        date_time = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
        if not os.path.exists('tree/'+algorithm):
            os.makedirs('tree/'+algorithm)
        UniqueDotExporter(mdp.root).to_picture("tree/"+algorithm+"/"+date_time+".png")
        print("tree stocked in tree/"+algorithm+"/"+date_time+".png")
----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\src\stock_tree.py -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\src\utils.py -----# def min_distance_position(position, positions):
#     # Create a cost vector
#     cost_vector = np.zeros(len(positions))
    
#     for i, point in enumerate(positions):
#         cost_vector[i] = ((position[0] - point[0]) ** 2 + (position[1] - point[1]) ** 2) ** 0.5

#     # Find the index of the minimum distance
#     min_index = np.argmin(cost_vector)

#     # Extract the closest position and its distance
#     closest_position = positions[min_index]
#     min_distance = cost_vector[min_index]

#     return closest_position, min_distance

from typing import List, Tuple


def get_distance(coord1, coord2):
    """Returns the distance between two coordinates"""
    x1, y1 = coord1
    x2, y2 = coord2
    return abs(x1 - x2) + abs(y1 - y2)

def min_distance_position(position : Tuple[int, int]
                          , positions: List[Tuple[int, int]] 
                            ) -> Tuple[Tuple[int, int], float]:
    """Returns the position in positions that is closest to position"""
    min_distance = float("inf")
    min_position = None
    for pos in positions:
        distance = 0
        distance = get_distance(position, pos)
        # print(distance)
        if distance < min_distance:
            min_distance = distance
            min_position = pos
    return min_position, min_distance

# def order_items()
# function to print visited set or stack items in terminal
def print_items(items
                , title="items:"
                , transform=None) -> None:
    """Prints items in terminal
    Args:
        items: items to print
    T is a generic type variable
    possible types for T:
    set, list, tuple, dict, etc."""
    print(title)
    i = 0
    for item in items:
        i += 1
        print(i, item)
    print("")


----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\src\utils.py -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\src\world_mdp.py -----import copy
from dataclasses import dataclass
import random
import sys
from typing import List, Optional, Tuple#, override
import lle
from lle import Position, World, Action
from mdp import A, MDP, State

# import auto_indent

from anytree import Node, RenderTree
from loguru import logger
import numpy as np
from scipy.optimize import linear_sum_assignment

# sys.stdout = auto_indent.AutoIndent(sys.stdout)

def get_distance(coord1, coord2):
    """Returns the distance between two coordinates"""
    x1, y1 = coord1
    x2, y2 = coord2
    return abs(x1 - x2) + abs(y1 - y2)

def min_distance_position(position : Tuple[int, int]
                          , positions: list[Tuple[int, int]] 
                            ) -> Tuple[Tuple[int, int], float]:
    """Returns the position in positions that is closest to position"""
    min_distance = float("inf")
    min_position = None
    for pos in positions:
        distance = 0
        distance = get_distance(position, pos)
        if distance < min_distance:
            min_distance = distance
            min_position = pos
    return min_position, min_distance

def min_distance_pairing(list_1
                             , list_2):
        # Create a cost matrix
        cost_matrix = np.zeros((len(list_1), len(list_2)))
        for i, point1 in enumerate(list_1):
            for j, point2 in enumerate(list_2):
                cost_matrix[i, j] = ((point1[0] - point2[0]) ** 2 + (point1[1] - point2[1]) ** 2) ** 0.5
        # Hungarian algorithm:
        # from cost_matrix, it does the pairing by minimizing the total distance
        row_ind, col_ind = linear_sum_assignment(cost_matrix)
        # Extract the paired points, their distances, and the minimum total distance
        paired_points = []
        distances = []
        min_total_distance = 0
        for i, j in zip(row_ind, col_ind):
            paired_points.append((list_1[i], list_2[j]))
            distances.append(cost_matrix[i, j])
            min_total_distance += cost_matrix[i, j]
        return paired_points, distances, min_total_distance


@dataclass
class MyWorldState(State):
    """Comme il s’agit d’un MDP à plusieurs agents et à tour par tour, 
    chaque état doit retenir à quel agent
    c’est le tour d’effectuer une action.
    """
    # la valeur d’un état correspond à la somme des rewards obtenues par les actions de l’agent 0 
    # (c’est-à-dire les gemmes collectées + arriver sur une case de ﬁn)
    value: float 
    current_agent: int
    last_action: Action
    agents_positions: list
    gems_collected: list[bool]
    value_vector: List[float]
    alpha: Optional[float] = None
    beta: Optional[float] = None
    # Add more attributes here if needed.
    def __init__(self
                 , value: float
                 , value_vector: List[float]
                 , current_agent: int
                 , world: World
                 , world_string: str = None
                 , last_action: Action = None
                 ):
        super().__init__(value, current_agent)
        self.world = world
        if world_string:
            self.world_string = world_string
        else:
            self.world_string = world.world_string
        self.agents_positions = world.agents_positions
        self.gems_collected = world.get_state().gems_collected
        self.value_vector = value_vector
        self.node = None
        if last_action:
            self.last_action = last_action
        else:
            self.last_action = None

    def get_agents_positions(self) -> list:
        # return self.agents_positions
        return self.world.agents_positions
    
    def layout_to_matrix(self
                         , layout):
        """
        Convert a given layout into a matrix where each first row of each line
        contains the (group of) character of the layout line.
        Parameters:
        layout (str): A multi-line string where each line represents a row in the layout.
        Returns:
        list of list of str: A matrix representing the layout.
        """
        # Split the layout into lines
        lines = layout.strip().split('\n')
        matrix = []
        max_cols = 0  # Keep track of the maximum number of columns
        # Convert each line into a row in the matrix
        for line in lines:
            row = [char for char in line.split() if char != ' ']
            matrix.append(row)
            max_cols = max(max_cols, len(row))
        # Fill in missing columns with '.'
        for row in matrix:
            while len(row) < max_cols:
                row.append('.')
        return matrix
    
    def matrix_to_layout(self
                         ,matrix):
        """
        Convert a given matrix into a layout.
        
        Parameters:
        matrix (list of list of str): A matrix representing the layout.

        Returns:
        list of str: Each string represents a row in the layout.
        """
        # Determine the maximum length of any element in the matrix for alignment
        max_len = max(len(str(item)) for row in matrix for item in row)
        layout = ""
        for row in matrix:
            # Align the elements by padding with spaces
            aligned_row = " ".join(str(item).ljust(max_len) for item in row)
            layout += aligned_row + "\n"
            
        return layout

    
    def update_world_string(self
                            ,current_agent: int
                            ,current_agent_previous_position: Position
                            ,action) -> None:
        """Updates world_string attribute with current world state:
        current agent position, gems collected, etc."""
        matrix = self.layout_to_matrix(self.world_string)
        if action != Action.STAY:
            agent_string = "S"+str(current_agent)
            matrix[current_agent_previous_position[0]][current_agent_previous_position[1]] = "."
            matrix[self.agents_positions[current_agent][0]][self.agents_positions[current_agent][1]] = agent_string
            matrix_after_action = matrix
            layout_after_action = self.matrix_to_layout(matrix_after_action)
            self.world_string = layout_after_action
            
    def to_string(self) -> str:
        """Returns a string representation of the state.
        with each state attribute on a new line."""
        # return f"current_agent: {self.current_agent}, value: {self.value}, value_vector: {self.value_vector}, agents_positions: {self.agents_positions}, gems_collected: {self.gems_collected}"
        state_attributes = f"current_agent: {self.current_agent}\n"
        
        if self.last_action :
            state_attributes += f"last_action: {self.last_action}\n"
        state_attributes += f"value: {self.value}\n"
        state_attributes += f"value_vector: {self.value_vector}\n"
        state_attributes += f"agents_positions: {self.agents_positions}\n"
        state_attributes += f"gems_collected: {self.gems_collected}\n"
        state_attributes += f"world: \n{self.world_string}\n"
        return state_attributes
    
    def serialize(self) -> tuple:
        """Serialize the given world state.
        Args:
            world_state: the world state to serialize.
        Returns:
            A tuple that represents the given world state.
        """
        return (tuple(self.agents_positions), tuple(self.gems_collected), self.current_agent)
    

class WorldMDP(MDP[Action, MyWorldState]):
    def __init__(self
                 , world: World):
        self.world = world
        world.reset()
        self.n_agents = world.n_agents

        self.initial_state = world.get_state()
        self.root = None

        self.visited = set() # visited states
        # nodes dict
        self.nodes = {} # key: state, value: node
        self.n_expanded_states = 0
        self.lasers_dangerous_for_agents = self.get_lasers_dangerous_for_agents()

    def get_lasers_dangerous_for_agents(self) -> list[list[Position]]:
        """Returns a list of lists
        , each corresponding to the agent of same index
        , containing positions of the lasers of a different agent_id (color)."""

        lasers_dangerous_for_agents = [[] for _ in range(self.world.n_agents)]
        laser_sources = self.world.laser_sources

        for laser_source in laser_sources:
            laser_source_position = laser_source[0]
            laser_source_agent_id = laser_source[1].agent_id
            #add the laser source position to the list of lasers dangerous for the agents of index different from laser_source_agent_id
            for agent_id in range(self.world.n_agents):
                if agent_id != laser_source_agent_id:
                    lasers_dangerous_for_agents[agent_id].append(laser_source_position)

        return lasers_dangerous_for_agents

    def reset(self):
        """The world.reset() method returns an initial state of the game. 
        After performing reset(), 
        it's Agent 0's turn to take an action. 
        Thus, world.transition(Action.NORTH) 
        will only move Agent 0 to the north, 
        while all other agents will remain in place. 
        Then, it's Agent 1's turn to move, and so on"""
        self.n_expanded_states = 0
        self.world.reset()
        return MyWorldState(0.0
                            , [0.0 for _ in range(self.world.n_agents)]
                            , 0
                            , self.world)

    def available_actions(self, state: MyWorldState) -> list[Action]:
        """returns the actions available to the current agent."""
        world_available_actions = state.world.available_actions()
        current_agent = state.current_agent
        current_agent_available_actions = world_available_actions[current_agent]
        return current_agent_available_actions
      
    def is_final(self, state: MyWorldState) -> bool:
        """returns True if the state is final, False otherwise."""
        return state.world.done and not [gem[0] for gem in state.world.gems if not gem[1].is_collected]

    def get_actions(self
                , current_agent: int
                , action: Action) -> list[Action]:
        """from current agent action, returns list with action at agent index and STAY at others's ."""
        actions = [Action.STAY for _ in range(self.world.n_agents)]
        actions[current_agent] = action
        return actions

    def convert_to_WorldState(self, state: MyWorldState) -> lle.WorldState:
        """Converts MyWorldState to lle.WorldState"""
        return lle.WorldState(state.agents_positions, state.gems_collected)
    
    def agents_each_on_different_exit_pos(self
                                          , state: MyWorldState) -> bool:
        """Whether each agent is on a different exit position."""
        agent_positions = set(state.world.agents_positions)  

        exit_positions = set(self.world.exit_pos)  
        # Intersect the sets to find agents that are on exit positions
        agents_on_exits = agent_positions.intersection(exit_positions)
        # Check if the number of agents on exits is equal to the total number of agents
        # and if each agent is on a different exit
        return len(agents_on_exits) == len(agent_positions) # and len(agents_on_exits) == len(exit_positions)

    def current_agent_on_exit(self
                              , state: MyWorldState
                              , current_agent: int
                              ) -> bool:
        """Whether the current agent is on an exit position."""
        current_agent_position = state.agents_positions[current_agent]
        return current_agent_position in self.world.exit_pos

    def add_to_visited(self
                          , state: MyWorldState) -> None:
        """Adds state to visited states."""
        self.visited.add(state.serialize())

    def remove_from_visited(self
                            , state: MyWorldState) -> None:
        """Removes state from visited states."""
        self.visited.remove(state.serialize())

    def was_visited(self,
                    state: MyWorldState) -> bool:
        return state.serialize() in self.visited
    
    def add_value_to_node(self
                          , state
                          , value: float
                          , discriminator: str
                          , alpha: float = None
                            , beta: float = None
                          ) -> None:
        """Adds value to node"""
        #add best_value to the node name
        new_state_string = state.to_string()
        new_state_string_with_best_value = new_state_string + "\n "+discriminator+" value : " + str(value)
        if alpha != None:
            new_state_string_with_best_value += "\n alpha : " + str(alpha)
        if beta != None:
            new_state_string_with_best_value += "\n beta : " + str(beta)
        if self.is_final(state):
            new_state_string_with_best_value += "\n FINAL"
        # replace
        self.nodes[new_state_string].name = new_state_string_with_best_value

    def transition(self
                   , state: MyWorldState
                   , action: Action
                   ) -> MyWorldState:
        """Returns the next state and the reward.
        If Agent 0 dies during a transition, 
        the state value immediately drops to 
        lle.REWARD_AGENT_DIED (-1), 
        without taking into account any gems already collected
        """
        self.n_expanded_states += 1
        simulation_world = copy.deepcopy(state.world)
        world_string = copy.deepcopy(state.world_string)
        simulation_world.set_state(self.convert_to_WorldState(state))

        simulation_state = simulation_world.get_state()
        simulation_state_current_agent = state.current_agent
        current_agent_previous_position = simulation_state.agents_positions[simulation_state_current_agent]
        actions = self.get_actions(simulation_state_current_agent, action)
        next_state_value_vector = copy.deepcopy(state.value_vector)
        reward = 0.0
        reward = simulation_world.step(actions)
        next_state_value_vector[simulation_state_current_agent] += reward
        if simulation_state_current_agent == 0:
            if reward == -1:
                next_state_value_vector[0] = -1.0 #lle.REWARD_AGENT_DIED
        next_state_current_agent = (simulation_state_current_agent+1)%simulation_world.n_agents
        my_world_state_transitioned = MyWorldState(next_state_value_vector[0]
                                                   , next_state_value_vector
                                                   , next_state_current_agent
                                                   , simulation_world
                                                   , world_string
                                                   , action
                                                   )
        my_world_state_transitioned.update_world_string(simulation_state_current_agent
                                                        , current_agent_previous_position
                                                        , actions)
        return my_world_state_transitioned
    

def balanced_multi_salesmen_greedy_tsp(remaining_cities: list[Tuple[int, int]]
                                       , num_salesmen: int
                                       , start_cities: list[Tuple[int, int]]
                                       , finish_cities: list[Tuple[int, int]]
                                       ) -> Tuple[dict[str, list[Tuple[int, int]]], dict[str, float], float]: 
    #todo: calculate the distance between the last city and the finish city one time at problem creation
    """Given a list of cities coordinates, returns a list of cities visited by each agent
    in the order that minimizes the total distance traveled.
    """
    routes = {f"agent_{i+1}": [start_cities[i]] for i in range(num_salesmen)}
    distances = {f"agent_{i+1}": 0.0 for i in range(num_salesmen)}
    while remaining_cities:
        for agent in routes.keys():
            if not remaining_cities:
                break
            nearest_city, nearest_distance = min_distance_position(routes[agent][-1], remaining_cities)
            distances[agent] += nearest_distance
            routes[agent].append(nearest_city)
            remaining_cities.remove(nearest_city)
    for agent in routes.keys():
        current_city = routes[agent][-1]
        finish_city, final_distance = min_distance_position(current_city, finish_cities)
        distances[agent] += final_distance
        routes[agent].append(finish_city)
    total_distance = sum(distances.values())
    return routes, distances, total_distance


class BetterValueFunction(WorldMDP):
    """Subclass of WorldMDP
    in which the state value
      is calculated more intelligently than simply considering Agent 0's score. 
     
        Improvements:

        If Agent 0 dies during a transition, 
            the state value is reduced by #todo
            , but the gems already collected are taken into account.
        The value of a state is increased by 
        the average of the score differences between Agent 0 and the other agents.."""
    def get_position_after_action(self
                                  , agent_pos: Tuple[int, int]
                                    , action: Action
                                    ) -> Tuple[int, int]:
        """Returns the position of the agent after performing the given action in the given state."""
        agent_pos_after_action = None
        # Apply the action to the agent's position
        if action == Action.NORTH:
            agent_pos_after_action = (agent_pos[0] - 1, agent_pos[1])
        elif action == Action.SOUTH:
            agent_pos_after_action = (agent_pos[0] + 1, agent_pos[1])
        elif action == Action.WEST:
            agent_pos_after_action = (agent_pos[0], agent_pos[1] - 1)
        elif action == Action.EAST:
            agent_pos_after_action = (agent_pos[0], agent_pos[1] + 1)
        elif action == Action.STAY:
            agent_pos_after_action = (agent_pos[0], agent_pos[1])
        else:
            raise ValueError("Invalid action")
        return agent_pos_after_action
    
    def get_available_actions_ordered(self
                                    , state: MyWorldState
                                    ) -> List[A]:
        """Returns the available actions ordered by heuristic value"""
        available_actions = super().available_actions(state)
        current_agent = state.current_agent
        # move STAY to the end of the list
        available_actions_ordered = [action for action in available_actions if action != Action.STAY]
        available_actions_ordered.append(Action.STAY)
        for action in available_actions:
            position_after_action = self.get_position_after_action(state.agents_positions[current_agent]
                                                                    , action
                                                                    )
            # if not all gems are collected,
            # not all (not gem for gem in state.gems_collected):
            gems_to_collect = [gem[0] for gem in state.world.gems if not gem[1].is_collected]

            if gems_to_collect:
                # if action leads to a gem, move it to the top of the list
                if position_after_action in [gem[0] for gem in state.world.gems]:
                    available_actions_ordered.remove(action)
                    available_actions_ordered.insert(0, action)
            # if a laser sources has not the same color as the agent, 
            if self.lasers_dangerous_for_agents[state.current_agent]:
                if position_after_action in [laser[0] for laser in state.world.lasers if laser[1].is_on]:
                    available_actions_ordered.remove(action)
                    available_actions_ordered.append(action)

        return available_actions_ordered

    def available_actions(self, state: MyWorldState) -> list[Action]:
        return self.get_available_actions_ordered(state)
    
    def transition(self
                   , state: MyWorldState
                   , action: Action
                   , depth: int = 0
                   ) -> MyWorldState:
        """Returns the next state and the reward.
        """
        # Change the value of the state here.
        state = super().transition(state
                                   , action
                                   )
        n_agents = self.world.n_agents
        previous_agent = (state.current_agent-1)%n_agents
        current_agent = previous_agent
        state_agents_positions = state.agents_positions
        value = state.value
        if value == -1 or value == 0:
            return state
        world_gems = state.world.gems
        gems_to_collect = [gem[0] for gem in world_gems if not gem[1].is_collected]
        _, distances, total_distance = balanced_multi_salesmen_greedy_tsp(copy.deepcopy(gems_to_collect)
                                                       , n_agents
                                                       , state_agents_positions
                                                       , self.world.exit_pos)
        current_agent_distance = distances[f"agent_{current_agent+1}"] # +1 because agent_0 is agent_1 #todo
        if current_agent_distance == 1:
            current_agent_distance = 1.5
        other_agents_distances = [distances[f"agent_{i+1}"] for i in range(n_agents) if i != current_agent]
        other_agents_average_distance_length = len(other_agents_distances)
        if other_agents_average_distance_length == 0:
            other_agents_average_distance_length = 1
        if gems_to_collect:
            # prefer current agent to be closer to the nearest gem
            # and other agents to be far from the nearest gem
            if current_agent_distance != 0:
                value = value + (len(gems_to_collect) / current_agent_distance)
        else:
            # prefer agent to be closer to the exit
            if current_agent_distance != 0:
                value = value + (lle.REWARD_AGENT_JUST_ARRIVED / current_agent_distance)
                # prefer all agents to be closer to the exit
                average_distance_to_exit = total_distance/n_agents
                # add reward for each agent being on exit lle.REWARD_AGENT_ON_EXIT/their distance to exit
                if other_agents_distances:
                    if all(distance == 0 for distance in other_agents_distances):
                        value = value + lle.REWARD_END_GAME/average_distance_to_exit
                else:
                    value = value + lle.REWARD_END_GAME*10/current_agent_distance
        state.value = value
        state.value_vector[current_agent] = value
        return state
    
----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\src\world_mdp.py -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\src\almost_equal.py -----def almost_equal(a, b):
    return abs(a - b) < 1e-6
----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\src\almost_equal.py -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\src\analysis.py -----from dataclasses import (
    dataclass,
)  # dataclass is a decorator that allows you to create a class with a minimal amount of code

# (see https://docs.python.org/3/library/dataclasses.html)


@dataclass
class Parameters:
    """Parameters for the MDP and the algorithm
    ModifiedRewardWorld.reward values"""

    reward_live: float
    """Reward for living at each time step"""

    gamma: float
    """Discount factor"""
    noise: float
    """Probability of taking a random action instead of the chosen one"""


def prefer_close_exit_following_the_cliff() -> Parameters:
    # A strategy focusing on reaching the closest exit quickly,
    # even if it means taking risks.
    return Parameters(
        reward_live=-1,  # negative to encourage speed,
        gamma=0.1,  # enough to value exit but not too much for preffering the close exit to the far one
        noise=0,  # low to avoid random actions and exploration
    )


def prefer_close_exit_avoiding_the_cliff() -> Parameters:
    # A cautious strategy aiming for the nearest exit while avoiding risks.
    return Parameters(
        reward_live=-1, gamma=0.1, noise=0.5  # noise is higher to encourage exploration
    )


def prefer_far_exit_following_the_cliff() -> Parameters:
    # A strategy that targets a distant exit but might involve risk-taking.
    return Parameters(
        reward_live=0,  # no hurry
        gamma=0.8,  # high for focusing on distant rewards,
        noise=0.2,  # moderate for some randomness in path selection
    )


def prefer_far_exit_avoiding_the_cliff() -> Parameters:
    # A strategy preferring a distant exit with an emphasis on safety and planning.
    # Less negative reward_live for longer routes,
    # high gamma for future-oriented planning, and
    # low noise for consistent decision-making.
    return Parameters(reward_live=1, 
                      gamma=0.8, 
                      noise=0.9
                      )  # prefer safe


def never_end_the_game() -> Parameters:
    # A unique strategy to avoid reaching terminal states and keep the game ongoing.

    return Parameters(
        reward_live=11,  # reward_live bigger than biggest reward to encourage continual play
        gamma=0,
        noise=0,
    )


if __name__ == "__main__":
    print("Testing analysis.py")
----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\src\analysis.py -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\src\auto_indent.py -----import sys
import inspect


class AutoIndent(object):
    def __init__(self, stream):
        self.stream = stream
        self.offset = 0
        self.frame_cache = {}

    def flush(self):
        pass

    def indent_level(self):
        i = 0
        base = sys._getframe(2)
        f = base.f_back
        while f:
            if id(f) in self.frame_cache:
                i += 1
            f = f.f_back
        if i == 0:
            # clear out the frame cache
            self.frame_cache = {id(base): True}
        else:
            self.frame_cache[id(base)] = True
        return i

    def write(self, stuff):
        # sys.stdout.reconfigure(encoding='utf-8')
        stuff = stuff.encode('utf-8', errors='replace').decode('utf-8')

        indentation = "  " * self.indent_level()

        def indent(l):
            if l:
                return indentation + l
            else:
                return l

        stuff = "\n".join([indent(line) for line in stuff.split("\n")])
        self.stream.write(stuff)
        # write stuff in file ./src/log.txt
        with open("./src/log.txt", "a") as f:
            # flush log.txt
            f.seek(0)
            f.write(stuff)

----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\src\auto_indent.py -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\src\graph_mdp.py -----import json
from dataclasses import dataclass
from mdp import MDP

Action = str
State = str


@dataclass
class Transition:
    source: State
    action: Action
    destination: State
    reward: float
    probability: float


class GraphMDP(MDP[str, str]):
    @staticmethod
    def from_json(filename: str) -> "GraphMDP":
        with open(filename, "r") as f:
            data = json.load(f)
            start_states = data["start_state"]
            end_states = data["end_states"]
            transitions = {}
            for source, actions in data["transitions"].items():
                transitions[source] = {}
                for action, destinations in actions.items():
                    transitions[source][action] = []
                    for dest in destinations:
                        transitions[source][action].append(
                            Transition(
                                source,
                                action,
                                dest["to"],
                                dest["reward"],
                                dest["probability"],
                            )
                        )
            return GraphMDP(start_states, end_states, transitions)

    def __init__(
        self,
        start_states: list[State],
        end_states: list[State],
        transitions: dict[State, dict[Action, list[Transition]]],
    ):
        self.start_states = set(start_states)
        self.end_states = set(end_states)
        self._transitions = transitions
        self._all_states = self.start_states.union(self.end_states).union(
            set(transitions.keys())
        )

    def is_final(self, state: State) -> bool:
        return state in self.end_states

    def transitions(self, state: State, action: Action) -> list[tuple[State, float]]:
        return [
            (t.destination, t.probability) for t in self._transitions[state][action]
        ]

    def available_actions(self, state: str):
        return self._transitions[state].keys()

    def reward(self, state: str, action: str, new_state) -> float:
        transitions = self._transitions[state][action]
        for t in transitions:
            if t.destination == new_state:
                return t.reward
        raise ValueError(
            f"Invalid transition from {state} to {new_state} with action {action}"
        )

    def states(self):
        return self._all_states
----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\src\graph_mdp.py -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\src\mdp.py -----from typing import TypeVar, Generic
from abc import abstractmethod, ABC


A = TypeVar("A")
S = TypeVar("S")


class MDP(ABC, Generic[S, A]):
    """Adversarial Markov Decision Process"""

    # gems_quantity = S.
    @abstractmethod
    def is_final(self, state: S) -> bool:
        """Returns true 
        if the given state is final (i.e. the game is over)."""


    @abstractmethod
    def available_actions(self, state: S) -> list[A]:
        """Returns the list of available actions for the current agent from the given state."""

    @abstractmethod
    def transitions(self, state: S, action: A) -> list[tuple[S, float]]:
        """Returns the list of next states with the probability of reaching it by performing the given action."""

    @abstractmethod
    def states(self) -> list[S]:
        """Returns the list of all states."""

    @abstractmethod
    def reward(self, state: S, action: A, new_state) -> float:
        """Reward function"""
        
----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\src\mdp.py -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\src\qlearning.py -----from rlenv import RLEnv, Observation
import numpy as np
import numpy.typing as npt

import sys

class QLearning:
    """Tabular QLearning"""

    def __init__(self, learning_rate: float, discount_factor: float, epsilon: float):
        # Initialize parameters
        self.learning_rate = learning_rate
        self.discount_factor = discount_factor
        self.epsilon = epsilon
        # Initialize Q-table
        self.q_table = {}

    def choose_action(self, state):
        # Implement action selection using epsilon-greedy strategy
        pass

    def update(self, state, action, reward, next_state):
        # Implement Q-table update
        pass

if __name__ == "__main__":
    print("Hello World")
    print(sys.path)----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\src\qlearning.py -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\src\value_iteration.py -----import copy
import sys

from lle import World, WorldState
from almost_equal import almost_equal
from graph_mdp import GraphMDP
from mdp import MDP, S, A
from typing import Generic

from auto_indent import AutoIndent
from world_mdp import WorldMDP
import utils

sys.stdout = AutoIndent(sys.stdout)


class ValueIteration(Generic[S, A]):
    def __init__(self, mdp: MDP[S, A], gamma: float):  # discount factor
        # senf.values est nécessaire pour fonctionner avec utils.show_values
        self.mdp = mdp
        self.gamma = gamma
        # self.values = dict[S, float]()
        self.values = {
            state: 0.0 for state in mdp.states()
        }  # Initialize all states with a default value
        # utils.show_values(self.values)
        # utils.

    def value(self, state: S) -> float:
        """Returns the value of the given state."""
        # return self.values[state]
        # return self.values.get(state, 0.0)  # Default value if state not found
        if state not in self.values:
            return 0.0
        return self.values.get(state)

    def policy(self, state: S) -> A:
        """Returns the action
        that maximizes the Q-value of the given state."""
        available_actions = self.mdp.available_actions(state)
        if not available_actions:
            print("No available actions for state", state)
            return None  # Or some default action if appropriate
        return max(available_actions, key=lambda action: self.qvalue(state, action))

    def qvalue(self, state: S, action: A) -> float:
        """
        Returns the Q-value
        of the given state-action pair
        based on the state values.
        from Bellman equation:
        Q(s,a) = Sum(P(s,a,s') * (R(s,a,s') + gamma * V(s')))
        """
        qvalue = 0.0
        next_states_and_probs = self.mdp.transitions(state, action)
        # print("next_states_and_probs: \n", next_states_and_probs, "\n")
        for next_state, prob in next_states_and_probs:
            reward = self.mdp.reward(state, action, next_state)
            # print("P(", state, action, next_state, "):", prob)
            # print("R(", state, action, next_state, "):", reward)
            next_state_value = self.value(next_state)
            # print("V(", next_state, "):", next_state_value)
            qvalue += prob * (reward + self.gamma * next_state_value)
        # print("Q-value of", state, action, ":", qvalue, "\n")
        return qvalue

    def _compute_value_from_qvalues(self, state: S) -> float:
        """
        Returns the value of the given state based on the Q-values.
        from Bellman equation:
        V(s) = max_a Sum(P(s,a,s') * (R(s,a,s') + gamma * V(s')))

        This is a private method,
        meant to be used by the value_iteration method.
        """
        # print("Computing value of", state)
        value = max(
            self.qvalue(state, action) for action in self.mdp.available_actions(state)
        )
        if value is None:
            return 0.0
        return value

    def get_values_at_position(self, i: int, j: int) -> list[float]:
        """Returns the values of the states at the given position."""
        # world_gems_quantity = self.mdp.
        states_at_position = [
            state for state in self.mdp.states() if state.agents_positions[0] == (i, j)
        ]
        values_at_position = [self.value(state) for state in states_at_position]
        increasing_values = sorted(values_at_position)

        return increasing_values

    def print_values_table(self, n: int = 0):
        """In a map's representation table,
        each tile contains the possible values at that position."""
        if not isinstance(self.mdp, WorldMDP):
            # print("Cannot print values table for non-world MDP")
            return None
        print("Iteration", n, "Values table: ")
        max_len = 0
        for i in range(self.mdp.world.height):
            for j in range(self.mdp.world.width):
                values = self.get_values_at_position(i, j)
                # Convert the list of values to a string and find the maximum length
                values_str = str(values)
                max_len = max(max_len, len(values_str))

        for i in range(self.mdp.world.height):
            for j in range(self.mdp.world.width):
                values = self.get_values_at_position(i, j)
                # Format each string to have the same width
                print(f"{str(values):<{max_len}}", end=" ")
            print()

    def print_iteration_values(self, iteration: int):
        """Prints the states and their values."""
        print("Iteration", iteration, "States and their values:")
        for state in self.mdp.states():
            print(state, self.value(state))

    def value_iteration(self, n: int):  # number of iterations
        """Performs value iteration for the given number of iterations."""
        for _ in range(n):
            # print("Iteration", _)
            new_values = copy.deepcopy(self.values)
            for state in self.mdp.states():  # All states generator (not a list)
                # print("State", state)
                if self.mdp.is_final(state):
                    # print("Final state", state)
                    new_values[state] = 0.0
                else:
                    new_values[state] = self._compute_value_from_qvalues(state)
            self.values = new_values
            self.print_values_table(_)
        # self.print_iteration_values(n)


if __name__ == "__main__":
    # graph
    # b - +1 - a - -1 - c
    # graph_file_name = "tests/graphs/graph1.json"
    # mdp = GraphMDP.from_json(graph_file_name)
    # gamma = 0.9
    # algo = ValueIteration(mdp, gamma)
    # # algo.value_iteration(10)
    # algo.value_iteration(100)
    # assert almost_equal(algo.qvalue("a", "left"), 0.6)  # no change from iteration 0
    # assert almost_equal(
    #     algo.qvalue("a", "right"), 0.90909090909
    # )  # more than iteration 0 & 1

    mdp = WorldMDP(
        World(
            """
    .  . . X
    .  @ . V
    S0 . . ."""
        )
    )
    algo = ValueIteration(mdp, 0.9)
    algo.value_iteration(100)
    expected = [
        [1.62, 1.80, 2.0, 0.0],
        [1.458, 0.0, 1.80, 0.0],
        [1.3122, 1.458, 1.62, 1.458],
    ]
    for i in range(mdp.world.height):
        for j in range(mdp.world.width):
            print("State", (i, j))
            state = WorldState([(i, j)], [])
            print("Value:", algo.value(state))
            assert almost_equal(algo.value(state), expected[i][j])
----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\src\value_iteration.py -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\src\world_mdp.py -----from itertools import product
from typing import Iterable
from lle import World, WorldState, Action
from mdp import MDP


class WorldMDP(MDP[WorldState, list[Action]]):
    def __init__(self, world: World):
        self.world = world

    def available_actions(self, state: WorldState):
        self.world.set_state(state)
        available = self.world.available_actions()
        return list(product(*available))

    def transitions(
        self, state: WorldState, action: list[Action]
    ) -> list[tuple[WorldState, float]]:
        self.world.set_state(state)
        self.world.step(action)
        return [(self.world.get_state(), 1.0)]

    def is_final(self, state: WorldState) -> bool:
        self.world.set_state(state)
        return self.world.done

    def reward(
        self,
        state: WorldState,
        action: list[Action],
        new_state: WorldState,
    ) -> float:
        # Step the world and check if the new state is the same as the given one
        # If if is not the same, then test all the other available actions.
        self.world.set_state(state)
        actions = [action] + [[a] for a in self.world.available_actions()[0]]
        for a in actions:
            r = self.world.step(a)
            if self.world.get_state() == new_state:
                return r
            self.world.set_state(state)
        raise ValueError("The new state is not reachable from the given state")

    def states(self) -> Iterable[WorldState]:
        all_positions = set(product(range(self.world.height), range(self.world.width)))
        all_positions = all_positions.difference(set(self.world.wall_pos))
        agents_positions = list(product(all_positions, repeat=self.world.n_agents))
        collection_status = list(product([True, False], repeat=self.world.n_gems))

        for pos, collected in product(agents_positions, collection_status):
            s = WorldState(list(pos), list(collected))
            yield s
----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\src\world_mdp.py -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\src\__init__.py ---------- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\src\__init__.py -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\tests\graph_mdp.py -----import json
from dataclasses import dataclass
from src.mdp import MDP

Action = str
State = str


@dataclass
class Transition:
    source: State
    action: Action
    destination: State
    reward: float
    probability: float


class GraphMDP(MDP[str, str]):
    @staticmethod
    def from_json(filename: str) -> "GraphMDP":
        with open(filename, "r") as f:
            data = json.load(f)
            start_states = data["start_state"]
            end_states = data["end_states"]
            transitions = {}
            for source, actions in data["transitions"].items():
                transitions[source] = {}
                for action, destinations in actions.items():
                    transitions[source][action] = []
                    for dest in destinations:
                        transitions[source][action].append(
                            Transition(
                                source,
                                action,
                                dest["to"],
                                dest["reward"],
                                dest["probability"],
                            )
                        )
            return GraphMDP(start_states, end_states, transitions)

    def __init__(
        self,
        start_states: list[State],
        end_states: list[State],
        transitions: dict[State, dict[Action, list[Transition]]],
    ):
        self.start_states = set(start_states)
        self.end_states = set(end_states)
        self._transitions = transitions
        self._all_states = self.start_states.union(self.end_states).union(
            set(transitions.keys())
        )

    def is_final(self, state: State) -> bool:
        return state in self.end_states

    def transitions(self, state: State, action: Action) -> list[tuple[State, float]]:
        return [
            (t.destination, t.probability) for t in self._transitions[state][action]
        ]

    def available_actions(self, state: str):
        return self._transitions[state].keys()

    def reward(self, state: str, action: str, new_state) -> float:
        transitions = self._transitions[state][action]
        for t in transitions:
            if t.destination == new_state:
                return t.reward
        raise ValueError(
            f"Invalid transition from {state} to {new_state} with action {action}"
        )

    def states(self):
        return self._all_states
----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\tests\graph_mdp.py -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\tests\modified_reward_world.py -----from .world_mdp import WorldMDP
from lle import Action, World, WorldState


class ModifiedRewardWorld(WorldMDP):
    def __init__(self, 
                 reward_live: float # Reward for living at each time step
                 ):
        super().__init__(World.from_file("tests/graphs/cliff"))
        self.world.reset() # Reset the world to its initial state
        self.reward_live = reward_live

    def reward(
        self, 
        state: WorldState, 
        action: list[Action], 
        new_state: WorldState
    ) -> float:
        reward = super().reward(state, action, new_state)
        # The agent has died
        if reward < 0:
            return -10.0
        # The agent has collected a gem
        # The agent has reached the exit
        if reward > 0:
            # Close exit
            if new_state.agents_positions[0] == (2, 2):
                return 1.0
            # Far exit
            return 10.0
        # The agent just lives (reward should be 0)
        
        return self.reward_live
----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\tests\modified_reward_world.py -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\tests\random_wrapper.py -----from src.mdp import MDP, S, A


class RandomWrapper(MDP[S, A]):
    """Wrapper around an MDP such that the action taken can be random with probability p.
    It only changes the `transitions` method. The other methods are unchanged."""

    def __init__(self, mdp: MDP[S, A], p: float):
        super().__init__()
        self.mdp = mdp
        self.p = p
        """The probability of the agent to perform a random action"""

    def is_final(self, state: S) -> bool:
        return self.mdp.is_final(state)

    def available_actions(self, state: S) -> list[A]:
        return self.mdp.available_actions(state)

    def transitions(self, state: S, action: A) -> list[tuple[S, float]]:
        if self.p == 0:
            return self.mdp.transitions(state, action)
        # Create the dictionary of "normal" destinations
        destination_probs = dict(self.mdp.transitions(state, action))
        # The probabilities must be multiplied by (1 - p) because of the wrapper
        for s, prob in destination_probs.items():
            destination_probs[s] = prob * (1 - self.p)
        # Add the random destinations
        available_actions = list(self.available_actions(state))
        n_actions = len(available_actions)
        for action in available_actions:
            for s, prob in self.mdp.transitions(state, action):
                # The probability of taking this action must be multiplied by (p / n_actions)
                prob = (prob * self.p) / n_actions
                destination_probs[s] = destination_probs.get(s, 0.0) + prob
        return list(destination_probs.items())

    def states(self) -> list[S]:
        return self.mdp.states()

    def reward(self, state: S, action: A, new_state) -> float:
        return self.mdp.reward(state, action, new_state)
----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\tests\random_wrapper.py -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\tests\test_analysis.py -----from src.analysis import (
    prefer_close_exit_avoiding_the_cliff,
    prefer_close_exit_following_the_cliff,
    prefer_far_exit_avoiding_the_cliff,
    prefer_far_exit_following_the_cliff,
    never_end_the_game,
    Parameters,
)
from src.mdp import MDP
from src.value_iteration import ValueIteration
from .random_wrapper import RandomWrapper
from .modified_reward_world import ModifiedRewardWorld


def setup_mdp_and_get_path(param: Parameters):
    mdp = ModifiedRewardWorld(param.reward_live)
    start_state = mdp.world.get_state()
    noisy_mdp = RandomWrapper(mdp, param.noise) # Add noise to the MDP
    algo = ValueIteration(noisy_mdp, param.gamma) # Create the algorithm 
    algo.value_iteration(100)
    return apply_policy(start_state, mdp, algo)


def apply_policy(state, 
                 mdp: MDP, 
                 algo: ValueIteration
                 ):
    """Apply the policy 
    until the end of the game 
    or a loop is detected. 
    Returns the path taken."""
    path = []
    while not mdp.is_final(state) and state not in path: # While the game is not over and the state is not in the path
        path.append(state)
        action = algo.policy(state)
        print(action)
        transitions = mdp.transitions(state, action) # Get the possible transitions
        assert len(transitions) == 1 # There is only one possible transition
        state, _ = transitions[0]
    path.append(state)
    print(path)
    return path


def test_close_exit_following_the_cliff():
    params = prefer_close_exit_following_the_cliff()
    path = setup_mdp_and_get_path(params)
    expected = [(3, 0), (3, 1), (3, 2), (2, 2)]
    assert len(path) == len(expected)
    for exp, state in zip(expected, path):
        assert state.agents_positions[0] == exp


def test_close_exit_avoiding_the_cliff():
    params = prefer_close_exit_avoiding_the_cliff()
    path = setup_mdp_and_get_path(params)
    expected = [(3, 0), (2, 0), (1, 0), (0, 0), (0, 1), (0, 2), (1, 2), (2, 2)]
    assert len(path) == len(expected)
    for exp, state in zip(expected, path):
        assert state.agents_positions[0] == exp


def test_far_exit_following_the_cliff():
    params = prefer_far_exit_following_the_cliff()
    path = setup_mdp_and_get_path(params)
    expected = [(3, 0), (3, 1), (3, 2), (3, 3), (3, 4), (2, 4)]
    assert len(path) == len(expected)
    for exp, state in zip(expected, path):
        assert state.agents_positions[0] == exp


def test_far_exit_avoiding_the_cliff():
    params = prefer_far_exit_avoiding_the_cliff()
    path = setup_mdp_and_get_path(params)
    assert len(path) == 10
    # This is only the start of the path because the second half could vary
    expected = [(3, 0), (2, 0), (1, 0), (0, 0), (0, 1), (0, 2)]
    for exp, state in zip(expected, path):
        assert state.agents_positions[0] == exp


def test_never_end():
    params = never_end_the_game()
    path = setup_mdp_and_get_path(params)
    # Check that there is a loop in the path
    assert path[-1] in path[:-1] # The last state is in the path before the last state
----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\tests\test_analysis.py -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\tests\test_value_iteration.py -----from src.value_iteration import ValueIteration
from lle import World, WorldState, Action, REWARD_AGENT_JUST_ARRIVED, REWARD_END_GAME
from tests.world_mdp import WorldMDP
from .graph_mdp import GraphMDP
from matplotlib import pyplot as plt


def almost_equal(a, b):
    return abs(a - b) < 1e-6


graph_file_name = "tests/graphs/graph1.json"

# 4-rl\tests\graphs\graph1.json
def test_value_0():
    mdp = GraphMDP.from_json(graph_file_name)
    algo = ValueIteration(mdp, 0.9)
    algo.value_iteration(0)
    for s in mdp.states():
        assert algo.value(s) == 0.0


def test_value_end_states():
    """
    tests that the value of end states is 0 after 100 iterations
    """
    mdp = GraphMDP.from_json(graph_file_name)
    algo = ValueIteration(mdp, 0.9)
    algo.value_iteration(100)
    for s in mdp.states():
        if mdp.is_final(s):
            assert algo.value(s) == 0.0


def test_qvalues_0():
    """
    test qvalues for the graph mdp after 0 iterations
    """
    mdp = GraphMDP.from_json(graph_file_name)
    algo = ValueIteration(mdp, 0.9)
    algo.value_iteration(0)
    assert almost_equal(algo.qvalue("a", "left"), 0.6)
    assert algo.qvalue("a", "right") == 0.5


def test_max_action_0():
    """
    test policy max_action for the graph mdp after 0 iterations
    """
    mdp = GraphMDP.from_json(graph_file_name)
    algo = ValueIteration(mdp, 0.9)
    algo.value_iteration(0)
    assert algo.policy("a") == "left"


def test_value_1():
    """
    test value of states after 1 iteration
    """
    mdp = GraphMDP.from_json(graph_file_name)
    gamma = 0.9
    algo = ValueIteration(mdp, gamma)
    algo.value_iteration(1)
    assert almost_equal(algo.qvalue("a", "left"), 0.6) # no change from iteration 0
    expected = 0.5 + 0.5 * gamma * 0.6 # 0.77 # = 0.5 + 0.5 * 0.9 * 0.6
    assert almost_equal(algo.qvalue("a", "right"), expected) # more than iteration 0


def test_value_100():
    """
    test value of states after 100 iterations
    """
    mdp = GraphMDP.from_json(graph_file_name)
    gamma = 0.9
    algo = ValueIteration(mdp, gamma)
    algo.value_iteration(100)
    assert almost_equal(algo.qvalue("a", "left"), 0.6) # no change from iteration 0
    assert almost_equal(algo.qvalue("a", "right"), 0.90909090909) # more than iteration 0 & 1


def test_value_world_0():
    """
    test value of world states after 0 iterations
    """
    mdp = WorldMDP(
        World(
            """
    .  . . X
    .  @ . V
    S0 . . ."""
        )
    )
    algo = ValueIteration(mdp, 0.9)
    algo.value_iteration(0)
    for s in mdp.states():
        assert algo.value(s) == 0.0


def test_qvalues_world():
    """
    test qvalues for the world mdp after 0 iterations
    """
    mdp = WorldMDP(
        World(
            """
    .  . . X
    .  @ . V
    S0 . . ."""
        )
    )
    algo = ValueIteration(mdp, 0.9)
    algo.value_iteration(0)
    state = WorldState([(0, 2)], [])
    assert (
        algo.qvalue(state, [Action.EAST]) == REWARD_END_GAME + REWARD_AGENT_JUST_ARRIVED
    )


def test_value_world_100():
    """
    test value of world states after 100 iterations
    """
    mdp = WorldMDP(
        World(
            """
    .  . . X
    .  @ . V
    S0 . . ."""
        )
    )
    algo = ValueIteration(mdp, 0.9)
    algo.value_iteration(100)
    expected = [
        [1.62, 1.80, 2.0, 0.0],
        [1.458, 0.0, 1.80, 0.0],
        [1.3122, 1.458, 1.62, 1.458],
    ]
    for i in range(mdp.world.height):
        for j in range(mdp.world.width):
            state = WorldState([(i, j)], [])
            assert almost_equal(algo.value(state), expected[i][j])


if __name__ == "__main__":
    print("hello world")
    test_value_0()
    test_value_end_states()
    test_qvalues_0()
    test_max_action_0()
    test_value_1()
    test_value_100()
    test_value_world_0()
    test_qvalues_world()
    test_value_world_100()
    print("ok")
----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\tests\test_value_iteration.py -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\tests\world_mdp.py -----from itertools import product
from typing import Iterable
from lle import World, WorldState, Action
from src.mdp import MDP


class WorldMDP(MDP[WorldState, list[Action]]):
    def __init__(self, world: World):
        self.world = world

    def available_actions(self, state: WorldState):
        self.world.set_state(state)
        available = self.world.available_actions()
        return list(product(*available))

    def transitions(
        self, 
        state: WorldState, 
        action: list[Action]
    ) -> list[tuple[WorldState, float]]:
        self.world.set_state(state)
        self.world.step(action)
        return [(self.world.get_state(), 1.0)]

    def is_final(self, state: WorldState) -> bool:
        self.world.set_state(state)
        return self.world.done

    def reward(
        self,
        state: WorldState,
        action: list[Action],
        new_state: WorldState,
    ) -> float:
        # Step the world and check if the new state is the same as the given one
        # If if is not the same, then test all the other available actions.
        self.world.set_state(state)
        actions = [action] + [[a] for a in self.world.available_actions()[0]]
        for a in actions:
            r = self.world.step(a)
            if self.world.get_state() == new_state:
                return r
            self.world.set_state(state) # Reset the world to the given state
        raise ValueError("The new state is not reachable from the given state")

    def states(self) -> Iterable[WorldState]:
        all_positions = set(product(range(self.world.height), range(self.world.width)))
        all_positions = all_positions.difference(set(self.world.wall_pos))
        agents_positions = list(product(all_positions, repeat=self.world.n_agents))
        collection_status = list(product([True, False], repeat=self.world.n_gems))

        for pos, collected in product(agents_positions, collection_status):
            s = WorldState(list(pos), list(collected))
            yield s
----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\tests\world_mdp.py -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\tests\__init__.py ---------- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\tests\__init__.py -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\tests\graphs\cliff -----.  . . . .
.  @ . . .
.  @ X @ X
S0 . . . .
V  V V V V----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\tests\graphs\cliff -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\tests\graphs\cliff.json -----{
    "states": [
        "A1",
        "A2",
        "A3",
        "A4",
        "A5",
        "B1",
        "B2",
        "B3",
        "B4",
        "B5",
        "C1",
        "C2",
        "C3",
        "C4",
        "C5",
        "D1",
        "D2",
        "D3",
        "D4",
        "D5",
        "E1",
        "E2",
        "E3",
        "E4",
        "E5"
    ],
    "start_state": "A2",
    "end_states": [
        "A1",
        "B1",
        "C1",
        "D1",
        "E1"
    ]
}----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\tests\graphs\cliff.json -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\tests\graphs\graph1.json -----{
    "states": [
        "a",
        "b",
        "c"
    ],
    "start_state": "a",
    "end_states": [
        "b",
        "c"
    ],
    "actions": [
        "left",
        "right"
    ],
    "transitions": {
        "a": {
            "left": [
                {
                    "to": "b",
                    "probability": 0.8,
                    "reward": 1
                },
                {
                    "to": "c",
                    "probability": 0.2,
                    "reward": -1
                }
            ],
            "right": [
                {
                    "to": "a",
                    "probability": 0.5,
                    "reward": 0
                },
                {
                    "to": "b",
                    "probability": 0.5,
                    "reward": 1
                }
            ]
        }
    }
}----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\tests\graphs\graph1.json -----

