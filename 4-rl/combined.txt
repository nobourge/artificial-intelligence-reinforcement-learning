----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2023-2024 projet RL for chatGPT.txt -----Dans ce projet, vous allez implémenter des algorithmes d’apprentissage par renforcement (reinforce-
ment learning). On vous fournit des fichiers de base pour le projet que vous pouvez les télécharger sur
l’université virtuelle.
2 Introduction
Intuitivement, un processus de décision markovien (Markov Decision Process, MDP) est composé d’é-
tats dans lesquels un agent effectue des actions qui ont une certaine probabilité d’atterrir dans un autre
état.
Après chaque transition, l’agent reçoit une récompense qui donne une indication sur la qualité de
l’action: plus la récompense est élevée, meilleure est l’action. Le but d’un agent est de maximiser la
somme des récompenses au cours d’un épisode (c’est-à-dire une partie).
Par conséquent, pour chaque état 𝑠, on peut calculer sa valeur 𝑉(𝑠) qui correspond à la meilleure
somme des récompenses possible à partir de 𝑠 grâce à l’équation de Bellman présentée dans l’E-
quation 1.
𝑉(𝑠) = max∑𝑃(𝑠,𝑎,𝑠′)[𝑅(𝑠,𝑎,𝑠′)+𝛾𝑉(𝑠′)] (1)
𝑎∈𝐴
3 Value iteration
L’algorithme de «value iteration» a pour but d’approximer itérativement la valeur 𝑉(𝑠) de chaque état
s à l’aide de l’Equation 2.
𝑉 = max∑𝑃(𝑠,𝑎,𝑠′)[𝑅(𝑠,𝑎,𝑠′)+𝛾𝑉 ]
𝑘+1 𝑘(𝑠′) (2)
𝑎
𝑠′
L’algorithme de value iteration prend en paramètre un entier 𝑘 qui détermine combien d’itération ef-
fectuer. Implémentez l’algorithme value iteration dans le fichier value_iteration.py.
Conseils:
• Quand vous implémentez l’algorithme, faites attention à vous baser sur 𝑉 pour calculer 𝑉
𝑘 𝑘+1
et à ne pas modifier 𝑉 durant l’itération.
𝑘
• N’oubliez pas que la valeur d’un état terminal est 0 par définition.
4 Trouvez les bons paramètres
Dans cet exercice, on vous demande de trouver les bons paramètres du MDP et de value iteration pour
induire un certain comportement à l’agent.
Le MDP considéré est celui présenté dans la Figure 1. Il s’agit d’un lle.World dont les rewards ont été
modifiées et qui comprend sept états terminaux:
• Cinq “ravins” en bas de la carte. L’agent meurt s’il s’y rend et reçoit une “récompense” de −10;
• Une sortie proche qui rapporte une récompense de 1;
• Une sortie lointaine qui rapporte une récompense de 10.
Figure 1: MDP pour lequel trouver les bons paramètres
Le but de cet exercice est d’induire les comportements suivants:
1. Préférer la sortie proche (+1) en longeant la falaise.
2. Préférer la sortie proche (+1) en évitant la falaise.
3. Préférer la sortie distante (+10) en longeant la falaise.
4. Préférer la sortie distante (+10) en évitant la falaise.
5. Eviter de terminer le jeu (l’épisode ne se termine jamais).
Pour ce faire, choisissez des valeurs adéquates de chaque comportement dans la fonction correspon-
dante pour reward_live (la récompense à chaque étape pour continuer le jeu), gamma (le discount fac-
tor) et noise (la probabilité de prendre une action aléatoire) dans le fichier analysis.py.
5 Q-learning
L’algorithme de Q-learning consiste à interagir avec l’environnement pour mettre à jour une fonction
d’évaluation 𝑄(𝑠,𝑎). Contrairement aux exercices précédents, on évalue ici la valeur d’une action dans
un état 𝑄(𝑠,𝑎) et pas la valeur de l’état 𝑉(𝑠).
Une fois que votre agent est entraîné, vous pouvez exploiter la stratégie apprise en prenant l’action
ayant la plus haute q-value max 𝑄(𝑠,𝑎) dans chaque état.
𝑎∈𝐴
Pour entraîner votre agent, il est nécessaire d’explorer l’environnement. Pour ce faire, vous devez im-
plémenter la stratégie d’exploration dite «𝜺-greedy» qui consiste à choisir une action aléatoire avec
une probabilité ε, et à prendre la meilleure action avec une probabilité 1−𝜀.
Note: Il n’y a quasiment pas de tests pour les exercices liés au q-learning car ceux-ci sont très diffi-
ciles à tester. On vous demande par contre d’analyser vos résultats dans le rapport (voir Section 6).
5.1 Q-learning tabulaire
Pour entraîner la fonction 𝑄(𝑠,𝑎), on emploie une méthode adaptée de l’équation de Bellman (Equa-
tion 1) illustrée dans l’Equation 3, où 𝛼 est le learning rate.
𝑄(𝑠,𝑎) ← (1−𝛼)𝑄(𝑠,𝑎)+𝛼[𝑅(𝑠,𝑎,𝑠′)+𝛾𝑉(𝑠′)] (3)
Implémentation
Implémentez l’algorithme de Q-learning dans le fichier qlearning.py. Faites en sorte que cette implé-
mentation utilise un dictionnaire pour stocker les qvalues de chaque état.
Conseils:
• Pour hasher des tableaux numpy, nous vous conseillons d’utiliser hash(array.tobytes()).
• Pour favoriser l’exploration, initialisez vos 𝑄(𝑠,𝑎) à 1 et non à 0.
Entraînement et rapport
Entraînez votre algorithme sur les niveaux 1, 3 et 6 de LLE. Dans votre rapport, créez un graphique
pour chacun de ces trois niveaux qui montre le score (c’est-à-dire la somme des rewards par épisode)
au cours de l’entrainement. Voir Section 6 pour plus d’informations sur le rapport.
Table 1: De gauche à droite: niveau 1, niveau 3 et niveau 6 de LLE
Exemple
L’extrait de code ci-dessous montre ce à quoi pourrait ressembler votre boucle d’entrainement. Il n’y
a aucune obligation de suivre ce canvas.
from lle import LLE
from rlenv.wrappers import TimeLimit
env = TimeLimit(LLE.level(1), 80) # Maximum 80 time steps
agents = [QAgent(lr, gamma, ...), QAgent(lr, gamma, ...), ...]
observation = env.reset()
done = truncated = False
score = 0
while not (done or truncated):
actions = [a.choose_action(observation) for a in agents]
next_observation, reward, done, truncated, info = env.step(actions)
for a in agents:
a.update(...)
score += reward
...
Lorsque vous récupérez une observation, vous pouvez accéder à son contenu avec observation.data
qui contient un tableau numpy dont la forme est (n_agents, ...).
Conseil: LLE est un environnement multi-agent et utilise la librairie rlenv prévue à cet effet. N’ou-
bliez pas de bien prendre en compte le caractère multi-agent dans la représentation de vos données.
5.2 Approximate Q-learning
Le but de cet exercice est d’implémenter l’Approximate Q-Learning (AQL) pour votre agent. Pour rap-
pel, AQL suppose l’existence d’une fonction 𝑓(𝑠) qui associe à chaque état (et pour chaque agent) un
vecteur de features [𝑓 (𝑠),𝑓 (𝑠),…,𝑓 (𝑠)] avec 𝑓 (𝑠) : 𝑆 → ℝ.
1 2 𝑛 𝑘
Concrètement, dans le cas de LLE, 𝑓 (𝑠) pourrait être le nombre de gemmes non collectées, 𝑓 (𝑠) la
1 2
distance (en lignes) à la gemme la plus proche, 𝑓 (𝑠) la distance (en colonnes) à la gemme la plus
3
proche, 𝑓 (𝑠) la présense (1) ou l’absence (0) d’un laser nord de l’agent, etc.
4
Dans AQL, la fonction 𝑄(𝑠,𝑎) d’un agent 𝑛 est définie comme indiqué dans l’Equation 4.
𝑘
𝑄 (𝑠,𝑎) = ∑𝑓 (𝑠)𝑤 (4)
𝑛 𝑖 𝑎,𝑖
𝑖=1
Dans le Q-learning, on définit la TD-error 𝛿 (temporal difference error) comme la différence entre la
Q-value estimée et la Q-value effective (estimée avec l’équation de Bellman).
𝛿 = [𝑅(𝑠,𝑎,𝑠′)+𝛾𝑉(𝑠′)]−𝑄(𝑠,𝑎) (5)
Dans AQL, le principe est de mettre à jour les poids 𝑤 de 𝑄 (𝑠,𝑎) de sorte à diminuer la TD-error 𝛿
𝑛
conformément à l’Equation 6
𝑤 ← 𝑤 +𝛼𝛿𝑓(𝑠) (6)
𝑎,𝑖 𝑎,𝑖
Implémentation
Dans le fichier approximate_qlearning.py:
1. Définissez une fonction feature_extraction(world) qui prend en entrée le World et renvoie
pour chaque agent les features que vous aurez choisies.
2. Implémentez l’algorithme de Approximate Q-Learning.
Entrainement et rapport
Entraînez votre algorithme AQL sur les niveaux 1, 3 et 6 de LLE. Dans votre rapport, créez un graphique
pour chacun de ces trois niveaux qui montre le score (c’est-à-dire la somme des rewards par épisode)
au cours de l’entrainement. Voir Section 6 pour plus d’informations sur le rapport.
6 Rapport
6.1 Value iteration
• Quelle limitation voyez-vous à l’algorithme de Value Iteration ? Dans quelle mesure est-il applic-
able à un niveau avec un nombre plus important d’agents ou avec une carte plus grande ?
• Expliquez pourquoi Value Iteration n’est pas un algorithme de RL mais plutôt un planificateur
hors-ligne.
• Appliquez l’algorithme de Value Iteration sur le niveau 1 de LLE jusqu’à ce que la policy se stabilise.
Ensuite, montrez graphiquement la stratégie découverte par l’algorithme.
6.2 Expériences avec le Q-learning
Le but ici est de mesurer les performances des deux algorithmes (Q-learning et AQL) dans les niveaux
évoqués précedemment. Vous devez ensuite présenter ces données dans des graphiques adaptés (type
de graphique, échelle, axes, titre).
Métrique
La métrique que vous devez tracer est le score, c’est-à-dire la somme des récompenses obtenues au
cours d’un épisode.
Méthode
Pour chaque expérience, donnez les paramètres que vous avez utilisés:
• le discount factor 𝛾
• le learning rate 𝛼
• la fonction de feature extraction 𝑓(𝑠)
• le niveau dont il est question
• la limite de temps que vous avez utilisée
• la manière dont évolue votre 𝜀 au cours du temps
• …
Graphiques
Nous vous conseillons la librairie matplotlib pour tracer des graphiques. Cette librairie est installée
automatiquement si vous avez utilisé poetry.
Les graphiques que vous devez produire sont ceux qui indiquent le score moyen au cours du temps,
calculé sur minimum 10 entrainements. Vos graphiques doivent aussi montrer la déviation standard
en plus de la moyenne, comme dans ce deuxième exemple de la documentation de matplotlib.
Remise
Le livrable de ce projet se présente sous la forme d’un fichier zip contenant vos sources python ainsi
que votre rapport en PDF. Nous vous encourageons à utiliser un outil tel que Typst ou Latex (par ex-
emple avec Overleaf) pour rédiger votre rapport.
Ce travail est individuel et doit être rendu sur l’Université Virtuelle pour le 10/12/2023 à 23:59.
7 Annexes
7.1 Précisions sur lle.LLE
La classe lle.LLE est directement dédiée au RL multi-agents (MARL) coopératif. Par soucis de général-
ité, LLE utilise le vocabulaire des observations plutôt que des états afin de différencier ce que les agents
voient de l’état de l’environnement.
Dans LLE, une observation peut se représenter couche par couche, comme montré dans la Figure 2.
Figure 2: Représentation d’une observation correspondant à la situation de l’imade de droite dans la
Table 1. Les cases noires représentent des 1, les cases grises des −1 et les cases blanches des 0. Par
soucis de concision, certaines couches (agent 2, agent 3, laser 2, laser 3) ont été ommise.
Le type d’observation indiqué dans la Figure 2 peut être utilisé en utilisant ObservationType.LAYERED.
Dans ce cas, chaque agent reçoit une observation (x, height, width) où x dépend du nombre d’a-
gents.
from lle import LLE, ObservationType
env = LLE.level(6, ObservationType.LAYERED)
print(env.observation_shape) # (12, 12, 13)
Cependant, par défaut (et pour pouvoir être utilisées dans des réseaux de neurones linéaires), LLE utilise
le type d’observation ObservationType.FLATTENED, qui encodent exactement les mêmes informations
que ObservationType.LAYERED, mais applaties sur une seule dimension.
from lle import LLE, ObservationType
env = LLE.level(6, ObservationType.FLATTENED) # On peut ommettre le second paramètre
print(env.observation_shape) # Affiche (1872,), càd 12 * 12 * 13
Notez que chaque observation possède aussi des informations sur l’état du World dans
observation.state, à la différence que le WorldState auquel vous êtes habitués a lui aussi été trans-
formé en tableau numpy pour être utilisable dans le contexte du deep MARL.
7----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2023-2024 projet RL for chatGPT.txt -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\src\almost_equal.py -----def almost_equal(a, b):
    return abs(a - b) < 1e-6
----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\src\almost_equal.py -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\src\analysis.py -----from dataclasses import (
    dataclass,
)  
@dataclass
class Parameters:
    """Parameters for the MDP and the algorithm
    ModifiedRewardWorld.reward values"""
    reward_live: float
    """Reward for living at each time step"""
    gamma: float
    """Discount factor"""
    noise: float
    """Probability of taking a random action instead of the chosen one"""

def prefer_close_exit_following_the_cliff() -> Parameters:
    # A strategy focusing on reaching the closest exit quickly,
    # even if it means taking risks.
    return Parameters(
        reward_live=-1,  # negative to encourage speed,
        gamma=0.1,  # enough to value exit but not too much for preffering the close exit to the far one
        noise=0,  # low to avoid random actions and exploration
    )

def prefer_close_exit_avoiding_the_cliff() -> Parameters:
    # A cautious strategy aiming for the nearest exit while avoiding risks.
    return Parameters(
        reward_live=-1, 
        gamma=0.1, 
        noise=0.5  # noise is higher to encourage exploration
    )

def prefer_far_exit_following_the_cliff() -> Parameters:
    # A strategy that targets a distant exit but might involve risk-taking.
    return Parameters(
        reward_live=0,  # no hurry
        gamma=0.8,  # high for focusing on distant rewards,
        noise=0.2,  # moderate for some randomness in path selection
    )

def prefer_far_exit_avoiding_the_cliff() -> Parameters:
    # A strategy preferring a distant exit with an emphasis on safety and planning.
    # Less negative reward_live for longer routes,
    # high gamma for future-oriented planning, and
    # low noise for consistent decision-making.
    return Parameters(reward_live=1, 
                      gamma=0.8, 
                      noise=0.9
                      )

def never_end_the_game() -> Parameters:
    # A unique strategy to avoid reaching terminal states and keep the game ongoing.
    return Parameters(
        reward_live=11,  # reward_live bigger than biggest reward to encourage continual play
        gamma=0,
        noise=0,
    )
----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\src\analysis.py -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\src\ApproximateQLearning.py -----# -Min 20 features pour 4 agents
----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\src\ApproximateQLearning.py -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\src\mdp.py -----from typing import TypeVar, Generic
from abc import abstractmethod, ABC


A = TypeVar("A")
S = TypeVar("S")


class MDP(ABC, Generic[S, A]):
    """Adversarial Markov Decision Process"""

    # gems_quantity = S.
    @abstractmethod
    def is_final(self, state: S) -> bool:
        """Returns true 
        if the given state is final (i.e. the game is over)."""


    @abstractmethod
    def available_actions(self, state: S) -> list[A]:
        """Returns the list of available actions for the current agent from the given state."""

    @abstractmethod
    def transitions(self, state: S, action: A) -> list[tuple[S, float]]:
        """Returns the list of next states with the probability of reaching it by performing the given action."""

    @abstractmethod
    def states(self) -> list[S]:
        """Returns the list of all states."""

    @abstractmethod
    def reward(self, state: S, action: A, new_state) -> float:
        """Reward function"""
        
----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\src\mdp.py -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\src\qagent.py -----# qlearning inheriting agent

from itertools import product
import numpy as np
import random
import sys
import os
import time
from rlenv import Observation, RLEnv
from typing import Dict, Tuple, List, Iterable, Generic, Optional, Callable, Set
from mdp import MDP, S, A
from auto_indent import AutoIndent
from world_mdp import WorldMDP
import utils
from lle import LLE, Action, Agent, AgentId, Position, WorldState
from rlenv.wrappers import TimeLimit

# import Action class :


sys.stdout = AutoIndent(sys.stdout)


# class QAgent(QLearning):
class QAgent:
    def __init__(
        self,
        # env: RLEnv,
        mdp: MDP[S, A],
        learning_rate: float = 0.1,
        discount_factor: float = 0.9,
        epsilon: float = 0.1,
        seed: int = None,
        id: AgentId = None,
    ):
        # Initialize the MDP
        self.mdp = mdp
        # Initialize parameters
        self.learning_rate = learning_rate
        self.discount_factor = discount_factor
        self.epsilon = epsilon
        self.id = id
        print("self.id:", self.id)

        # Initialize the environment
        # self.env = env

        # Initialize Q-table as a dictionary
        # Pour favoriser l’exploration, initialisez vos 𝑄(𝑠, 𝑎) à 1 et non à 0
        self.q_table = {
            state: {action.value: 1 for action in Action.ALL} for state in mdp.states()
        }  # dict of dicts

        # Initialize a random number generator
        self.rng = np.random.default_rng(seed)  # Random number generator instance

    def hash(self, state: WorldState, action: Action) -> Tuple[WorldState, Action]:
        return (state, action.value)

    def observe(self, observation: Observation):
        """Observe the given observation"""
        # Lorsque vous récupérez une observation, vous pouvez accéder à son contenu avec observation.data
        # qui contient un tableau numpy dont la forme est (n_agents, ...)

        observation_data = observation.data
        print("observation_data:", observation_data)

    def get_state_observation(
        self,
        state: WorldState,
    ) -> Observation:
        """Get the observation for the given state"""
        return Observation(state)

    def get_position(self, matrix: np.ndarray) -> np.ndarray:
        """Get the position of the agent"""
        # return np.where(matrix == 1)

        print("matrix:", matrix)
        print("np.nonzero(matrix):", np.nonzero(matrix))
        print("np.transpose(np.nonzero(matrix)):", np.transpose(np.nonzero(matrix)))
        return np.transpose(np.nonzero(matrix))

    def get_valid_actions(
        self,
        observation: Observation,
    ) -> List[Action]:
        """Get the list of valid actions for the given observation"""
        observation_data = observation.data
        print("observation_data:", observation_data)
        agent_position = self.get_position(observation_data[0][0])

        return world.available_actions()

    def agent_position_after_action(
        self, agent_pos: Position, action: Action
    ) -> Position:
        """The position of an agent after applying the given action."""
        try:
            print("agent_pos", agent_pos)
            print("action", action)
            agent_pos_after_action = agent_pos + action.delta
            print("agent_pos_after_action", agent_pos_after_action)
        except ValueError:
            raise ValueError("Invalid action")
        return agent_pos_after_action

    def are_valid_joint_actions(
        self, state: WorldState, joint_actions: Tuple[Action, ...]
    ) -> bool:
        """Whether the given joint actions are valid.
        an action is valid if it is available for an agent
        and if it does not lead the agent to be on the same position as another agent"""
        # print("are_valid_joint_actions()")
        # print("state", state)
        # print("joint_actions", joint_actions)
        # print("state.agents_positions", state.agents_positions)
        # # calculate agent positions after applying the joint action
        agents_positions_after_joint_actions = []
        for i, agent_pos in enumerate(state.agents_positions):
            agent_pos_after_action = self.agent_position_after_action(
                agent_pos, joint_actions[i]
            )
            agents_positions_after_joint_actions.append(agent_pos_after_action)
        return self.no_duplicate_in(agents_positions_after_joint_actions)

    def get_valid_joint_actions(
        self, state: WorldState, available_actions: Tuple[Tuple[Action, ...], ...]
    ) -> Iterable[Tuple[Action, ...]]:
        """Yield all possible joint actions that can be taken from the given state.
        Hint: you can use `self.world.available_actions()` to get the available actions for each agent.
        """
        # print("available_actions", available_actions)
        # cartesian product of the agents' actions
        for joint_actions in product(*available_actions):
            # print("joint_actions", joint_actions)

            if self.are_valid_joint_actions(state, joint_actions):
                yield joint_actions

    def get_agents_positions(
        self,
        #  state: np.ndarray
        observation: Observation,
    ) -> List[Position]:
        """Get the positions of all agents in the given observation.state"""
        observation_data = observation.data
        print("observation_data:", observation_data)
        state = observation.state
        print("state:", state)

        agents_positions = []
        for agent in self.mdp.world.agents:
            agent_position = self.get_position(state[agent.id])
            agents_positions.append(agent_position)

        return agents_positions

    def get_ones_indexes(
        self,
        array: np.ndarray,
    ) -> List[int]:
        """Get the indexes of all ones in the given array"""
        ones_indexes = []
        for i, value in enumerate(array):
            if value == 1:
                ones_indexes.append(i)
     
        print("ones_indexes:", ones_indexes)
        return ones_indexes

    def choose_action(
        self,
        observation: Observation,  # from instructions
    ):
        """Choose an action using the epsilon-greedy policy"""
        state = observation.state
        # #state type:
        # print("type(state):", type(state))
        # print("state:", state)
        # world = self.mdp.world
        # print("world:", world)

        # # valid_actions = self.get_valid_joint_actions(
        # # world_available_actions = self.mdp.world.available_actions()
        # world_available_actions = self.mdp.available_actions(observation.state)
        # print("world_available_actions:", world_available_actions)
        # valid_actions = self.mdp.world.available_actions()[self.id]
        # print("valid_actions:", valid_actions)
        # self.get_position(observation.data[0][0])
        observation_available_actions = observation.available_actions
        print("observation_available_actions:", observation_available_actions)
        current_agent_available_actions = observation_available_actions[self.id]

        valid_actions = self.get_ones_indexes(current_agent_available_actions)
        print("valid_actions:", valid_actions)
        if self.rng.uniform(0, 1) < self.epsilon:
            # Exploration: Random Action

            action = self.rng.choice(valid_actions)
        else:
            # Exploitation: Best known action
            state_actions = self.q_table.get(observation, {})
            if state_actions:
                action = max(state_actions, key=state_actions.get)
            else:
                action = self.rng.choice(valid_actions)
        return action

    def update(self, state, action, reward, next_state):
        """Update the Q-table using the Bellman equation"""
        # Get the current Q value
        current_q = self.q_table.get(state, {}).get(action, 1)  # 1 = default value
        # Find the max Q value for the actions in the next state
        next_state_actions = self.q_table.get(next_state, {})
        max_next_q = max(next_state_actions.values(), default=0)
        # Update the Q value using the Bellman equation
        new_q = current_q + self.learning_rate * (
            reward + self.discount_factor * max_next_q - current_q
        )
        # Update the Q-table
        self.q_table.setdefault(state, {})[action] = new_q


if __name__ == "__main__":
    # Create the environment
    env = LLE.level(1)
    # Create the MDP
    mdp = WorldMDP(env.world)
    print(mdp.world)

    # Create the agents
    agent = QAgent(mdp, AgentId(1))

    # # Train the agent
    # agent.train(env, episodes_quantity=100)
    # # Test the agent
    # agent.test(env, episodes_quantity=100)
    # # Save the agent
    # agent.save(
    #     "qlearning_agent.pkl"
    # )  # pkl = pickle = sérialisation de données en Python
----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\src\qagent.py -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\src\qlearning.py -----from lle import Agent, ObservationType
from rlenv import RLEnv, Observation
import numpy as np
import numpy.typing as npt
import sys
from mdp import MDP, S, A
from lle import LLE, Action, Agent, AgentId, WorldState
from rlenv.wrappers import TimeLimit
from qagent import QAgent
from auto_indent import AutoIndent
from world_mdp import WorldMDP

sys.stdout = AutoIndent(sys.stdout)


class QLearning:
    """Tabular QLearning"""

    def __init__(
        self,
        mdp: MDP[S, A],
        learning_rate: float,
        discount_factor: float,
        epsilon: float,
        seed: int = None,
    ):
        # Initialize parameters
        self.mdp = mdp
        self.learning_rate = learning_rate
        self.discount_factor = discount_factor
        self.epsilon = epsilon
        # Create the agents
        self.agents = [
            QAgent(mdp, learning_rate, discount_factor, epsilon, id=AgentId(i))
            # for i in range(env.world.n_agents)
            for i in range(mdp.world.n_agents)
        ]

    # # Initialize a random number generator
    # self.rng = np.random.default_rng(seed)  # Random number generator instance

    def train(self, agents, episodes_quantity: int):
        """Train the agent for the given number of episodes"""
        # from instructions:
        env = TimeLimit(
            LLE.level(1, ObservationType.LAYERED), 80
        )  # Maximum 80 time steps         # from instructions

        observation = env.reset()  # from instructions
        observation_data = observation.data
        observation_hash = self.numpy_table_hash(observation_data)
        print("observation_data:", observation_data)
        print("observation_hash:", observation_hash)

        done = truncated = False  # from instructions
        score = 0  # from instructions
        while not (done or truncated):  # from instructions
            actions = [  # from instructions
                a.choose_action(observation) for a in agents  # from instructions
            ]  # from instructions
            print("actions:", actions)
            # get action[0] type:
            print("type(actions[0]):", type(actions[0]))
            # north = Action(0)
            # print("north:", north)
            # south = Action(1)
            # print("south:", south)

            next_observation, reward, done, truncated, info = env.step(
                actions
            )  # from instructions
            print("observation:", next_observation)
            print("reward:", reward)
            print("done:", done)
            print("truncated:", truncated)
            print("info:", info)

            for a in agents:  # from instructions
                print("a:", a)
                print("a.id:", a.id)
                a.update(  # from instructions
                    observation, actions[a.id], reward, next_observation
                )
            score += reward  # from instructions
            print("score:", score)
            observation = next_observation

    def test(self, env: RLEnv, trained_agents, episodes_quantity: int):
        """Test the agent for the given number of episodes"""
        for episode in range(episodes_quantity):
            # Reset the environment
            observation = env.reset()
            done = False
            actions_taken = []
            while not done:
                actions = [a.choose_action(observation) for a in trained_agents]
                actions_taken.append(actions)
                next_observation, _, done = env.step(actions)
                observation = next_observation
            # Print the result of the episode
            if done:
                print(f"Episode {episode + 1} finished. Actions taken: {actions_taken}")
            else:
                print(f"Episode {episode + 1} did not finish.")

    def show(self):
        """Show the Q-table"""
        print(self.q_table)

    def __str__(self):
        """Return the Q-table as a string"""
        return str(self.q_table)

    def __repr__(self):
        """Return the Q-table as a string"""
        return str(self.q_table)

    def numpy_table_hash(self, numpy_table: npt.ArrayLike) -> int:
        """Return the hash of the Q-table as a numpy array"""
        # return np.array(list(self.q_table.items()), dtype=object).hash

        # using  hash(array.tobytes())
        return hash(np.array(list(numpy_table), dtype=object).tobytes())


if __name__ == "__main__":
    # Create the environment
    env = LLE.level(1, ObservationType.LAYERED)
    mdp = WorldMDP(env.world)
    print("mdp.world :", mdp.world)

    # Train the agent
    qlearning = QLearning(mdp, 0.1, 0.9, 0.1)
    trained_agents = qlearning.train(qlearning.agents, episodes_quantity=100)
    # Test the agents
    qlearning.test(env, trained_agents, episodes_quantity=1)
    # Save the agent
----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\src\qlearning.py -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\src\qlearning_training.py -----# Entraînez votre algorithme sur les niveaux 1, 3 et 6 de LLE. Dans votre rapport, créez un graphique
# pour chacun de ces trois niveaux qui montre le score (c’est-à-dire la somme des rewards par épisode)
# au cours de l’entrainement. 


from rlenv import RLEnv


def train_qlearning_agent(env: RLEnv, episodes_quantity: int):
    """Train the agent for the given number of episodes"""
    # Create the agent
    agent = QLearningAgent(env)
    # Train the agent
    agent.train(env, episodes_quantity)
    # Test the agent
    agent.test(env, episodes_quantity)
    # Return the agent
    return agent

----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\src\qlearning_training.py -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\src\value_iteration.py -----import copy
import sys

from lle import World, WorldState
from almost_equal import almost_equal
from graph_mdp import GraphMDP
from mdp import MDP, S, A
from typing import Generic

from auto_indent import AutoIndent
from world_mdp import WorldMDP
import utils

sys.stdout = AutoIndent(sys.stdout)


class ValueIteration(Generic[S, A]):
    def __init__(self, mdp: MDP[S, A], gamma: float):  # discount factor
        # senf.values est nécessaire pour fonctionner avec utils.show_values
        self.mdp = mdp
        self.gamma = gamma
        # Initialize all states as dictionary keys
        # with a default value of 0.0
        self.values = {state: 0.0 for state in mdp.states()}
        # utils.show_values(self.values)
        # utils.

    def value(self, state: S) -> float:
        """Returns the value of the given state."""
        # return self.values[state]
        # return self.values.get(state, 0.0)  # Default value if state not found
        if state not in self.values:
            return 0.0
        return self.values.get(state)

    def policy(self, state: S) -> A:
        """Returns the action
        that maximizes the Q-value of the given state."""
        available_actions = self.mdp.available_actions(state)
        if not available_actions:
            print("No available actions for state", state)
            return None  # Or some default action if appropriate
        return max(available_actions, key=lambda action: self.qvalue(state, action))

    def qvalue(self, state: S, action: A) -> float:
        """
        Returns the Q-value
        of the given state-action pair
        based on the state values.
        from Bellman equation:
        Q(s,a) = Sum(P(s,a,s') * (R(s,a,s') + gamma * V(s')))
        """
        qvalue = 0.0
        next_states_and_probs = self.mdp.transitions(state, action)
        # print("next_states_and_probs: \n", next_states_and_probs, "\n")
        for next_state, prob in next_states_and_probs:
            reward = self.mdp.reward(state, action, next_state)
            # print("P(", state, action, next_state, "):", prob)
            # print("R(", state, action, next_state, "):", reward)
            next_state_value = self.value(next_state)
            # print("V(", next_state, "):", next_state_value)
            qvalue += prob * (reward + self.gamma * next_state_value)
        # print("Q-value of", state, action, ":", qvalue, "\n")
        return qvalue

    def _compute_value_from_qvalues(self, state: S) -> float:
        """
        Returns the value of the given state based on the Q-values.
        from Bellman equation:
        V(s) = max_a Sum(P(s,a,s') * (R(s,a,s') + gamma * V(s')))

        This is a private method,
        meant to be used by the value_iteration method.
        """
        # print("Computing value of", state)
        value = max(
            self.qvalue(state, action) for action in self.mdp.available_actions(state)
        )
        if value is None:
            return 0.0
        return value

    def get_values_at_position(self, i: int, j: int) -> list[float]:
        """Returns the values of the states at the given position."""
        # world_gems_quantity = self.mdp.
        states_at_position = [
            state for state in self.mdp.states() if state.agents_positions[0] == (i, j)
        ]
        values_at_position = [self.value(state) for state in states_at_position]
        increasing_values = sorted(values_at_position)

        return increasing_values

    def print_values_table(self, n: int = 0):
        """In a map's representation table,
        each tile contains the possible values at that position."""
        if not isinstance(self.mdp, WorldMDP):
            # print("Cannot print values table for non-world MDP")
            return None
        print("Iteration", n, "Values table: ")
        max_len = 0
        for i in range(self.mdp.world.height):
            for j in range(self.mdp.world.width):
                values = self.get_values_at_position(i, j)
                # Convert the list of values to a string and find the maximum length
                values_str = str(values)
                max_len = max(max_len, len(values_str))

        for i in range(self.mdp.world.height):
            for j in range(self.mdp.world.width):
                values = self.get_values_at_position(i, j)
                # Format each string to have the same width
                print(f"{str(values):<{max_len}}", end=" ")
            print()

    def print_iteration_values(self, iteration: int):
        """Prints the states and their values."""
        print("Iteration", iteration, "States and their values:")
        for state in self.mdp.states():
            print(state, self.value(state))

    def value_iteration(self, n: int):  # number of iterations
        """Performs value iteration for the given number of iterations."""
        for _ in range(n):
            # print("Iteration", _)
            new_values = copy.deepcopy(self.values)
            for state in self.mdp.states():  # All states generator (not a list)
                # print("State", state)
                if self.mdp.is_final(state):
                    # print("Final state", state)
                    new_values[state] = 0.0
                else:
                    new_values[state] = self._compute_value_from_qvalues(state)
            self.values = new_values
            self.print_values_table(_)
        # self.print_iteration_values(n)


if __name__ == "__main__":
    # graph
    # b - +1 - a - -1 - c
    # graph_file_name = "tests/graphs/graph1.json"
    # mdp = GraphMDP.from_json(graph_file_name)
    # gamma = 0.9
    # algo = ValueIteration(mdp, gamma)
    # # algo.value_iteration(10)
    # algo.value_iteration(100)
    # assert almost_equal(algo.qvalue("a", "left"), 0.6)  # no change from iteration 0
    # assert almost_equal(
    #     algo.qvalue("a", "right"), 0.90909090909
    # )  # more than iteration 0 & 1
    mdp = WorldMDP(
        World(
            """
    .  . . X
    .  @ . V
    S0 . . ."""
        )
    )
    algo = ValueIteration(mdp, 0.9)
    algo.value_iteration(100)
    expected = [
        [1.62, 1.80, 2.0, 0.0],
        [1.458, 0.0, 1.80, 0.0],
        [1.3122, 1.458, 1.62, 1.458],
    ]
    for i in range(mdp.world.height):
        for j in range(mdp.world.width):
            print("State", (i, j))
            state = WorldState([(i, j)], [])
            print("Value:", algo.value(state))
            assert almost_equal(algo.value(state), expected[i][j])
----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\src\value_iteration.py -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\src\world_mdp.py -----from itertools import product
from typing import Iterable
from lle import World, WorldState, Action
from mdp import MDP


class WorldMDP(MDP[WorldState, list[Action]]):
    def __init__(self, world: World):
        self.world = world

    def available_actions(self, state: WorldState):
        self.world.set_state(state)
        available = self.world.available_actions()
        return list(product(*available))

    def transitions(
        self, state: WorldState, action: list[Action]
    ) -> list[tuple[WorldState, float]]:
        self.world.set_state(state)
        self.world.step(action)
        return [(self.world.get_state(), 1.0)]

    def is_final(self, state: WorldState) -> bool:
        self.world.set_state(state)
        return self.world.done

    def reward(
        self,
        state: WorldState,
        action: list[Action],
        new_state: WorldState,
    ) -> float:
        # Step the world and check if the new state is the same as the given one
        # If if is not the same, then test all the other available actions.
        self.world.set_state(state)
        actions = [action] + [[a] for a in self.world.available_actions()[0]]
        for a in actions:
            r = self.world.step(a)
            if self.world.get_state() == new_state:
                return r
            self.world.set_state(state)
        raise ValueError("The new state is not reachable from the given state")

    def states(self) -> Iterable[WorldState]:
        all_positions = set(product(range(self.world.height), range(self.world.width)))
        all_positions = all_positions.difference(set(self.world.wall_pos))
        agents_positions = list(product(all_positions, repeat=self.world.n_agents))
        collection_status = list(product([True, False], repeat=self.world.n_gems))

        for pos, collected in product(agents_positions, collection_status):
            s = WorldState(list(pos), list(collected))
            yield s
----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\src\world_mdp.py -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\tests\modified_reward_world.py -----from .world_mdp import WorldMDP
from lle import Action, World, WorldState


class ModifiedRewardWorld(WorldMDP):
    def __init__(self, 
                 reward_live: float # Reward for living at each time step
                 ):
        super().__init__(World.from_file("tests/graphs/cliff"))
        self.world.reset() # Reset the world to its initial state
        self.reward_live = reward_live

    def reward(
        self, 
        state: WorldState, 
        action: list[Action], 
        new_state: WorldState
    ) -> float:
        reward = super().reward(state, action, new_state)
        # The agent has died
        if reward < 0:
            return -10.0
        # The agent has collected a gem
        # The agent has reached the exit
        if reward > 0:
            # Close exit
            if new_state.agents_positions[0] == (2, 2):
                return 1.0
            # Far exit
            return 10.0
        # The agent just lives (reward should be 0)
        
        return self.reward_live
----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\tests\modified_reward_world.py -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\tests\random_wrapper.py -----from src.mdp import MDP, S, A


class RandomWrapper(MDP[S, A]):
    """Wrapper around an MDP such that the action taken can be random with probability p.
    It only changes the `transitions` method. The other methods are unchanged."""

    def __init__(self, mdp: MDP[S, A], p: float):
        super().__init__()
        self.mdp = mdp
        self.p = p
        """The probability of the agent to perform a random action"""

    def is_final(self, state: S) -> bool:
        return self.mdp.is_final(state)

    def available_actions(self, state: S) -> list[A]:
        return self.mdp.available_actions(state)

    def transitions(self, state: S, action: A) -> list[tuple[S, float]]:
        if self.p == 0:
            return self.mdp.transitions(state, action)
        # Create the dictionary of "normal" destinations
        destination_probs = dict(self.mdp.transitions(state, action))
        # The probabilities must be multiplied by (1 - p) because of the wrapper
        for s, prob in destination_probs.items():
            destination_probs[s] = prob * (1 - self.p)
        # Add the random destinations
        available_actions = list(self.available_actions(state))
        n_actions = len(available_actions)
        for action in available_actions:
            for s, prob in self.mdp.transitions(state, action):
                # The probability of taking this action must be multiplied by (p / n_actions)
                prob = (prob * self.p) / n_actions
                destination_probs[s] = destination_probs.get(s, 0.0) + prob
        return list(destination_probs.items())

    def states(self) -> list[S]:
        return self.mdp.states()

    def reward(self, state: S, action: A, new_state) -> float:
        return self.mdp.reward(state, action, new_state)
----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\tests\random_wrapper.py -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\tests\test_analysis.py -----from src.analysis import (
    prefer_close_exit_avoiding_the_cliff,
    prefer_close_exit_following_the_cliff,
    prefer_far_exit_avoiding_the_cliff,
    prefer_far_exit_following_the_cliff,
    never_end_the_game,
    Parameters,
)
from src.mdp import MDP
from src.value_iteration import ValueIteration
from .random_wrapper import RandomWrapper
from .modified_reward_world import ModifiedRewardWorld


def setup_mdp_and_get_path(param: Parameters):
    mdp = ModifiedRewardWorld(param.reward_live)
    start_state = mdp.world.get_state()
    noisy_mdp = RandomWrapper(mdp, param.noise) # Add noise to the MDP
    algo = ValueIteration(noisy_mdp, param.gamma) # Create the algorithm 
    algo.value_iteration(100)
    return apply_policy(start_state, mdp, algo)


def apply_policy(state, 
                 mdp: MDP, 
                 algo: ValueIteration
                 ):
    """Apply the policy 
    until the end of the game 
    or a loop is detected. 
    Returns the path taken."""
    path = []
    while not mdp.is_final(state) and state not in path: # While the game is not over and the state is not in the path
        path.append(state)
        action = algo.policy(state)
        print(action)
        transitions = mdp.transitions(state, action) # Get the possible transitions
        assert len(transitions) == 1 # There is only one possible transition
        state, _ = transitions[0]
    path.append(state)
    print(path)
    return path


def test_close_exit_following_the_cliff():
    params = prefer_close_exit_following_the_cliff()
    path = setup_mdp_and_get_path(params)
    expected = [(3, 0), (3, 1), (3, 2), (2, 2)]
    assert len(path) == len(expected)
    for exp, state in zip(expected, path):
        assert state.agents_positions[0] == exp


def test_close_exit_avoiding_the_cliff():
    params = prefer_close_exit_avoiding_the_cliff()
    path = setup_mdp_and_get_path(params)
    expected = [(3, 0), (2, 0), (1, 0), (0, 0), (0, 1), (0, 2), (1, 2), (2, 2)]
    assert len(path) == len(expected)
    for exp, state in zip(expected, path):
        assert state.agents_positions[0] == exp


def test_far_exit_following_the_cliff():
    params = prefer_far_exit_following_the_cliff()
    path = setup_mdp_and_get_path(params)
    expected = [(3, 0), (3, 1), (3, 2), (3, 3), (3, 4), (2, 4)]
    assert len(path) == len(expected)
    for exp, state in zip(expected, path):
        assert state.agents_positions[0] == exp


def test_far_exit_avoiding_the_cliff():
    params = prefer_far_exit_avoiding_the_cliff()
    path = setup_mdp_and_get_path(params)
    assert len(path) == 10
    # This is only the start of the path because the second half could vary
    expected = [(3, 0), (2, 0), (1, 0), (0, 0), (0, 1), (0, 2)]
    for exp, state in zip(expected, path):
        assert state.agents_positions[0] == exp


def test_never_end():
    params = never_end_the_game()
    path = setup_mdp_and_get_path(params)
    # Check that there is a loop in the path
    assert path[-1] in path[:-1] # The last state is in the path before the last state
----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\tests\test_analysis.py -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\tests\test_value_iteration.py -----from src.value_iteration import ValueIteration
from lle import World, WorldState, Action, REWARD_AGENT_JUST_ARRIVED, REWARD_END_GAME
from tests.world_mdp import WorldMDP
from .graph_mdp import GraphMDP
from matplotlib import pyplot as plt


def almost_equal(a, b):
    return abs(a - b) < 1e-6


graph_file_name = "tests/graphs/graph1.json"

# 4-rl\tests\graphs\graph1.json
def test_value_0():
    mdp = GraphMDP.from_json(graph_file_name)
    algo = ValueIteration(mdp, 0.9)
    algo.value_iteration(0)
    for s in mdp.states():
        assert algo.value(s) == 0.0


def test_value_end_states():
    """
    tests that the value of end states is 0 after 100 iterations
    """
    mdp = GraphMDP.from_json(graph_file_name)
    algo = ValueIteration(mdp, 0.9)
    algo.value_iteration(100)
    for s in mdp.states():
        if mdp.is_final(s):
            assert algo.value(s) == 0.0


def test_qvalues_0():
    """
    test qvalues for the graph mdp after 0 iterations
    """
    mdp = GraphMDP.from_json(graph_file_name)
    algo = ValueIteration(mdp, 0.9)
    algo.value_iteration(0)
    assert almost_equal(algo.qvalue("a", "left"), 0.6)
    assert algo.qvalue("a", "right") == 0.5


def test_max_action_0():
    """
    test policy max_action for the graph mdp after 0 iterations
    """
    mdp = GraphMDP.from_json(graph_file_name)
    algo = ValueIteration(mdp, 0.9)
    algo.value_iteration(0)
    assert algo.policy("a") == "left"


def test_value_1():
    """
    test value of states after 1 iteration
    """
    mdp = GraphMDP.from_json(graph_file_name)
    gamma = 0.9
    algo = ValueIteration(mdp, gamma)
    algo.value_iteration(1)
    assert almost_equal(algo.qvalue("a", "left"), 0.6) # no change from iteration 0
    expected = 0.5 + 0.5 * gamma * 0.6 # 0.77 # = 0.5 + 0.5 * 0.9 * 0.6
    assert almost_equal(algo.qvalue("a", "right"), expected) # more than iteration 0


def test_value_100():
    """
    test value of states after 100 iterations
    """
    mdp = GraphMDP.from_json(graph_file_name)
    gamma = 0.9
    algo = ValueIteration(mdp, gamma)
    algo.value_iteration(100)
    assert almost_equal(algo.qvalue("a", "left"), 0.6) # no change from iteration 0
    assert almost_equal(algo.qvalue("a", "right"), 0.90909090909) # more than iteration 0 & 1


def test_value_world_0():
    """
    test value of world states after 0 iterations
    """
    mdp = WorldMDP(
        World(
            """
    .  . . X
    .  @ . V
    S0 . . ."""
        )
    )
    algo = ValueIteration(mdp, 0.9)
    algo.value_iteration(0)
    for s in mdp.states():
        assert algo.value(s) == 0.0


def test_qvalues_world():
    """
    test qvalues for the world mdp after 0 iterations
    """
    mdp = WorldMDP(
        World(
            """
    .  . . X
    .  @ . V
    S0 . . ."""
        )
    )
    algo = ValueIteration(mdp, 0.9)
    algo.value_iteration(0)
    state = WorldState([(0, 2)], [])
    assert (
        algo.qvalue(state, [Action.EAST]) == REWARD_END_GAME + REWARD_AGENT_JUST_ARRIVED
    )


def test_value_world_100():
    """
    test value of world states after 100 iterations
    """
    mdp = WorldMDP(
        World(
            """
    .  . . X
    .  @ . V
    S0 . . ."""
        )
    )
    algo = ValueIteration(mdp, 0.9)
    algo.value_iteration(100)
    expected = [
        [1.62, 1.80, 2.0, 0.0],
        [1.458, 0.0, 1.80, 0.0],
        [1.3122, 1.458, 1.62, 1.458],
    ]
    for i in range(mdp.world.height):
        for j in range(mdp.world.width):
            state = WorldState([(i, j)], [])
            assert almost_equal(algo.value(state), expected[i][j])


if __name__ == "__main__":
    print("hello world")
    test_value_0()
    test_value_end_states()
    test_qvalues_0()
    test_max_action_0()
    test_value_1()
    test_value_100()
    test_value_world_0()
    test_qvalues_world()
    test_value_world_100()
    print("ok")
----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\tests\test_value_iteration.py -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\tests\world_mdp.py -----from itertools import product
from typing import Iterable
from lle import World, WorldState, Action
from src.mdp import MDP


class WorldMDP(MDP[WorldState, list[Action]]):
    def __init__(self, world: World):
        self.world = world

    def available_actions(self, state: WorldState):
        self.world.set_state(state)
        available = self.world.available_actions()
        return list(product(*available))

    def transitions(
        self, 
        state: WorldState, 
        action: list[Action]
    ) -> list[tuple[WorldState, float]]:
        self.world.set_state(state)
        self.world.step(action)
        return [(self.world.get_state(), 1.0)]

    def is_final(self, state: WorldState) -> bool:
        self.world.set_state(state)
        return self.world.done

    def reward(
        self,
        state: WorldState,
        action: list[Action],
        new_state: WorldState,
    ) -> float:
        # Step the world and check if the new state is the same as the given one
        # If if is not the same, then test all the other available actions.
        self.world.set_state(state)
        actions = [action] + [[a] for a in self.world.available_actions()[0]]
        for a in actions:
            r = self.world.step(a)
            if self.world.get_state() == new_state:
                return r
            self.world.set_state(state) # Reset the world to the given state
        raise ValueError("The new state is not reachable from the given state")

    def states(self) -> Iterable[WorldState]:
        all_positions = set(product(range(self.world.height), range(self.world.width)))
        all_positions = all_positions.difference(set(self.world.wall_pos))
        agents_positions = list(product(all_positions, repeat=self.world.n_agents))
        collection_status = list(product([True, False], repeat=self.world.n_gems))

        for pos, collected in product(agents_positions, collection_status):
            s = WorldState(list(pos), list(collected))
            yield s
----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\tests\world_mdp.py -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\tests\graphs\cliff -----.  . . . .
.  @ . . .
.  @ X @ X
S0 . . . .
V  V V V V----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\tests\graphs\cliff -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\tests\graphs\cliff.json -----{
    "states": [
        "A1",
        "A2",
        "A3",
        "A4",
        "A5",
        "B1",
        "B2",
        "B3",
        "B4",
        "B5",
        "C1",
        "C2",
        "C3",
        "C4",
        "C5",
        "D1",
        "D2",
        "D3",
        "D4",
        "D5",
        "E1",
        "E2",
        "E3",
        "E4",
        "E5"
    ],
    "start_state": "A2",
    "end_states": [
        "A1",
        "B1",
        "C1",
        "D1",
        "E1"
    ]
}----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\tests\graphs\cliff.json -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\tests\graphs\graph1.json -----{
    "states": [
        "a",
        "b",
        "c"
    ],
    "start_state": "a",
    "end_states": [
        "b",
        "c"
    ],
    "actions": [
        "left",
        "right"
    ],
    "transitions": {
        "a": {
            "left": [
                {
                    "to": "b",
                    "probability": 0.8,
                    "reward": 1
                },
                {
                    "to": "c",
                    "probability": 0.2,
                    "reward": -1
                }
            ],
            "right": [
                {
                    "to": "a",
                    "probability": 0.5,
                    "reward": 0
                },
                {
                    "to": "b",
                    "probability": 0.5,
                    "reward": 1
                }
            ]
        }
    }
}----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\tests\graphs\graph1.json -----

