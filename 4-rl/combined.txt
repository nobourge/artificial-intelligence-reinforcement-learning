----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\pyproject.toml -----
[tool.poetry]
name = "4-rl"
version = "0.1.0"
description = ""
authors = ["Yannick Molinghen <yannick.molinghen@ulb.be>"]
readme = "README.md"

[tool.poetry.dependencies]
python = ">=3.10, <3.13"
matplotlib = "3.6"
seaborn = "^0.12.2"
pytest = "^7.4.2"
gymnasium = "^0.29.1"
laser-learning-environment = "0.1.8"
opencv-python = "^4.8.1.78"
rlenv = { git = "https://github.com/yamoling/rlenv", tag = "v0.4.4" }
numpy = "^1.21.2"
pandas = "^1.3.3"
scipy = "^1.7.1"
scikit-learn = "^1.0"
loguru = "^0.7.2"
watchdog = "^3.0.0"
pdfplumber = "^0.10.3"


[tool.poetry.group.dev.dependencies]
pytest = "^7.4.2"


[build-system]
requires = ["poetry-core"]
build-backend = "poetry.core.masonry.api"

[tool.pytest.ini_options]
pythonpath = ["src"]


[tool.ruff]
# Check https://beta.ruff.rs/docs/rules/ for all rules
fixable = ["ALL"]
line-length = 140


----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\pyproject.toml -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\__init__.py -----


----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\__init__.py -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\pyproject.toml -----
[tool.poetry]
name = "2-adversarial"
version = "0.1.0"
description = ""
authors = ["Yannick Molinghen <yannick.molinghen@ulb.be>"]
readme = "README.md"

[tool.poetry.dependencies]
python = ">=3.10, <3.13"
typing-extensions = "^4.8.0"
laser-learning-environment = "^0.1.5"
opencv-python = "^4.8.1.78"
rlenv = {git = "https://github.com/yamoling/rlenv"}
anytree = "^2.10.0"
graphviz = "^0.20.1"
cairosvg = "^2.7.1"
loguru = "^0.7.2"
scipy = "^1.11.3"
matplotlib = "^3.8.0"


[tool.poetry.dev-dependencies]
pytest = "^7.0"

[build-system]
requires = ["poetry-core"]
build-backend = "poetry.core.masonry.api"


[tool.pytest.ini_options]
pythonpath = ["src"]


[tool.ruff]
# Check https://beta.ruff.rs/docs/rules/ for all rules
fixable = ["ALL"]

# Do not fix imports automatically (it removes unused imports in __init__.py files)
# unfixable = ["F401"]
line-length = 140


----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\pyproject.toml -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\graph_search\priority_queue.py -----
import heapq
import json
from typing import Dict, Generic, TypeVar
# from typing_extensions import deprecated

T = TypeVar("T")


class PriorityQueue(Generic[T]):
    """
    Implements a priority queue data structure. Each inserted item
    has a priority associated with it and the client is usually interested
    in quick retrieval of the lowest-priority item in the queue. This
    data structure allows O(1) access to the lowest-priority item.

    Credits: Berkley AI Pacman Project
    """

    def __init__(self):
        self.heap: list[T] = []
        self.count = 0

    def push(self, item: T, priority: float):
        entry = (priority, self.count, item)
        heapq.heappush(self.heap, entry)
        self.count += 1

    def pop(self) -> T:
        """pop the item with the minimum priority"""
        (_, _, item) = heapq.heappop(self.heap)
        return item
    
    def pop_maximum_priority(self) -> T:
        """pop the item with the maximum priority"""
        (_, _, item) = heapq.heappop(self.heap)
        return item

    # @deprecated
    # def isEmpty(self):
    #     return len(self.heap) == 0

    def is_empty(self):
        return len(self.heap) == 0

    def update(self, item: T, priority: float):
        # If item already in priority queue with higher priority, update its priority and rebuild the heap.
        # If item already in priority queue with equal or lower priority, do nothing.
        # If item not in priority queue, do the same thing as self.push.
        for index, (p, c, i) in enumerate(self.heap):
            if i == item:
                if p <= priority:
                    break
                del self.heap[index]
                self.heap.append((priority, c, item))
                heapq.heapify(self.heap)
                break
        else:
            self.push(item, priority)

class PriorityQueueOptimized(Generic[T]): #todo: 2x slower than PriorityQueue at least
    """heapq default minheap
    
    heapify to rebuild the heap, which is an 
    O(n) operation optimized using a dictionary 
    to keep track of the heap indices for each item, allowing to 
    update the heap in 
    O(logn) time."""
    def __init__(self):
        self.heap = []
        # self.entry_finder: Dict[T, int] = {}  
        self.entry_finder: Dict[str, int] = {}  # String keys for unhashable types
        self.count = 0

    # PriorityQueueOptimized iterable form
    def __iter__(self):
        return iter(self.heap)
    
    def _stringify(self, item: T) -> str:
        # return json.dumps(item)
        return str(id(item))
    
    def serialize(self, world_state: T) -> tuple:
        return (tuple(world_state.agents_positions), tuple(world_state.gems_collected))

    def push(self, item: T, priority: float):
        # serialized_item = self.serialize(item)
        # entry = (priority, self.count, serialized_item)
        # self.entry_finder[serialized_item] = len(self.heap)  # Keep track of the index
        entry = (priority, self.count, item)
        key = self._stringify(item)
        self.entry_finder[key] = len(self.heap)
        heapq.heappush(self.heap, entry)
        self.count += 1

    def pop(self) -> T:
        while self.heap:
            _, _, item = heapq.heappop(self.heap)
            key = self._stringify(item)
            # key = self.serialize(item)
            if key in self.entry_finder:
                del self.entry_finder[key]
                return item
        raise KeyError('pop from an empty priority queue')

    def is_empty(self):
        return len(self.heap) == 0

    def update(self, item: T, priority: float):
        key = self._stringify(item)
        # key = self.serialize(item)

        if key in self.entry_finder:
            index = self.entry_finder[key]
            _, _, existing_item = self.heap[index]
            # if existing_item is not key:
            if existing_item is not item:
                return
            self.heap[index] = (priority, self.count, item)
            heapq._siftup(self.heap, index)
            heapq._siftdown(self.heap, 0, index)
            self.count += 1
        else:
            self.push(item, priority)



----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\graph_search\priority_queue.py -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\graph_search\problem.py -----
from abc import ABC, abstractmethod
import copy
from itertools import product
from scipy.optimize import linear_sum_assignment
import numpy as np
from typing import Tuple, Iterable, Generic, TypeVar
from lle import Position, World, Action, WorldState

T = TypeVar('T', bound=WorldState)  # Declare the generic type variable with a default bound

def get_distance(coord1, coord2):
    """Returns the distance between two coordinates"""
    x1, y1 = coord1
    x2, y2 = coord2
    return abs(x1 - x2) + abs(y1 - y2)

def min_distance_position(position : Tuple[int, int]
                          , positions: list[Tuple[int, int]] 
                            ) -> Tuple[Tuple[int, int], float]:
    """Returns the position in positions that is closest to position"""
    min_distance = float("inf")
    min_position = None
    for pos in positions:
        distance = 0
        distance = get_distance(position, pos)
        if distance < min_distance:
            min_distance = distance
            min_position = pos
    return min_position, min_distance

def balanced_multi_salesmen_greedy_tsp(remaining_cities: list[Tuple[int, int]]
                                       , num_salesmen: int
                                       , start_cities: list[Tuple[int, int]]
                                       , finish_cities: list[Tuple[int, int]]
                                       ) -> Tuple[dict[str, list[Tuple[int, int]]], dict[str, float], float]: 
    #todo: calculate the distance between the last city and the finish city one time at problem creation
    """Given a list of cities coordinates, returns a list of cities visited by each agent
    in the order that minimizes the total distance traveled.
    """
    routes = {f"agent_{i+1}": [start_cities[i]] for i in range(num_salesmen)}
    distances = {f"agent_{i+1}": 0.0 for i in range(num_salesmen)}

    while remaining_cities:
        for agent in routes.keys():
            if not remaining_cities:
                break
            current_city = routes[agent][-1]
            nearest_city, nearest_distance = min_distance_position(routes[agent][-1], remaining_cities)
            distances[agent] += nearest_distance
            routes[agent].append(nearest_city)
            remaining_cities.remove(nearest_city)

    for agent in routes.keys():
        current_city = routes[agent][-1]
        finish_city, final_distance = min_distance_position(current_city, finish_cities)
        distances[agent] += final_distance
        routes[agent].append(finish_city)
        
    total_distance = sum(distances.values())
    return routes, distances, total_distance

def serialize(world_state: WorldState
              ,objectives_reached: list[Position] = None
              ) -> tuple:
    """Serialize the given world state.
    Args:
        world_state: the world state to serialize.
    Returns:
        A tuple that represents the given world state.
    """
    if objectives_reached:
        return (tuple(world_state.agents_positions), tuple(world_state.gems_collected), tuple(objectives_reached))
    else:
        return (tuple(world_state.agents_positions), tuple(world_state.gems_collected))

def was(state: WorldState
        , objectives_reached: list[Position]
        , visited: set) -> bool:
    return serialize(state, objectives_reached) in visited

def min_distance_pairing(list_1
                             , list_2):
        # Create a cost matrix
        cost_matrix = np.zeros((len(list_1), len(list_2)))
        for i, point1 in enumerate(list_1):
            for j, point2 in enumerate(list_2):
                cost_matrix[i, j] = ((point1[0] - point2[0]) ** 2 + (point1[1] - point2[1]) ** 2) ** 0.5
        # Hungarian algorithm:
        # from cost_matrix, it does the pairing by minimizing the total distance
        row_ind, col_ind = linear_sum_assignment(cost_matrix)
        
        # Extract the paired points, their distances, and the minimum total distance
        paired_points = []
        distances = []
        min_total_distance = 0
        for i, j in zip(row_ind, col_ind):
            paired_points.append((list_1[i], list_2[j]))
            distances.append(cost_matrix[i, j])
            min_total_distance += cost_matrix[i, j]
        print("paired_points", paired_points)
        print("distances", distances)
        
        return paired_points, distances, min_total_distance

class SearchProblem(ABC, Generic[T]):
    """
    A Search Problem is a problem that can be solved by a search algorithm.

    The generic parameter T is the type of the problem state, 
    which must inherit from WorldState.
    """
    def __init__(self, world: World):
        self.world = world
        world.reset()
        self.initial_state = world.get_state()
        self.objectives = []

        self.path_size = 0
        self.nodes_expanded = 0

    @abstractmethod
    def is_goal_state(self, problem_state: T) -> bool:
        """Whether the given state is the goal state"""
        
    @abstractmethod
    def get_successors(self, state: T) -> Iterable[Tuple[T, Tuple[Action, ...], float]]:
        """
        Yield all possible states that can be reached from the given world state.
        Returns
            - the new problem state
            - the joint action that was taken to reach it
            - the cost of taking the action
        """

    def heuristic(self, problem_state: T) -> float:
        return 0.0
class SimpleSearchProblem(SearchProblem[T], Generic[T]):  # Use Generic[T] to make the class generic

    def no_duplicate_in(self, agents_positions: list[Position]) -> bool:
        """Whether each agent is on a different position."""
        agents_positions_set = set(agents_positions)  
        # Check if the number of agents on exits is equal to the total number of agents
        # and if each agent is on a different exit
        result = len(agents_positions) == len(agents_positions_set)
        return result
    
    def agents_each_on_different_exit_pos(self, state: WorldState) -> bool:
        """Whether each agent is on a different exit position."""
        agent_positions = set(state.agents_positions)  
        exit_positions = set(self.world.exit_pos)  
        # Intersect the sets to find agents that are on exit positions
        agents_on_exits = agent_positions.intersection(exit_positions)
        # Check if the number of agents on exits is equal to the total number of agents
        # and if each agent is on a different exit
        return len(agents_on_exits) == len(agent_positions) # and len(agents_on_exits) == len(exit_positions)

    def is_goal_state(self, state: WorldState) -> bool:
        """Whether the given 
        SimpleStateProblem state is the 
        SimpleSearchProblem goal state.
        True if all agents are on exit tiles
        """
        return self.agents_each_on_different_exit_pos(state)
    
    def agent_position_after_action(self, agent_pos: Position, action: Action) -> Position:
        """The position of an agent after applying the given action."""
        # print("agent_position_after_action()")
        # print("agent_pos", agent_pos)
        # print("action", action)
        agent_pos_after_action = None
        # Apply the action to the agent's position
        if action == Action.NORTH:
            agent_pos_after_action = (agent_pos[0] - 1, agent_pos[1])
        elif action == Action.SOUTH:
            agent_pos_after_action = (agent_pos[0] + 1, agent_pos[1])
        elif action == Action.WEST:
            agent_pos_after_action = (agent_pos[0], agent_pos[1] - 1)
        elif action == Action.EAST:
            agent_pos_after_action = (agent_pos[0], agent_pos[1] + 1)
        elif action == Action.STAY:
            agent_pos_after_action = (agent_pos[0], agent_pos[1])
        else:
            raise ValueError("Invalid action")
        return agent_pos_after_action

    def are_valid_joint_actions(self, state: WorldState, joint_actions: Tuple[Action, ...]) -> bool:
        """Whether the given joint actions are valid.
        an action is valid if it is available for an agent 
        and if it does not lead the agent to be on the same position as another agent"""
        # print("are_valid_joint_actions()")
        # print("state", state)
        # print("joint_actions", joint_actions)
        # print("state.agents_positions", state.agents_positions)
        # # calculate agent positions after applying the joint action
        agents_positions_after_joint_actions = []
        for i, agent_pos in enumerate(state.agents_positions):
            agent_pos_after_action = self.agent_position_after_action(agent_pos, joint_actions[i])
            agents_positions_after_joint_actions.append(agent_pos_after_action)
        return self.no_duplicate_in(agents_positions_after_joint_actions)

    
    def get_valid_joint_actions(self
                                , state: WorldState
                                , available_actions: Tuple[Tuple[Action, ...], ...]) -> Iterable[Tuple[Action, ...]]:
        """Yield all possible joint actions that can be taken from the given state.
        Hint: you can use `self.world.available_actions()` to get the available actions for each agent.
        """
        # print("available_actions", available_actions)
        # cartesian product of the agents' actions
        for joint_actions in product(*available_actions):
            # print("joint_actions", joint_actions)
           
            if self.are_valid_joint_actions(state, joint_actions):
                yield joint_actions
    
    def get_successor_state(self
                            , state: WorldState
                            , joint_actions: Tuple[Action, ...]) -> WorldState:
        """The successor state of the given state after applying the given joint actions."""
        self.world.set_state(state)
        self.world.step(list(joint_actions))
        successor_state = self.world.get_state()
        # print("successor_state", successor_state)
        return successor_state

    def get_successors(self
                       , state: WorldState
                       , visited: set = None
                       , objectives_reached_before_successor: list[Position] = None
                       ):
        # - N'oubliez pas de jeter un oeil aux méthodes de la classe World (set_state, done, step, available_actions, ...)
        # - Vous aurez aussi peut-être besoin de `from itertools import product`
        """Yield all possible states that can be reached from the given world state."""
        if visited is None: # for tests
            visited = set()
        self.nodes_expanded += 1
        real_state = self.world.get_state()
        self.world.set_state(state)
        # For each possible joint actions set (i.e. cartesian product of the agents' actions)
        available_actions = self.world.available_actions()
        valid_joint_actions = self.get_valid_joint_actions(state, available_actions)
        i = 0
        for joint_actions in valid_joint_actions:
            i += 1
            objectives_reached_by_successor = objectives_reached_before_successor
            try:
                successor_state = self.get_successor_state(state, joint_actions)
            except ValueError:
                # print("ValueError: World is done, cannot step anymore")
                continue
            if isinstance(self, CornerSearchProblem):
                objectives_reached_by_successor = self.update_corners_reached(copy.deepcopy(objectives_reached_before_successor)
                                                              , joint_actions
                                                              , successor_state.agents_positions
                                                              )
            elif isinstance(self, GemSearchProblem):
                objectives_reached_by_successor = self.update_gems_collected(copy.deepcopy(objectives_reached_before_successor)
                                                              , joint_actions
                                                              , successor_state.agents_positions
                                                              )
            if was(successor_state
                    , objectives_reached_by_successor
                   , visited):
                continue
            if isinstance(self, CornerSearchProblem):
                # Compute the cost of the new state
                cost = self.heuristic(successor_state, objectives_reached_by_successor)
                yield successor_state, joint_actions, cost, objectives_reached_by_successor
            elif isinstance(self, GemSearchProblem):
                # Compute the cost of the new state
                cost = self.heuristic(successor_state, objectives_reached_by_successor)
                yield successor_state, joint_actions, cost, objectives_reached_by_successor
            elif not isinstance(self, CornerSearchProblem) and not isinstance(self, GemSearchProblem):
                # Compute the cost of the new state
                cost = self.heuristic(successor_state)
                yield successor_state, joint_actions, cost #todo must not change for test
        self.world.set_state(real_state)

    def manhattan_distance(self, pos1: Position, pos2: Position) -> float:
        """The Manhattan distance between two positions"""
        return abs(pos1[0] - pos2[0]) + abs(pos1[1] - pos2[1])

    def average_manhattan_distance_from_agents_to_exits(self, state: WorldState) -> float:
        """The average Manhattan distance from each agent to each exit
        divided by the number of agents"""
        agents_positions = state.agents_positions
        exit_positions = self.world.exit_pos
        # For each agent, compute its Manhattan distance to each exit
        total_distance = 0
        for agent_pos in agents_positions:
            for exit_pos in exit_positions:
                total_distance += self.manhattan_distance(agent_pos, exit_pos)
        # Divide the total distance by the number of agents
        average_distance = total_distance / len(agents_positions)
        return average_distance
    
    def heuristic(self
                  , state: WorldState
                  , last_actions: Tuple[Action, ...] = None
                  ) -> float:
        """Manhattan distance for each agent to the closest exit"""
        agent_positions = self.world.agents_positions
        exit_positions = self.world.exit_pos
        total_distance = self.average_manhattan_distance_from_agents_to_exits(state)
        # problem:  if agent has to get away from exit to get around a wall to reach the exit, a star chooses for him to stay in place
        # tie breaking with the last actions 
        # a last action STAY agent (if he had other options, but we don't take that into account here for simplicity) could have moved
        # so he actually lost 1 turn
        # for each action, if it was STAY
        # and if the agent is not on an exit
        # , add 1 to the total distance
        if last_actions:
            for i, action in enumerate(last_actions):
                if action == Action.STAY and agent_positions[i] not in exit_positions:
                    total_distance += 1
        return total_distance

class CornerProblemState:
    def __init__(self, world_state: WorldState):
        self.agents_positions = world_state.agents_positions
        self.gems_collected = world_state.gems_collected
        self.world_state = world_state

class CornerSearchProblem(SimpleSearchProblem[WorldState]):
    """Problème qui consiste à passer par les quatre coins du World 
    puis d’atteindre une sortie."""
    def __init__(self, world: World):
        super().__init__(world)
        self.corners = [(0, 0), (0, world.width - 1), (world.height - 1, 0), (world.height - 1, world.width - 1)]
        self.initial_state = world.get_state()

    def update_corners_reached(self
                                 , corners_reached: list[Position]
                                    , joint_actions: Tuple[Action, ...]
                                    , agent_positions: list[Position]) -> list[Position]:
        """Update the list of corners reached"""

        for action in joint_actions:
            if action != Action.STAY:
                agent_position = agent_positions[joint_actions.index(action)]
                if agent_position not in corners_reached and agent_position in self.corners:
                    corners_reached.append(agent_position)

        return corners_reached

    def all_corners_reached(self
                            , state
                            , corners_reached: list[Position]) -> bool:
        """Whether all corners are reached"""
        return len(corners_reached) == len(self.corners)

    def is_goal_state(self
                      , state: WorldState
                      , corners_reached: list[Position]) -> bool:
        """Whether the given state is the goal state.
        True if all corners are reached and all agents are on exit tiles
        """
        return self.all_corners_reached(state, corners_reached) and SimpleSearchProblem.is_goal_state(self, state)

    def heuristic(self
                  , state: WorldState
                  , corners_reached: list[Position]
                  ) -> float:
        """"""
        agents_positions = state.agents_positions
        corners_to_reach = [corner for corner in self.corners if corner not in corners_reached]

        cost = balanced_multi_salesmen_greedy_tsp(corners_to_reach
                                                  , len(agents_positions)
                                                  , agents_positions
                                                  , self.world.exit_pos
                                                  )[2]
        return cost

class GemProblemState:
    """The state of the GemSearchProblem"""
    def __init__(self, world_state: WorldState):
        self.agents_positions = world_state.agents_positions
        self.gems_collected = world_state.gems_collected
        self.world_state = world_state

class GemSearchProblem(SimpleSearchProblem[WorldState]):
    """Modéliez le problème qui consiste à collecter toutes les gemmes de l’environnement 
    puis à rejoindre les cases de sortie"""
    def __init__(self, world: World):
        super().__init__(world)
        self.initial_state = world.get_state()

    def update_gems_collected(self
                                , gems_collected: list[Position]
                                , joint_actions: Tuple[Action, ...]
                                , agent_positions: list[Position]) -> list[Position]:
        """Update the list of gems collected"""
        for action in joint_actions:
            if action != Action.STAY:
                agent_position = agent_positions[joint_actions.index(action)]
                if agent_position not in gems_collected and agent_position in [pos for pos, gem in self.world.gems]:
                    gems_collected.append(agent_position)
        return gems_collected

    def all_gems_collected(self, state):
        return sum(state.gems_collected) == self.world.n_gems

    def is_goal_state(self
                      , state
                      ) -> bool:
        return self.all_gems_collected(state) and super().is_goal_state(state)

    def heuristic(self
                  , state: WorldState
                  , gems_collected
                  ) -> float:
        """The distance of each agent to each uncollected gem and to the closest exit
        when all gems are collected, the distance of each agent to the closest exit"""
        gems_to_collect = [gem[0] for gem in self.world.gems if not gem[0] in gems_collected]

        cost = balanced_multi_salesmen_greedy_tsp(gems_to_collect
                                                  , len(state.agents_positions)
                                                  , state.agents_positions
                                                  , self.world.exit_pos
                                                  )[2]
        return cost

    

----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\graph_search\problem.py -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\graph_search\search.py -----
from dataclasses import dataclass
from typing import Optional
from lle import Action, World, WorldState

from problem import CornerSearchProblem, GemSearchProblem, SearchProblem, SimpleSearchProblem, serialize, was
from priority_queue import PriorityQueue
# import sys
# import auto_indent
# from utils import print_items

# sys.stdout = auto_indent.AutoIndent(sys.stdout)

@dataclass
class Solution:
    actions: list[tuple[Action]]

    @property
    def n_steps(self) -> int:
        return len(self.actions)

def is_empty(data_structure) -> bool:
    """Returns True if data_structure is empty, False otherwise"""
    if isinstance(data_structure, list):
        return len(data_structure) == 0
    elif isinstance(data_structure, set):
        return len(data_structure) == 0
    elif isinstance(data_structure, PriorityQueue):
        return data_structure.is_empty()

def check_goal_state(problem: SearchProblem
                     , current_state: WorldState
                     , actions: list[tuple[Action]]
                     , objectives_reached = None
                     ) -> bool:
    # Check if the current state is the goal state
    if isinstance(problem, CornerSearchProblem):
        current_state_is_goal_state = problem.is_goal_state(current_state, objectives_reached)
    else:
        current_state_is_goal_state = problem.is_goal_state(current_state)
    if current_state_is_goal_state:
        # print("Solution found!")
        print("nodes expanded: ", problem.nodes_expanded)
        # print("actions: ", actions)
        problem.path_size = len(actions)
        print( "n_steps: ", len(actions))
        return Solution(actions)

def get_initial_objectives_reached(problem: SearchProblem
                                        , initial_state: WorldState
                                        ) -> list[tuple[int, int]]:
    objectives_reached = []
    if isinstance(problem, CornerSearchProblem):
        for corner in problem.corners:
            if corner in initial_state.agents_positions:
                objectives_reached.append(corner)
    return objectives_reached
    
def tree_search(problem: SearchProblem, mode: str) -> Optional[Solution]:
    """Tree search algorithm.
    Args:
        problem: the problem to solve.
        mode: the search mode to use:
            - "dfs": Depth-First Search
            - "bfs": Breadth-First Search
            - "astar": A* Search
    Returns:
        A solution to the problem, or None if no solution exists.
    """
    initial_state = problem.initial_state
    actions = []
    cost = 0
    objectives_reached = get_initial_objectives_reached(problem, initial_state)
    if mode == "astar":
        data_structure = PriorityQueue() 
        if isinstance(problem, CornerSearchProblem) or isinstance(problem, GemSearchProblem):
            data_structure.push((initial_state
                                 , actions
                                 , objectives_reached)
                                , cost)
        else:
            data_structure.push((initial_state
                                 , actions
                                 )
                                , cost)
    else:
        data_structure = [(initial_state
                           , actions)]  #  to keep track of states
    visited = set()  # Set to keep track of visited states
    while not is_empty(data_structure):
        # Pop the top state from the data_structure
        if mode == "bfs":
            current_state, actions = data_structure.pop(0)
        else:
            if isinstance(problem, CornerSearchProblem) or isinstance(problem, GemSearchProblem):
                current_state, actions, objectives_reached = data_structure.pop()
            else:
                current_state, actions = data_structure.pop()
        # Check if the current state is in the visited set
        if was(current_state
               , objectives_reached
               , visited):
            continue
        solution = None
        if isinstance(problem, CornerSearchProblem):
            solution = check_goal_state(problem
                            , current_state
                            , actions
                            , objectives_reached)
        else:
            solution = check_goal_state(problem
                            , current_state
                            , actions
                            , None)
        if solution:
            return solution
        current_state_hashable = serialize(current_state, objectives_reached)
        visited.add(current_state_hashable)
        # Add successors to data_structure
        if isinstance(problem, CornerSearchProblem) or isinstance(problem, GemSearchProblem):
            successors = problem.get_successors(current_state
                                                ,visited
                                                ,objectives_reached)
        else:
            successors = problem.get_successors(current_state
                                            ,visited)
        for successor_tuple in successors:  
            if isinstance(problem, CornerSearchProblem) or isinstance(problem, GemSearchProblem):
                successor, successor_actions, cost, objectives_reached = successor_tuple
            else:
                successor, successor_actions, cost = successor_tuple
                objectives_reached = None
            new_actions = actions + [successor_actions]
            if mode == "astar":
                if isinstance(problem, CornerSearchProblem) or isinstance(problem, GemSearchProblem):
                    successor_cost = problem.heuristic(successor, objectives_reached)
                    total_cost = cost + successor_cost
                    data_structure.push((successor
                                         , new_actions
                                         , objectives_reached)
                                        , total_cost)
                else:
                    successor_cost = problem.heuristic(successor, successor_actions)
                    total_cost = cost + successor_cost
                    data_structure.push((successor
                                         , new_actions)
                                        , total_cost)
            else:
                data_structure.append((successor, new_actions))
    return None

def dfs(problem: SearchProblem) -> Optional[Solution]:
    """Depth-First Search"""
    return tree_search(problem, "dfs")

def bfs(problem: SearchProblem) -> Optional[Solution]:
    """Breadth-First Search"""
    return tree_search(problem, "bfs")

def astar(problem: SearchProblem) -> Optional[Solution]:
    """A* Search"""
    return tree_search(problem, "astar")

if __name__ == "__main__":
    world = World.from_file("cartes/corners")
    problem = CornerSearchProblem(world)
    solution = astar(problem)
    world.reset()
    corners = set([(0, 0), (0, world.width - 1), (world.height - 1, 0), (world.height - 1, world.width - 1)])
    for action in solution.actions:
        world.step(action)
        agent_pos = world.agents_positions[0]
        if agent_pos in corners:
            corners.remove(agent_pos)
    assert len(corners) == 0, f"The agent did not reach these corners: {corners}"

----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\graph_search\search.py -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\graph_search\travel_sales_man.py -----
# Adjusted code based on user input
import heapq
from typing import List, Tuple
import numpy as np
from priority_queue import PriorityQueueOptimized
from utils import min_distance_position

# Adapted Greedy Algorithm with Balanced Total Distances
def balanced_multi_salesmen_greedy_tsp(remaining_cities: List[Tuple[int, int]]
                                       , num_salesmen: int
                                       , start_cities: List[Tuple[int, int]]
                                       , finish_cities: List[Tuple[int, int]]):
    """Given a list of cities coordinates, returns a list of cities visited by each salesman
    in the order that minimizes the total distance traveled.
    """
    
    routes = {f"Salesman_{i+1}": [start_cities[i]] for i in range(num_salesmen)}
    distances = {f"Salesman_{i+1}": 0.0 for i in range(num_salesmen)}

    while remaining_cities:
        for salesman in routes.keys():
            if not remaining_cities:
                break
            
            current_city = routes[salesman][-1]
            nearest_city, nearest_distance = min_distance_position(routes[salesman][-1], remaining_cities)
            distances[salesman] += nearest_distance
            routes[salesman].append(nearest_city)
            remaining_cities.remove(nearest_city)

    for salesman in routes.keys():
        current_city = routes[salesman][-1]
        finish_city, final_distance = min_distance_position(current_city, finish_cities)
        distances[salesman] += final_distance
        routes[salesman].append(finish_city)
        
    total_distance = sum(distances.values())
    return routes, distances, total_distance


# Greedy Algorithm with Min Heap (Priority Queue)
def greedy_tsp_heap(cities_to_visit
                    , start_city=None
                    , finish_city=None):
    """Given a dict of cities and their coordinates, returns a list of cities
    visited in the order that minimizes the total distance traveled.
    Route: A→C→B→D→E→A
    Total Distance: 12.35
    Elapsed Time: 0.0000703 seconds"""

    road = [start_city]
    current_city = start_city
    total_distance = 0.0

    def distance(city1, city2):
        x1, y1 = cities_to_visit[city1]
        x2, y2 = cities_to_visit[city2]
        return np.sqrt((x2 - x1)**2 + (y2 - y1)**2)

    while cities_to_visit:
        heap = [(distance(current_city, city), city) for city in cities_to_visit]
        heapq.heapify(heap)
        nearest_distance, nearest_city = heapq.heappop(heap)
        total_distance += nearest_distance
        road.append(nearest_city)
        current_city = nearest_city
        cities_to_visit.remove(nearest_city)

    total_distance += distance(current_city, finish_city)
    road.append(finish_city)

    return road, total_distance


def greedy_tsp_optimized_pq(cities):
    """Given a dict of cities and their coordinates, returns a list of cities
    visited in the order that minimizes the total distance traveled.
    Elapsed Time: 0.0002046 seconds"""
    start_city = list(cities.keys())[0]
    road = [start_city]
    cities_to_visit = list(set(cities.keys()) - set(road))
    current_city = start_city
    total_distance = 0.0

    def distance(city1, city2):
        x1, y1 = cities[city1]
        x2, y2 = cities[city2]
        return np.sqrt((x2 - x1)**2 + (y2 - y1)**2)

    pq = PriorityQueueOptimized()

    while cities_to_visit:
        for city in cities_to_visit:
            pq.push(city, distance(current_city, city))
        
        nearest_city = pq.pop()
        nearest_distance = distance(current_city, nearest_city)
        total_distance += nearest_distance
        road.append(nearest_city)
        current_city = nearest_city
        cities_to_visit.remove(nearest_city)

    total_distance += distance(current_city, start_city)
    road.append(start_city)

    return road, total_distance

----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\graph_search\travel_sales_man.py -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\src\adversarial_search.py -----
from typing import List, Tuple
from lle import Action
from mdp import MDP, S, A
from world_mdp import BetterValueFunction, WorldMDP

WORLD_STEP_NOT_POSSIBLE_ERROR = "There is no more step to take"

def transition(mdp: MDP[A, S], state: S, action: A, depth: int = 0) -> S:
    """Returns the state reached by performing the given action in the given state."""
    if isinstance(mdp, BetterValueFunction):
        new_state = mdp.transition(state, action, depth)
    else:
        new_state = mdp.transition(state, action)
    if isinstance(mdp, WorldMDP):
        if mdp.was_visited(new_state):
            raise ValueError("was visited")
        mdp.add_to_visited(new_state)
    return new_state
            
        
def _max(mdp: MDP[A, S], state: S, max_depth: int, depth: int = 0) -> Tuple[float, A]:
    """Returns the value of the state and the action that maximizes it."""
    if mdp.is_final(state) or depth == max_depth :
        return state.value, None
    best_value = float('-inf')
    best_action = None
    mdp_available_actions = mdp.available_actions(state)
    for action in mdp_available_actions:
        try:
            new_state = transition(mdp, state, action, depth)
      
            if new_state.current_agent == 0:
                value, _ = _max(mdp, new_state, max_depth, depth + 1)
            else:
                value = _min(mdp, new_state, max_depth, depth + 1)
            if value > best_value:
                best_value = value
                best_action = action
            if isinstance(mdp, WorldMDP):
                mdp.remove_from_visited(new_state)
        except ValueError:
            # print(WORLD_STEP_NOT_POSSIBLE_ERROR)
            pass
    return best_value, best_action

def _min(mdp: MDP[A, S], state: S, max_depth: int, depth: int = 0) -> float:
    """Returns the worst value of the state."""
    if mdp.is_final(state) or depth == max_depth:
        return state.value
    worst_value = float('inf')
    for action in mdp.available_actions(state):
        try:
            new_state = transition(mdp, state, action, depth)
        
            if new_state.current_agent == 0:
                value, _ = _max(mdp, new_state, max_depth, depth + 1)
            else:
                value = _min(mdp, new_state, max_depth, depth)
            worst_value = min(worst_value, value)
            if isinstance(mdp, WorldMDP):
                mdp.remove_from_visited(new_state)
        except ValueError:
            # print(WORLD_STEP_NOT_POSSIBLE_ERROR)
            pass
        
    return worst_value

def minimax(mdp: MDP[A, S], state: S, max_depth: int) -> A:
    """Returns the action to be performed by Agent 0 in the given state. 
    This function only accepts 
    states where it's Agent 0's turn to play 
    and raises a ValueError otherwise. 
    Don't forget that there may be more than one opponent"""
    if state.current_agent != 0:
        raise ValueError("It's not Agent 0's turn to play")
    value, action = _max(mdp, state, max_depth, 0)
    return action

def _alpha_beta_max(mdp: MDP[A, S], state: S, alpha: float, beta: float, max_depth: int, depth: int = 0) -> Tuple[float, A, float, float]:
    if mdp.is_final(state) or depth == max_depth:
        return state.value, None
    best_value = float('-inf')
    best_action = None
    available_actions = mdp.available_actions(state)
    for action in available_actions:
        try:
            new_state = transition(mdp, state, action, depth)
    
            if new_state.current_agent == 0:
                value, _ = _alpha_beta_max(mdp, new_state, alpha, beta, max_depth, depth + 1)
            else:
                value = _alpha_beta_min(mdp, new_state, alpha, beta, max_depth, depth + 1)
            if value > best_value:
                best_value = value
                best_action = action
            if isinstance(mdp, WorldMDP):
                mdp.remove_from_visited(new_state)
            if beta <= best_value:  # Beta cutoff
                return best_value, best_action
            alpha = max(alpha, best_value)  # Update alpha after cutoff: fail hard
        except ValueError:
            # print(WORLD_STEP_NOT_POSSIBLE_ERROR)
            pass

    return best_value, best_action

def _alpha_beta_min(mdp: MDP[A, S], state: S, alpha: float, beta: float, max_depth: int, depth: int = 0) -> Tuple[float, A, float, float]:
    if mdp.is_final(state) or depth == max_depth:
        return state.value
    worst_value = float('inf')
    available_actions = mdp.available_actions(state)
    for action in available_actions:
        try:
            new_state = transition(mdp, state, action, depth)
    
            if new_state.current_agent == 0:
                value, _ = _alpha_beta_max(mdp, new_state, alpha, beta, max_depth, depth + 1)
            else:
                value = _alpha_beta_min(mdp, new_state, alpha, beta, max_depth, depth)
            worst_value = min(worst_value, value)
            if isinstance(mdp, WorldMDP):
                mdp.remove_from_visited(new_state)
            if worst_value <= alpha:  # Alpha cutoff
                return worst_value
            beta = min(beta, worst_value)  # Update beta after cutoff: fail hard
        except ValueError:
            # print(WORLD_STEP_NOT_POSSIBLE_ERROR)
            pass
    return worst_value

def alpha_beta(mdp: MDP[A, S]
               , state: S
               , max_depth: int) -> A: # todo good node ordering reduces time complexity to O(b^m/2)
    """The alpha-beta pruning algorithm 
    is an improvement over 
    minimax 
    that allows for pruning of the search tree."""
    # todo In maxn (Luckhardt and Irani, 1986), 
    # the extension of minimax to multi-player games
    # , pruning is not as successful.
    if state.current_agent != 0:
        raise ValueError("It's not Agent 0's turn to play")
    alpha = float('-inf')
    beta = float('inf')
    value, action = _alpha_beta_max(mdp, 
                                    state, 
                                    alpha, 
                                    beta, 
                                    max_depth, 
                                    0)
    return action

def _expectimax_max(mdp: MDP[A, S], 
                    state: S, 
                    max_depth: int, 
                    depth: int = 0
                    ) -> Tuple[float, A]:
    if mdp.is_final(state) or depth == max_depth:
        return state.value, None
    best_value = float('-inf')
    best_action = None
    for action in mdp.available_actions(state):
        try:
            new_state = transition(mdp, 
                                   state, 
                                   action, 
                                   depth
                                    )

            if new_state.current_agent == 0:
                value, _ = _expectimax_max(mdp, 
                                        new_state, 
                                        max_depth, 
                                        depth + 1
                                        )
            else:
                value = _expectimax_exp(mdp, new_state, max_depth, depth + 1)
            if value > best_value:
                best_value = value
                best_action = action
            if isinstance(mdp, WorldMDP):
                mdp.remove_from_visited(new_state)
        except ValueError:
            # print(WORLD_STEP_NOT_POSSIBLE_ERROR)
            pass
    return best_value, best_action

def _expectimax_exp(mdp: MDP[A, S],
                     state: S, 
                     max_depth: int, 
                     depth: int = 0
                    ) -> float:
    """Returns the expected value of the state.
    The expected value of a state is
    the average value of the state
    after all possible actions are performed.
    """
    if mdp.is_final(state) or depth == max_depth:
        return state.value
    total_value = 0
    num_actions = len(mdp.available_actions(state))
    for action in mdp.available_actions(state):
        try:
            new_state = transition(mdp, state, action, depth)
    
            if new_state.current_agent == 0:
                value, _ = _expectimax_max(mdp, new_state, max_depth, depth + 1)
            else:
                value = _expectimax_exp(mdp, new_state, max_depth, depth)
            total_value += value
            if isinstance(mdp, WorldMDP):
                mdp.remove_from_visited(new_state)
        except ValueError:
            # print(WORLD_STEP_NOT_POSSIBLE_ERROR)
            pass
    expected_value = total_value / num_actions if num_actions != 0 else 0
    return expected_value

def expectimax(mdp: MDP[A, S], 
               state: S, 
               max_depth: int
               ) -> Action:
    """ The 'expectimax' algorithm allows for 
    modeling the probabilistic behavior of humans 
    who might make suboptimal choices. 
    The nature of expectimax requires that we know 
    the probability that the opponent will take each action. 
    Here, we will assume that 
    the other agents take actions that are uniformly random."""
    if state.current_agent != 0:
        raise ValueError("It's not Agent 0's turn to play")
    _, action = _expectimax_max(mdp, state, max_depth, 0)
    return action


----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\src\adversarial_search.py -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\src\latextest.py -----
import matplotlib.pyplot as plt

plt.rcParams['text.usetex'] = True
plt.rcParams['text.latex.preamble'] = r'\usepackage{amsmath}'

plt.figure()
plt.title(r'Test $\text{with\;\;\;spaces}$')
plt.show()


----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\src\latextest.py -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\src\map_display.py -----
import cv2
from lle import World


map2023_10_29_13_48_15 = """.   G   S1 
L0E S0  .  
X   @   X  
"""
results2023_10_29_13_48_15 = {
"depth":  3 ,
"minimax":  27 ,
"minimax_with_better_value_function":  27 ,
"alpha_beta":  22 ,
"alpha_beta_with_better_value_function":  16 ,
"expectimax":  27 ,
"action_minimax":  " North " ,
"action_minimax_with_better_value_function":  " North " ,
"action_alpha_beta":  " North " ,
"action_alpha_beta_with_better_value_function":  " North " ,
"action_expectimax":  " North " ,
}

map2023_10_29_13_49_35 = """.  S0 S2 X 
.  .  G  . 
.  @  .  G 
.  .  S1 . 
.  .  X  X 
"""
results2023_10_29_13_49_35 = {
"depth":  4 ,
"minimax":  2706 ,
"minimax_with_better_value_function":  2706 ,
"alpha_beta":  312 ,
"alpha_beta_with_better_value_function":  366 ,
"expectimax":  2706 ,
"action_minimax":  " Stay " ,
"action_minimax_with_better_value_function":  " Stay " ,
"action_alpha_beta":  " Stay " ,
"action_alpha_beta_with_better_value_function":  " South " ,
"action_expectimax":  " South " ,
}

map2023_10_29_13_53_25 = """G  .  .  .  .  G 
X  .  G  S2 S0 G 
.  .  G  X  S1 X 
.  .  G  .  .  . 
"""
results2023_10_29_13_53_25 = {
"depth":  4 ,
"minimax":  3160 ,
"minimax_with_better_value_function":  3160 ,
"alpha_beta":  929 ,
"alpha_beta_with_better_value_function":  436 ,
"expectimax":  3160 ,
"action_minimax":  " East " ,
"action_minimax_with_better_value_function":  " East " ,
"action_alpha_beta":  " East " ,
"action_alpha_beta_with_better_value_function":  " East " ,
"action_expectimax":  " East " ,
}

map2023_10_29_13_55_47 = """.   .   L0S .   .   S2 
X   L1S .   S1  .   .  
X   .   .   @   S0  X  
"""
results2023_10_29_13_55_47 = {
"depth":  4 ,
"minimax":  701 ,
"minimax_with_better_value_function":  701 ,
"alpha_beta":  297 ,
"alpha_beta_with_better_value_function":  385 ,
"expectimax":  701 ,
"action_minimax":  " Stay " ,
"action_minimax_with_better_value_function":  " North " ,
"action_alpha_beta":  " Stay " ,
"action_alpha_beta_with_better_value_function":  " North " ,
"action_expectimax":  " Stay " ,
}

map2023_10_29_13_56_42 = """.  X  G 
@  @  S0
.  .  . 
.  .  . 
.  X  S1
"""

def display(map_str, map_name):
    world_instance = World(map_str)
    img = world_instance.get_image()
    cv2.imshow("Visualisation", img)
    cv2.waitKey(0) # Attend que l'utilisateur appuie sur 'enter'
    cv2.waitKey(1) # continue l'exécution du code
    #save img
    cv2.imwrite(map_name, img)

maps_names = ["map2023_10_29_13_48_15", "map2023_10_29_13_49_35", "map2023_10_29_13_53_25", "map2023_10_29_13_55_47", "map2023_10_29_13_56_42"]
map_names_png = ["map2023_10_29_13_48_15.png", "map2023_10_29_13_49_35.png", "map2023_10_29_13_53_25.png", "map2023_10_29_13_55_47.png", "map2023_10_29_13_56_42.png"]
maps = [map2023_10_29_13_48_15, map2023_10_29_13_49_35, map2023_10_29_13_53_25, map2023_10_29_13_55_47, map2023_10_29_13_56_42]
for map_str, map_name in zip(maps, map_names_png):
    display(map_str, map_name)

----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\src\map_display.py -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\src\map_generator.py -----
import random


class MapGenerator:
    def __init__(self
                 , map_width = 9
                 , map_height = 9
                 , map_type = "random"
                 ):
        self.map_width = map_width
        self.map_height = map_height
        self.map_type = map_type
        self.map = []
        self.map = self.generate_map_str()
    

    def generate_map_str(self):
        # Generate map based on map_type
        random_params = self.generate_random_params()
        random_map = self.generate_random_map(**random_params)
        # random_map_str = self.map_to_str(random_map)
        matrix_layout = self.matrix_to_layout(random_map)
        return matrix_layout        
        
    def generate_random_map(self
                               , rows=9
                               , cols=9
                               , num_agents=1
                               , num_gems=0
                               , num_lasers=0
                               ):
        """
        Generate a random map of dimensions (rows x cols) with given elements.
        
        Parameters:
        - rows: Number of rows
        - cols: Number of columns
        - num_agents: Number of agents (default is 1)
        - num_gems: Number of gems (default is 0)
        - num_lasers: Number of lasers (default is 0)
        
        Returns:
        - A list of lists representing the map
        """
        # if rows*cols < num_agents*2 + num_gems + num_lasers:
        #     rows = cols = num_agents*2 + num_gems + num_lasers
        # Initialize the map with floor tiles '.' and walls '@'
        map_grid = [['.' for _ in range(cols)] for _ in range(rows)]
        
        
        
        # Place agents' start positions
        for i in range(num_agents):
            placed = False
            while not placed:
                r, c = random.randint(0, rows-1), random.randint(0, cols-1)
                if map_grid[r][c] == '.':
                    map_grid[r][c] = f'S{i}'
                    placed = True
        
        # Place exits (same number as agents)
        for _ in range(num_agents):
            placed = False
            while not placed:
                r, c = random.randint(0, rows-1), random.randint(0, cols-1)
                if map_grid[r][c] == '.':
                    map_grid[r][c] = 'X'
                    placed = True
        
        # Place gems
        for _ in range(num_gems):
            for _ in range(3):
                r, c = random.randint(0, rows-1), random.randint(0, cols-1)
                if map_grid[r][c] == '.':
                    map_grid[r][c] = 'G'
                    break
        
        # Place lasers
        for i in range(num_lasers):
            for _ in range(3):
                r, c = random.randint(0, rows-1), random.randint(0, cols-1)
                if map_grid[r][c] == '.':
                    direction = random.choice(['N', 'S', 'E', 'W'])
                    map_grid[r][c] = f'L{i}{direction}'
                    break
        # Randomly place walls (20% of the map)
        for _ in range((rows * cols) // 5):
            r, c = random.randint(0, rows-1), random.randint(0, cols-1)
            if map_grid[r][c] == '.':
                map_grid[r][c] = '@'
        
        return map_grid

    # Function to generate random but reasonable parameters for the map
    def generate_random_params(self
                            #    , max_rows=4
                               , max_rows=6
                            #    , max_rows=9
                            #    , max_rows=5
                            #    , max_cols=4
                               , max_cols=6
                            #    , max_cols=9
                            #    , max_cols=5
                               , max_agents=3
                               , max_gems=6
                               , max_lasers=3):
        """
        Generate random but reasonable parameters for the map.
        
        Parameters:
        - max_rows: Maximum number of rows (default is 9)
        - max_cols: Maximum number of columns (default is 9)
        - max_agents: Maximum number of agents (default is 4)
        - max_gems: Maximum number of gems (default is 6)
        - max_lasers: Maximum number of lasers (default is 3)
        
        Returns:
        - A dictionary containing the generated parameters
        """
        rows_min = 3
        # rows_min = 5
        cols_min = 3
        # cols_min = 5
        rows = random.randint(rows_min
                              , max_rows)
        cols = random.randint(cols_min
                              , max_cols)
        max_agents = min(max_agents, rows*cols//2)
        num_agents = random.randint(2, max_agents)
        print("rows: ", rows)
        print("cols: ", cols)
        print("num_agents: ", num_agents)

        params = {
            "rows": rows,
            "cols": cols,
            "num_agents": num_agents,
            "num_gems": random.randint(0, max_gems),
            "num_lasers": random.randint(0, max_lasers)
        }
        
        return params

    def map_to_string(self
                      , map_grid):
        """
        Convert a given map into a string.
        
        Parameters:
        - map_grid: A list of lists representing the map.
        
        Returns:
        - A string representing the map.
        """
        return "\n".join(" ".join(row) for row in map_grid)
    
    def matrix_to_layout(self
                         ,matrix):
        """
        Convert a given matrix into a layout.
        
        Parameters:
        matrix (list of list of str): A matrix representing the layout.

        Returns:
        list of str: Each string represents a row in the layout.
        """
        # Determine the maximum length of any element in the matrix for alignment
        max_len = max(len(str(item)) for row in matrix for item in row)
        
        layout = """"""
        # layout += "\n"

        for row in matrix:
            # Align the elements by padding with spaces
            aligned_row = " ".join(str(item).ljust(max_len) for item in row)
            layout += aligned_row + "\n"
            
        return layout



----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\src\map_generator.py -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\src\mdp.py -----
from typing import TypeVar, Generic
from abc import abstractmethod, ABC
from dataclasses import dataclass


@dataclass
class State(ABC):
    """
    State in an adversarial MDP.
    It must somehow know whose agent's turn it is.
    """

    value: float
    current_agent: int


A = TypeVar("A")
S = TypeVar("S", bound=State)


class MDP(ABC, Generic[A, S]):
    """Adversarial Markov Decision Process"""

    def __init__(self):
        super().__init__()
        self.n_expanded_states = 0

    @abstractmethod
    def reset(self) -> S:
        """Reset the MDP to its initial state and returns it."""

    @abstractmethod
    def available_actions(self, state: S) -> list[A]:
        """Returns the list of available actions for the current agent from the given state."""

    @abstractmethod
    def transition(self, state: S, action: A) -> S:
        """Returns the next state and the reward."""

    @abstractmethod
    def is_final(self, state: S) -> bool:
        """Returns whether the given state is final."""


----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\src\mdp.py -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\src\stock_tree.py -----
import copy
import datetime
import os
import sys
from typing import List, Tuple

from lle import Action, World
from mdp import MDP, S, A

from world_mdp import BetterValueFunction, WorldMDP
from anytree import Node, RenderTree
from anytree.exporter import UniqueDotExporter


def stock_tree(mdp: MDP[A, S]
               , algorithm: str
                ) -> None:
    """Stocks the tree in a png file"""
    if isinstance(mdp, WorldMDP):
        if not os.path.exists('tree/current/'+algorithm):
            os.makedirs('tree/current/'+algorithm)
        UniqueDotExporter(mdp.root).to_picture("tree/current/"+algorithm+".png")
        print("tree stocked in tree/current/"+algorithm+".png")

        date_time = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
        if not os.path.exists('tree/'+algorithm):
            os.makedirs('tree/'+algorithm)
        UniqueDotExporter(mdp.root).to_picture("tree/"+algorithm+"/"+date_time+".png")
        print("tree stocked in tree/"+algorithm+"/"+date_time+".png")


----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\src\stock_tree.py -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\src\utils.py -----
# def min_distance_position(position, positions):
#     # Create a cost vector
#     cost_vector = np.zeros(len(positions))
    
#     for i, point in enumerate(positions):
#         cost_vector[i] = ((position[0] - point[0]) ** 2 + (position[1] - point[1]) ** 2) ** 0.5

#     # Find the index of the minimum distance
#     min_index = np.argmin(cost_vector)

#     # Extract the closest position and its distance
#     closest_position = positions[min_index]
#     min_distance = cost_vector[min_index]

#     return closest_position, min_distance

from typing import List, Tuple


def get_distance(coord1, coord2):
    """Returns the distance between two coordinates"""
    x1, y1 = coord1
    x2, y2 = coord2
    return abs(x1 - x2) + abs(y1 - y2)

def min_distance_position(position : Tuple[int, int]
                          , positions: List[Tuple[int, int]] 
                            ) -> Tuple[Tuple[int, int], float]:
    """Returns the position in positions that is closest to position"""
    min_distance = float("inf")
    min_position = None
    for pos in positions:
        distance = 0
        distance = get_distance(position, pos)
        # print(distance)
        if distance < min_distance:
            min_distance = distance
            min_position = pos
    return min_position, min_distance

# def order_items()
# function to print visited set or stack items in terminal
def print_items(items
                , title="items:"
                , transform=None) -> None:
    """Prints items in terminal
    Args:
        items: items to print
    T is a generic type variable
    possible types for T:
    set, list, tuple, dict, etc."""
    print(title)
    i = 0
    for item in items:
        i += 1
        print(i, item)
    print("")




----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\src\utils.py -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\src\world_mdp.py -----
import copy
from dataclasses import dataclass
import random
import sys
from typing import List, Optional, Tuple#, override
import lle
from lle import Position, World, Action
from mdp import A, MDP, State

# import auto_indent

from anytree import Node, RenderTree
from loguru import logger
import numpy as np
from scipy.optimize import linear_sum_assignment

# sys.stdout = auto_indent.AutoIndent(sys.stdout)

def get_distance(coord1, coord2):
    """Returns the distance between two coordinates"""
    x1, y1 = coord1
    x2, y2 = coord2
    return abs(x1 - x2) + abs(y1 - y2)

def min_distance_position(position : Tuple[int, int]
                          , positions: list[Tuple[int, int]] 
                            ) -> Tuple[Tuple[int, int], float]:
    """Returns the position in positions that is closest to position"""
    min_distance = float("inf")
    min_position = None
    for pos in positions:
        distance = 0
        distance = get_distance(position, pos)
        if distance < min_distance:
            min_distance = distance
            min_position = pos
    return min_position, min_distance

def min_distance_pairing(list_1
                             , list_2):
        # Create a cost matrix
        cost_matrix = np.zeros((len(list_1), len(list_2)))
        for i, point1 in enumerate(list_1):
            for j, point2 in enumerate(list_2):
                cost_matrix[i, j] = ((point1[0] - point2[0]) ** 2 + (point1[1] - point2[1]) ** 2) ** 0.5
        # Hungarian algorithm:
        # from cost_matrix, it does the pairing by minimizing the total distance
        row_ind, col_ind = linear_sum_assignment(cost_matrix)
        # Extract the paired points, their distances, and the minimum total distance
        paired_points = []
        distances = []
        min_total_distance = 0
        for i, j in zip(row_ind, col_ind):
            paired_points.append((list_1[i], list_2[j]))
            distances.append(cost_matrix[i, j])
            min_total_distance += cost_matrix[i, j]
        return paired_points, distances, min_total_distance


@dataclass
class MyWorldState(State):
    """Comme il s’agit d’un MDP à plusieurs agents et à tour par tour, 
    chaque état doit retenir à quel agent
    c’est le tour d’effectuer une action.
    """
    # la valeur d’un état correspond à la somme des rewards obtenues par les actions de l’agent 0 
    # (c’est-à-dire les gemmes collectées + arriver sur une case de ﬁn)
    value: float 
    current_agent: int
    last_action: Action
    agents_positions: list
    gems_collected: list[bool]
    value_vector: List[float]
    alpha: Optional[float] = None
    beta: Optional[float] = None
    # Add more attributes here if needed.
    def __init__(self
                 , value: float
                 , value_vector: List[float]
                 , current_agent: int
                 , world: World
                 , world_string: str = None
                 , last_action: Action = None
                 ):
        super().__init__(value, current_agent)
        self.world = world
        if world_string:
            self.world_string = world_string
        else:
            self.world_string = world.world_string
        self.agents_positions = world.agents_positions
        self.gems_collected = world.get_state().gems_collected
        self.value_vector = value_vector
        self.node = None
        if last_action:
            self.last_action = last_action
        else:
            self.last_action = None

    def get_agents_positions(self) -> list:
        # return self.agents_positions
        return self.world.agents_positions
    
    def layout_to_matrix(self
                         , layout):
        """
        Convert a given layout into a matrix where each first row of each line
        contains the (group of) character of the layout line.
        Parameters:
        layout (str): A multi-line string where each line represents a row in the layout.
        Returns:
        list of list of str: A matrix representing the layout.
        """
        # Split the layout into lines
        lines = layout.strip().split('\n')
        matrix = []
        max_cols = 0  # Keep track of the maximum number of columns
        # Convert each line into a row in the matrix
        for line in lines:
            row = [char for char in line.split() if char != ' ']
            matrix.append(row)
            max_cols = max(max_cols, len(row))
        # Fill in missing columns with '.'
        for row in matrix:
            while len(row) < max_cols:
                row.append('.')
        return matrix
    
    def matrix_to_layout(self
                         ,matrix):
        """
        Convert a given matrix into a layout.
        
        Parameters:
        matrix (list of list of str): A matrix representing the layout.

        Returns:
        list of str: Each string represents a row in the layout.
        """
        # Determine the maximum length of any element in the matrix for alignment
        max_len = max(len(str(item)) for row in matrix for item in row)
        layout = ""
        for row in matrix:
            # Align the elements by padding with spaces
            aligned_row = " ".join(str(item).ljust(max_len) for item in row)
            layout += aligned_row + "\n"
            
        return layout

    
    def update_world_string(self
                            ,current_agent: int
                            ,current_agent_previous_position: Position
                            ,action) -> None:
        """Updates world_string attribute with current world state:
        current agent position, gems collected, etc."""
        matrix = self.layout_to_matrix(self.world_string)
        if action != Action.STAY:
            agent_string = "S"+str(current_agent)
            matrix[current_agent_previous_position[0]][current_agent_previous_position[1]] = "."
            matrix[self.agents_positions[current_agent][0]][self.agents_positions[current_agent][1]] = agent_string
            matrix_after_action = matrix
            layout_after_action = self.matrix_to_layout(matrix_after_action)
            self.world_string = layout_after_action
            
    def to_string(self) -> str:
        """Returns a string representation of the state.
        with each state attribute on a new line."""
        # return f"current_agent: {self.current_agent}, value: {self.value}, value_vector: {self.value_vector}, agents_positions: {self.agents_positions}, gems_collected: {self.gems_collected}"
        state_attributes = f"current_agent: {self.current_agent}\n"
        
        if self.last_action :
            state_attributes += f"last_action: {self.last_action}\n"
        state_attributes += f"value: {self.value}\n"
        state_attributes += f"value_vector: {self.value_vector}\n"
        state_attributes += f"agents_positions: {self.agents_positions}\n"
        state_attributes += f"gems_collected: {self.gems_collected}\n"
        state_attributes += f"world: \n{self.world_string}\n"
        return state_attributes
    
    def serialize(self) -> tuple:
        """Serialize the given world state.
        Args:
            world_state: the world state to serialize.
        Returns:
            A tuple that represents the given world state.
        """
        return (tuple(self.agents_positions), tuple(self.gems_collected), self.current_agent)
    

class WorldMDP(MDP[Action, MyWorldState]):
    def __init__(self
                 , world: World):
        self.world = world
        world.reset()
        self.n_agents = world.n_agents

        self.initial_state = world.get_state()
        self.root = None

        self.visited = set() # visited states
        # nodes dict
        self.nodes = {} # key: state, value: node
        self.n_expanded_states = 0
        self.lasers_dangerous_for_agents = self.get_lasers_dangerous_for_agents()

    def get_lasers_dangerous_for_agents(self) -> list[list[Position]]:
        """Returns a list of lists
        , each corresponding to the agent of same index
        , containing positions of the lasers of a different agent_id (color)."""

        lasers_dangerous_for_agents = [[] for _ in range(self.world.n_agents)]
        laser_sources = self.world.laser_sources

        for laser_source in laser_sources:
            laser_source_position = laser_source[0]
            laser_source_agent_id = laser_source[1].agent_id
            #add the laser source position to the list of lasers dangerous for the agents of index different from laser_source_agent_id
            for agent_id in range(self.world.n_agents):
                if agent_id != laser_source_agent_id:
                    lasers_dangerous_for_agents[agent_id].append(laser_source_position)

        return lasers_dangerous_for_agents

    def reset(self):
        """The world.reset() method returns an initial state of the game. 
        After performing reset(), 
        it's Agent 0's turn to take an action. 
        Thus, world.transition(Action.NORTH) 
        will only move Agent 0 to the north, 
        while all other agents will remain in place. 
        Then, it's Agent 1's turn to move, and so on"""
        self.n_expanded_states = 0
        self.world.reset()
        return MyWorldState(0.0
                            , [0.0 for _ in range(self.world.n_agents)]
                            , 0
                            , self.world)

    def available_actions(self, state: MyWorldState) -> list[Action]:
        """returns the actions available to the current agent."""
        world_available_actions = state.world.available_actions()
        current_agent = state.current_agent
        current_agent_available_actions = world_available_actions[current_agent]
        return current_agent_available_actions
      
    def is_final(self, state: MyWorldState) -> bool:
        """returns True if the state is final, False otherwise."""
        return state.world.done and not [gem[0] for gem in state.world.gems if not gem[1].is_collected]

    def get_actions(self
                , current_agent: int
                , action: Action) -> list[Action]:
        """from current agent action, returns list with action at agent index and STAY at others's ."""
        actions = [Action.STAY for _ in range(self.world.n_agents)]
        actions[current_agent] = action
        return actions

    def convert_to_WorldState(self, state: MyWorldState) -> lle.WorldState:
        """Converts MyWorldState to lle.WorldState"""
        return lle.WorldState(state.agents_positions, state.gems_collected)
    
    def agents_each_on_different_exit_pos(self
                                          , state: MyWorldState) -> bool:
        """Whether each agent is on a different exit position."""
        agent_positions = set(state.world.agents_positions)  

        exit_positions = set(self.world.exit_pos)  
        # Intersect the sets to find agents that are on exit positions
        agents_on_exits = agent_positions.intersection(exit_positions)
        # Check if the number of agents on exits is equal to the total number of agents
        # and if each agent is on a different exit
        return len(agents_on_exits) == len(agent_positions) # and len(agents_on_exits) == len(exit_positions)

    def current_agent_on_exit(self
                              , state: MyWorldState
                              , current_agent: int
                              ) -> bool:
        """Whether the current agent is on an exit position."""
        current_agent_position = state.agents_positions[current_agent]
        return current_agent_position in self.world.exit_pos

    def add_to_visited(self
                          , state: MyWorldState) -> None:
        """Adds state to visited states."""
        self.visited.add(state.serialize())

    def remove_from_visited(self
                            , state: MyWorldState) -> None:
        """Removes state from visited states."""
        self.visited.remove(state.serialize())

    def was_visited(self,
                    state: MyWorldState) -> bool:
        return state.serialize() in self.visited
    
    def add_value_to_node(self
                          , state
                          , value: float
                          , discriminator: str
                          , alpha: float = None
                            , beta: float = None
                          ) -> None:
        """Adds value to node"""
        #add best_value to the node name
        new_state_string = state.to_string()
        new_state_string_with_best_value = new_state_string + "\n "+discriminator+" value : " + str(value)
        if alpha != None:
            new_state_string_with_best_value += "\n alpha : " + str(alpha)
        if beta != None:
            new_state_string_with_best_value += "\n beta : " + str(beta)
        if self.is_final(state):
            new_state_string_with_best_value += "\n FINAL"
        # replace
        self.nodes[new_state_string].name = new_state_string_with_best_value

    def transition(self
                   , state: MyWorldState
                   , action: Action
                   ) -> MyWorldState:
        """Returns the next state and the reward.
        If Agent 0 dies during a transition, 
        the state value immediately drops to 
        lle.REWARD_AGENT_DIED (-1), 
        without taking into account any gems already collected
        """
        self.n_expanded_states += 1
        simulation_world = copy.deepcopy(state.world)
        world_string = copy.deepcopy(state.world_string)
        simulation_world.set_state(self.convert_to_WorldState(state))

        simulation_state = simulation_world.get_state()
        simulation_state_current_agent = state.current_agent
        current_agent_previous_position = simulation_state.agents_positions[simulation_state_current_agent]
        actions = self.get_actions(simulation_state_current_agent, action)
        next_state_value_vector = copy.deepcopy(state.value_vector)
        reward = 0.0
        reward = simulation_world.step(actions)
        next_state_value_vector[simulation_state_current_agent] += reward
        if simulation_state_current_agent == 0:
            if reward == -1:
                next_state_value_vector[0] = -1.0 #lle.REWARD_AGENT_DIED
        next_state_current_agent = (simulation_state_current_agent+1)%simulation_world.n_agents
        my_world_state_transitioned = MyWorldState(next_state_value_vector[0]
                                                   , next_state_value_vector
                                                   , next_state_current_agent
                                                   , simulation_world
                                                   , world_string
                                                   , action
                                                   )
        my_world_state_transitioned.update_world_string(simulation_state_current_agent
                                                        , current_agent_previous_position
                                                        , actions)
        return my_world_state_transitioned
    

def balanced_multi_salesmen_greedy_tsp(remaining_cities: list[Tuple[int, int]]
                                       , num_salesmen: int
                                       , start_cities: list[Tuple[int, int]]
                                       , finish_cities: list[Tuple[int, int]]
                                       ) -> Tuple[dict[str, list[Tuple[int, int]]], dict[str, float], float]: 
    #todo: calculate the distance between the last city and the finish city one time at problem creation
    """Given a list of cities coordinates, returns a list of cities visited by each agent
    in the order that minimizes the total distance traveled.
    """
    routes = {f"agent_{i+1}": [start_cities[i]] for i in range(num_salesmen)}
    distances = {f"agent_{i+1}": 0.0 for i in range(num_salesmen)}
    while remaining_cities:
        for agent in routes.keys():
            if not remaining_cities:
                break
            nearest_city, nearest_distance = min_distance_position(routes[agent][-1], remaining_cities)
            distances[agent] += nearest_distance
            routes[agent].append(nearest_city)
            remaining_cities.remove(nearest_city)
    for agent in routes.keys():
        current_city = routes[agent][-1]
        finish_city, final_distance = min_distance_position(current_city, finish_cities)
        distances[agent] += final_distance
        routes[agent].append(finish_city)
    total_distance = sum(distances.values())
    return routes, distances, total_distance


class BetterValueFunction(WorldMDP):
    """Subclass of WorldMDP
    in which the state value
      is calculated more intelligently than simply considering Agent 0's score. 
     
        Improvements:

        If Agent 0 dies during a transition, 
            the state value is reduced by #todo
            , but the gems already collected are taken into account.
        The value of a state is increased by 
        the average of the score differences between Agent 0 and the other agents.."""
    def get_position_after_action(self
                                  , agent_pos: Tuple[int, int]
                                    , action: Action
                                    ) -> Tuple[int, int]:
        """Returns the position of the agent after performing the given action in the given state."""
        agent_pos_after_action = None
        # Apply the action to the agent's position
        if action == Action.NORTH:
            agent_pos_after_action = (agent_pos[0] - 1, agent_pos[1])
        elif action == Action.SOUTH:
            agent_pos_after_action = (agent_pos[0] + 1, agent_pos[1])
        elif action == Action.WEST:
            agent_pos_after_action = (agent_pos[0], agent_pos[1] - 1)
        elif action == Action.EAST:
            agent_pos_after_action = (agent_pos[0], agent_pos[1] + 1)
        elif action == Action.STAY:
            agent_pos_after_action = (agent_pos[0], agent_pos[1])
        else:
            raise ValueError("Invalid action")
        return agent_pos_after_action
    
    def get_available_actions_ordered(self
                                    , state: MyWorldState
                                    ) -> List[A]:
        """Returns the available actions ordered by heuristic value"""
        available_actions = super().available_actions(state)
        current_agent = state.current_agent
        # move STAY to the end of the list
        available_actions_ordered = [action for action in available_actions if action != Action.STAY]
        available_actions_ordered.append(Action.STAY)
        for action in available_actions:
            position_after_action = self.get_position_after_action(state.agents_positions[current_agent]
                                                                    , action
                                                                    )
            # if not all gems are collected,
            # not all (not gem for gem in state.gems_collected):
            gems_to_collect = [gem[0] for gem in state.world.gems if not gem[1].is_collected]

            if gems_to_collect:
                # if action leads to a gem, move it to the top of the list
                if position_after_action in [gem[0] for gem in state.world.gems]:
                    available_actions_ordered.remove(action)
                    available_actions_ordered.insert(0, action)
            # if a laser sources has not the same color as the agent, 
            if self.lasers_dangerous_for_agents[state.current_agent]:
                if position_after_action in [laser[0] for laser in state.world.lasers if laser[1].is_on]:
                    available_actions_ordered.remove(action)
                    available_actions_ordered.append(action)

        return available_actions_ordered

    def available_actions(self, state: MyWorldState) -> list[Action]:
        return self.get_available_actions_ordered(state)
    
    def transition(self
                   , state: MyWorldState
                   , action: Action
                   , depth: int = 0
                   ) -> MyWorldState:
        """Returns the next state and the reward.
        """
        # Change the value of the state here.
        state = super().transition(state
                                   , action
                                   )
        n_agents = self.world.n_agents
        previous_agent = (state.current_agent-1)%n_agents
        current_agent = previous_agent
        state_agents_positions = state.agents_positions
        value = state.value
        if value == -1 or value == 0:
            return state
        world_gems = state.world.gems
        gems_to_collect = [gem[0] for gem in world_gems if not gem[1].is_collected]
        _, distances, total_distance = balanced_multi_salesmen_greedy_tsp(copy.deepcopy(gems_to_collect)
                                                       , n_agents
                                                       , state_agents_positions
                                                       , self.world.exit_pos)
        current_agent_distance = distances[f"agent_{current_agent+1}"] # +1 because agent_0 is agent_1 #todo
        if current_agent_distance == 1:
            current_agent_distance = 1.5
        other_agents_distances = [distances[f"agent_{i+1}"] for i in range(n_agents) if i != current_agent]
        other_agents_average_distance_length = len(other_agents_distances)
        if other_agents_average_distance_length == 0:
            other_agents_average_distance_length = 1
        if gems_to_collect:
            # prefer current agent to be closer to the nearest gem
            # and other agents to be far from the nearest gem
            if current_agent_distance != 0:
                value = value + (len(gems_to_collect) / current_agent_distance)
        else:
            # prefer agent to be closer to the exit
            if current_agent_distance != 0:
                value = value + (lle.REWARD_AGENT_JUST_ARRIVED / current_agent_distance)
                # prefer all agents to be closer to the exit
                average_distance_to_exit = total_distance/n_agents
                # add reward for each agent being on exit lle.REWARD_AGENT_ON_EXIT/their distance to exit
                if other_agents_distances:
                    if all(distance == 0 for distance in other_agents_distances):
                        value = value + lle.REWARD_END_GAME/average_distance_to_exit
                else:
                    value = value + lle.REWARD_END_GAME*10/current_agent_distance
        state.value = value
        state.value_vector[current_agent] = value
        return state
    


----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\doc\in\2-adversarial\src\world_mdp.py -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\src\almost_equal.py -----
def almost_equal(a, b):
    return abs(a - b) < 1e-6


----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\src\almost_equal.py -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\src\analysis.py -----
from dataclasses import (
    dataclass,
)  # dataclass is a decorator that allows you to create a class with a minimal amount of code 
# (see https://docs.python.org/3/library/dataclasses.html)


@dataclass
class Parameters:
    reward_live: float
    """Reward for living at each time step"""

    gamma: float
    """Discount factor"""
    noise: float
    """Probability of taking a random action instead of the chosen one"""

def prefer_close_exit_following_the_cliff() -> Parameters:
    # A strategy focusing on reaching the closest exit quickly, even if it means taking risks.
    # A small negative reward_live to encourage speed, moderate gamma for balancing immediate and future rewards, and low noise for deliberate actions.
    return Parameters(reward_live=-0.01, gamma=0.5, noise=0.1)


def prefer_close_exit_avoiding_the_cliff() -> Parameters:
    # A cautious strategy aiming for the nearest exit while avoiding risks.
    # Less negative reward_live for a safer, longer route, higher gamma for long-term planning, and low noise to avoid risky random moves.
    return Parameters(reward_live=-0.005, gamma=0.7, noise=0.05)


def prefer_far_exit_following_the_cliff() -> Parameters:
    # A strategy that targets a distant exit but might involve risk-taking.
    # Small negative reward_live, high gamma for focusing on distant rewards, and moderate noise for some randomness in path selection.
    return Parameters(reward_live=-0.01, gamma=0.8, noise=0.2)


def prefer_far_exit_avoiding_the_cliff() -> Parameters:
    # A strategy preferring a distant exit with an emphasis on safety and planning.
    # Less negative reward_live for longer routes, high gamma for future-oriented planning, and low noise for consistent decision-making.
    return Parameters(reward_live=-0.005, gamma=0.9, noise=0.05)


def never_end_the_game() -> Parameters:
    # A unique strategy to avoid reaching terminal states and keep the game ongoing.
    # Zero or slightly positive reward_live to encourage continual play, low gamma to de-emphasize distant futures (like exits), and high noise for unpredictability.
    return Parameters(reward_live=0.01, gamma=0.3, noise=0.5)


if __name__ == "__main__":
    print("Testing analysis.py")


----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\src\analysis.py -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\src\auto_indent.py -----
import sys
import inspect


class AutoIndent(object):
    def __init__(self, stream):
        self.stream = stream
        self.offset = 0
        self.frame_cache = {}

    def flush(self):
        pass

    def indent_level(self):
        i = 0
        base = sys._getframe(2)
        f = base.f_back
        while f:
            if id(f) in self.frame_cache:
                i += 1
            f = f.f_back
        if i == 0:
            # clear out the frame cache
            self.frame_cache = {id(base): True}
        else:
            self.frame_cache[id(base)] = True
        return i

    def write(self, stuff):
        # sys.stdout.reconfigure(encoding='utf-8')
        stuff = stuff.encode('utf-8', errors='replace').decode('utf-8')

        indentation = "  " * self.indent_level()

        def indent(l):
            if l:
                return indentation + l
            else:
                return l

        stuff = "\n".join([indent(line) for line in stuff.split("\n")])
        self.stream.write(stuff)
        # write stuff in file ./src/log.txt
        with open("./src/log.txt", "a") as f:
            # flush log.txt
            f.seek(0)
            f.write(stuff)



----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\src\auto_indent.py -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\src\graph_mdp.py -----
import json
from dataclasses import dataclass
from mdp import MDP

Action = str
State = str


@dataclass
class Transition:
    source: State
    action: Action
    destination: State
    reward: float
    probability: float


class GraphMDP(MDP[str, str]):
    @staticmethod
    def from_json(filename: str) -> "GraphMDP":
        with open(filename, "r") as f:
            data = json.load(f)
            start_states = data["start_state"]
            end_states = data["end_states"]
            transitions = {}
            for source, actions in data["transitions"].items():
                transitions[source] = {}
                for action, destinations in actions.items():
                    transitions[source][action] = []
                    for dest in destinations:
                        transitions[source][action].append(
                            Transition(
                                source,
                                action,
                                dest["to"],
                                dest["reward"],
                                dest["probability"],
                            )
                        )
            return GraphMDP(start_states, end_states, transitions)

    def __init__(
        self,
        start_states: list[State],
        end_states: list[State],
        transitions: dict[State, dict[Action, list[Transition]]],
    ):
        self.start_states = set(start_states)
        self.end_states = set(end_states)
        self._transitions = transitions
        self._all_states = self.start_states.union(self.end_states).union(
            set(transitions.keys())
        )

    def is_final(self, state: State) -> bool:
        return state in self.end_states

    def transitions(self, state: State, action: Action) -> list[tuple[State, float]]:
        return [
            (t.destination, t.probability) for t in self._transitions[state][action]
        ]

    def available_actions(self, state: str):
        return self._transitions[state].keys()

    def reward(self, state: str, action: str, new_state) -> float:
        transitions = self._transitions[state][action]
        for t in transitions:
            if t.destination == new_state:
                return t.reward
        raise ValueError(
            f"Invalid transition from {state} to {new_state} with action {action}"
        )

    def states(self):
        return self._all_states


----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\src\graph_mdp.py -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\src\mdp.py -----
from typing import TypeVar, Generic
from abc import abstractmethod, ABC


A = TypeVar("A")
S = TypeVar("S")


class MDP(ABC, Generic[S, A]):
    """Adversarial Markov Decision Process"""

    # gems_quantity = S.
    @abstractmethod
    def is_final(self, state: S) -> bool:
        """Returns true 
        if the given state is final (i.e. the game is over)."""


    @abstractmethod
    def available_actions(self, state: S) -> list[A]:
        """Returns the list of available actions for the current agent from the given state."""

    @abstractmethod
    def transitions(self, state: S, action: A) -> list[tuple[S, float]]:
        """Returns the list of next states with the probability of reaching it by performing the given action."""

    @abstractmethod
    def states(self) -> list[S]:
        """Returns the list of all states."""

    @abstractmethod
    def reward(self, state: S, action: A, new_state) -> float:
        """Reward function"""
        


----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\src\mdp.py -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\src\qlearning.py -----
from rlenv import RLEnv, Observation
import numpy as np
import numpy.typing as npt

import sys

print(sys.path)


class QLearning:
    """Tabular QLearning"""

    def __init__(self, learning_rate: float, discount_factor: float, epsilon: float):
        # Initialize parameters
        self.learning_rate = learning_rate
        self.discount_factor = discount_factor
        self.epsilon = epsilon
        # Initialize Q-table
        self.q_table = {}

    def choose_action(self, state):
        # Implement action selection using epsilon-greedy strategy
        pass

    def update(self, state, action, reward, next_state):
        # Implement Q-table update
        pass

if __name__ == "__main__":
    print("Hello World")
    print(sys.path)

----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\src\qlearning.py -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\src\value_iteration.py -----
import copy
import sys

from lle import World, WorldState
from almost_equal import almost_equal
from graph_mdp import GraphMDP
from mdp import MDP, S, A
from typing import Generic

from auto_indent import AutoIndent
from world_mdp import WorldMDP
import utils

sys.stdout = AutoIndent(sys.stdout)


class ValueIteration(Generic[S, A]):
    def __init__(self, 
                 mdp: MDP[S, A], 
                 gamma: float):  # discount factor
        # senf.values est nécessaire pour fonctionner avec utils.show_values
        self.mdp = mdp
        self.gamma = gamma
        # self.values = dict[S, float]()
        self.values = {
            state: 0.0 for state in mdp.states()
        }  # Initialize all states with a default value
        utils.show_values(self.values)

    def value(self, state: S) -> float:
        """Returns the value of the given state."""
        # return self.values[state]
        return self.values.get(state, 0.0)  # Default value if state not found

    def policy(self, state: S) -> A:
        """Returns the action
        that maximizes the Q-value of the given state."""
        available_actions = self.mdp.available_actions(state)
        if not available_actions:
            print("No available actions for state", state)
            return None  # Or some default action if appropriate
        return max(available_actions, key=lambda action: self.qvalue(state, action))

    def qvalue(self, state: S, action: A) -> float:
        """
        Returns the Q-value
        of the given state-action pair
        based on the state values.
        from Bellman equation:
        Q(s,a) = Sum(P(s,a,s') * (R(s,a,s') + gamma * V(s')))
        """
        qvalue = 0.0
        next_states_and_probs = self.mdp.transitions(state, action)
        print("next_states_and_probs: \n", next_states_and_probs, "\n")
        for next_state, prob in next_states_and_probs:
            reward = self.mdp.reward(state, action, next_state)
            print("P(", state, action, next_state, "):", prob)
            print("R(", state, action, next_state, "):", reward)
            qvalue += prob * (reward + self.gamma * self.value(next_state))
        print("Q-value of", state, action, ":", qvalue)
        print()
        return qvalue

    def _compute_value_from_qvalues(self, state: S) -> float:
        """
        Returns the value of the given state based on the Q-values.
        from Bellman equation:
        V(s) = max_a Sum(P(s,a,s') * (R(s,a,s') + gamma * V(s')))

        This is a private method,
        meant to be used by the value_iteration method.
        """
        return max(
            self.qvalue(state, action) for action in self.mdp.available_actions(state)
        )

    def get_values_at_position(self, i: int, j: int) -> list[float]:
        """Returns the values of the states at the given position."""
        # world_gems_quantity = self.mdp.
        states_at_position = [
            state
            for state in self.mdp.states()
            if state.agents_positions[0] == (i, j)
        ]
        values_at_position = [self.value(state) for state in states_at_position]
        increasing_values = sorted(values_at_position)

        return increasing_values

    def print_values_table(self):
        """In a map's representation table, 
        each tile contains the possible values at that position."""
        if not isinstance(self.mdp, WorldMDP):
            print("Cannot print values table for non-world MDP")
            return None
        max_len = 0
        for i in range(self.mdp.world.height):
            for j in range(self.mdp.world.width):
                values = self.get_values_at_position(i, j)
                # Convert the list of values to a string and find the maximum length
                values_str = str(values)
                max_len = max(max_len, len(values_str))

        for i in range(self.mdp.world.height):
            for j in range(self.mdp.world.width):
                values = self.get_values_at_position(i, j)
                # Format each string to have the same width
                print(f"{str(values):<{max_len}}", end=" ")
            print()

    def print_iteration_values(self, iteration: int):
        """Prints the states and their values."""
        print("Iteration", iteration, "States and their values:")
        for state in self.mdp.states():
            print(state, self.value(state))

    def value_iteration(self, n: int):  # number of iterations
        """Performs value iteration for the given number of iterations."""
        states = self.mdp.states()

        for _ in range(n):
            new_values = copy.deepcopy(self.values)
            for state in states:
                if self.mdp.is_final(state):
                    print("Final state", state)
                    new_values[state] = 0.0
                else:
                    new_values[state] = self._compute_value_from_qvalues(state)
            self.values = new_values
            # self.print_iteration_values(_)
        self.print_iteration_values(n)
        self.print_values_table()


if __name__ == "__main__":
    # graph
    # b - +1 - a - -1 - c
    # graph_file_name = "tests/graphs/graph1.json"
    # mdp = GraphMDP.from_json(graph_file_name)
    # gamma = 0.9
    # algo = ValueIteration(mdp, gamma)
    # # algo.value_iteration(10)
    # algo.value_iteration(100)
    # assert almost_equal(algo.qvalue("a", "left"), 0.6)  # no change from iteration 0
    # assert almost_equal(
    #     algo.qvalue("a", "right"), 0.90909090909
    # )  # more than iteration 0 & 1

    mdp = WorldMDP(
        World(
            """
    .  . . X
    .  @ . V
    S0 . . ."""
        )
    )
    algo = ValueIteration(mdp, 0.9)
    algo.value_iteration(10)
    # algo.value_iteration(100)
    # expected = [
    #     [1.62, 1.80, 2.0, 0.0],
    #     [1.458, 0.0, 1.80, 0.0],
    #     [1.3122, 1.458, 1.62, 1.458],
    # ]
    # for i in range(mdp.world.height):
    #     for j in range(mdp.world.width):
    #         state = WorldState([(i, j)], [])
    #         assert almost_equal(algo.value(state), expected[i][j])


----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\src\value_iteration.py -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\src\world_mdp.py -----
from itertools import product
from typing import Iterable
from lle import World, WorldState, Action
from mdp import MDP


class WorldMDP(MDP[WorldState, list[Action]]):
    def __init__(self, world: World):
        self.world = world

    def available_actions(self, state: WorldState):
        self.world.set_state(state)
        available = self.world.available_actions()
        return list(product(*available))

    def transitions(
        self, state: WorldState, action: list[Action]
    ) -> list[tuple[WorldState, float]]:
        self.world.set_state(state)
        self.world.step(action)
        return [(self.world.get_state(), 1.0)]

    def is_final(self, state: WorldState) -> bool:
        self.world.set_state(state)
        return self.world.done

    def reward(
        self,
        state: WorldState,
        action: list[Action],
        new_state: WorldState,
    ) -> float:
        # Step the world and check if the new state is the same as the given one
        # If if is not the same, then test all the other available actions.
        self.world.set_state(state)
        actions = [action] + [[a] for a in self.world.available_actions()[0]]
        for a in actions:
            r = self.world.step(a)
            if self.world.get_state() == new_state:
                return r
            self.world.set_state(state)
        raise ValueError("The new state is not reachable from the given state")

    def states(self) -> Iterable[WorldState]:
        all_positions = set(product(range(self.world.height), range(self.world.width)))
        all_positions = all_positions.difference(set(self.world.wall_pos))
        agents_positions = list(product(all_positions, repeat=self.world.n_agents))
        collection_status = list(product([True, False], repeat=self.world.n_gems))

        for pos, collected in product(agents_positions, collection_status):
            s = WorldState(list(pos), list(collected))
            yield s


----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\src\world_mdp.py -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\src\__init__.py -----


----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\src\__init__.py -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\tests\graph_mdp.py -----
import json
from dataclasses import dataclass
from src.mdp import MDP

Action = str
State = str


@dataclass
class Transition:
    source: State
    action: Action
    destination: State
    reward: float
    probability: float


class GraphMDP(MDP[str, str]):
    @staticmethod
    def from_json(filename: str) -> "GraphMDP":
        with open(filename, "r") as f:
            data = json.load(f)
            start_states = data["start_state"]
            end_states = data["end_states"]
            transitions = {}
            for source, actions in data["transitions"].items():
                transitions[source] = {}
                for action, destinations in actions.items():
                    transitions[source][action] = []
                    for dest in destinations:
                        transitions[source][action].append(
                            Transition(
                                source,
                                action,
                                dest["to"],
                                dest["reward"],
                                dest["probability"],
                            )
                        )
            return GraphMDP(start_states, end_states, transitions)

    def __init__(
        self,
        start_states: list[State],
        end_states: list[State],
        transitions: dict[State, dict[Action, list[Transition]]],
    ):
        self.start_states = set(start_states)
        self.end_states = set(end_states)
        self._transitions = transitions
        self._all_states = self.start_states.union(self.end_states).union(
            set(transitions.keys())
        )

    def is_final(self, state: State) -> bool:
        return state in self.end_states

    def transitions(self, state: State, action: Action) -> list[tuple[State, float]]:
        return [
            (t.destination, t.probability) for t in self._transitions[state][action]
        ]

    def available_actions(self, state: str):
        return self._transitions[state].keys()

    def reward(self, state: str, action: str, new_state) -> float:
        transitions = self._transitions[state][action]
        for t in transitions:
            if t.destination == new_state:
                return t.reward
        raise ValueError(
            f"Invalid transition from {state} to {new_state} with action {action}"
        )

    def states(self):
        return self._all_states


----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\tests\graph_mdp.py -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\tests\modified_reward_world.py -----
from .world_mdp import WorldMDP
from lle import Action, World, WorldState


class ModifiedRewardWorld(WorldMDP):
    def __init__(self, 
                 reward_live: float # Reward for living at each time step
                 ):
        super().__init__(World.from_file("tests/graphs/cliff"))
        self.world.reset() # Reset the world to its initial state
        self.reward_live = reward_live

    def reward(
        self, 
        state: WorldState, 
        action: list[Action], 
        new_state: WorldState
    ) -> float:
        reward = super().reward(state, action, new_state)
        # The agent has died
        if reward < 0:
            return -10.0
        # The agent has collected a gem
        # The agent has reached the exit
        if reward > 0:
            # Close exit
            if new_state.agents_positions[0] == (2, 2):
                return 1.0
            # Far exit
            return 10.0
        # The agent just lives (reward should be 0)
        
        return self.reward_live


----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\tests\modified_reward_world.py -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\tests\random_wrapper.py -----
from src.mdp import MDP, S, A


class RandomWrapper(MDP[S, A]):
    """Wrapper around an MDP such that the action taken can be random with probability p.
    It only changes the `transitions` method. The other methods are unchanged."""

    def __init__(self, mdp: MDP[S, A], p: float):
        super().__init__()
        self.mdp = mdp
        self.p = p
        """The probability of the agent to perform a random action"""

    def is_final(self, state: S) -> bool:
        return self.mdp.is_final(state)

    def available_actions(self, state: S) -> list[A]:
        return self.mdp.available_actions(state)

    def transitions(self, state: S, action: A) -> list[tuple[S, float]]:
        if self.p == 0:
            return self.mdp.transitions(state, action)
        # Create the dictionary of "normal" destinations
        destination_probs = dict(self.mdp.transitions(state, action))
        # The probabilities must be multiplied by (1 - p) because of the wrapper
        for s, prob in destination_probs.items():
            destination_probs[s] = prob * (1 - self.p)
        # Add the random destinations
        available_actions = list(self.available_actions(state))
        n_actions = len(available_actions)
        for action in available_actions:
            for s, prob in self.mdp.transitions(state, action):
                # The probability of taking this action must be multiplied by (p / n_actions)
                prob = (prob * self.p) / n_actions
                destination_probs[s] = destination_probs.get(s, 0.0) + prob
        return list(destination_probs.items())

    def states(self) -> list[S]:
        return self.mdp.states()

    def reward(self, state: S, action: A, new_state) -> float:
        return self.mdp.reward(state, action, new_state)


----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\tests\random_wrapper.py -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\tests\test_analysis.py -----
from src.analysis import (
    prefer_close_exit_avoiding_the_cliff,
    prefer_close_exit_following_the_cliff,
    prefer_far_exit_avoiding_the_cliff,
    prefer_far_exit_following_the_cliff,
    never_end_the_game,
    Parameters,
)
from src.mdp import MDP
from src.value_iteration import ValueIteration
from .random_wrapper import RandomWrapper
from .modified_reward_world import ModifiedRewardWorld


def setup_mdp_and_get_path(param: Parameters):
    mdp = ModifiedRewardWorld(param.reward_live)
    start_state = mdp.world.get_state()
    noisy_mdp = RandomWrapper(mdp, param.noise) # Add noise to the MDP
    algo = ValueIteration(noisy_mdp, param.gamma) # Create the algorithm 
    algo.value_iteration(100)
    return apply_policy(start_state, mdp, algo)


def apply_policy(state, 
                 mdp: MDP, 
                 algo: ValueIteration
                 ):
    """Apply the policy 
    until the end of the game 
    or a loop is detected. 
    Returns the path taken."""
    path = []
    while not mdp.is_final(state) and state not in path: # While the game is not over and the state is not in the path
        path.append(state)
        action = algo.policy(state)
        print(action)
        transitions = mdp.transitions(state, action) # Get the possible transitions
        assert len(transitions) == 1 # There is only one possible transition
        state, _ = transitions[0]
    path.append(state)
    print(path)
    return path


def test_close_exit_following_the_cliff():
    params = prefer_close_exit_following_the_cliff()
    path = setup_mdp_and_get_path(params)
    expected = [(3, 0), (3, 1), (3, 2), (2, 2)]
    assert len(path) == len(expected)
    for exp, state in zip(expected, path):
        assert state.agents_positions[0] == exp


def test_close_exit_avoiding_the_cliff():
    params = prefer_close_exit_avoiding_the_cliff()
    path = setup_mdp_and_get_path(params)
    expected = [(3, 0), (2, 0), (1, 0), (0, 0), (0, 1), (0, 2), (1, 2), (2, 2)]
    assert len(path) == len(expected)
    for exp, state in zip(expected, path):
        assert state.agents_positions[0] == exp


def test_far_exit_following_the_cliff():
    params = prefer_far_exit_following_the_cliff()
    path = setup_mdp_and_get_path(params)
    expected = [(3, 0), (3, 1), (3, 2), (3, 3), (3, 4), (2, 4)]
    assert len(path) == len(expected)
    for exp, state in zip(expected, path):
        assert state.agents_positions[0] == exp


def test_far_exit_avoiding_the_cliff():
    params = prefer_far_exit_avoiding_the_cliff()
    path = setup_mdp_and_get_path(params)
    assert len(path) == 10
    # This is only the start of the path because the second half could vary
    expected = [(3, 0), (2, 0), (1, 0), (0, 0), (0, 1), (0, 2)]
    for exp, state in zip(expected, path):
        assert state.agents_positions[0] == exp


def test_never_end():
    params = never_end_the_game()
    path = setup_mdp_and_get_path(params)
    # Check that there is a loop in the path
    assert path[-1] in path[:-1] # The last state is in the path before the last state


----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\tests\test_analysis.py -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\tests\test_value_iteration.py -----
from src.value_iteration import ValueIteration
from lle import World, WorldState, Action, REWARD_AGENT_JUST_ARRIVED, REWARD_END_GAME
from tests.world_mdp import WorldMDP
from .graph_mdp import GraphMDP
from matplotlib import pyplot as plt


def almost_equal(a, b):
    return abs(a - b) < 1e-6


graph_file_name = "tests/graphs/graph1.json"

# 4-rl\tests\graphs\graph1.json
def test_value_0():
    mdp = GraphMDP.from_json(graph_file_name)
    algo = ValueIteration(mdp, 0.9)
    algo.value_iteration(0)
    for s in mdp.states():
        assert algo.value(s) == 0.0


def test_value_end_states():
    """
    tests that the value of end states is 0 after 100 iterations
    """
    mdp = GraphMDP.from_json(graph_file_name)
    algo = ValueIteration(mdp, 0.9)
    algo.value_iteration(100)
    for s in mdp.states():
        if mdp.is_final(s):
            assert algo.value(s) == 0.0


def test_qvalues_0():
    """
    test qvalues for the graph mdp after 0 iterations
    """
    mdp = GraphMDP.from_json(graph_file_name)
    algo = ValueIteration(mdp, 0.9)
    algo.value_iteration(0)
    assert almost_equal(algo.qvalue("a", "left"), 0.6)
    assert algo.qvalue("a", "right") == 0.5


def test_max_action_0():
    """
    test policy max_action for the graph mdp after 0 iterations
    """
    mdp = GraphMDP.from_json(graph_file_name)
    algo = ValueIteration(mdp, 0.9)
    algo.value_iteration(0)
    assert algo.policy("a") == "left"


def test_value_1():
    """
    test value of states after 1 iteration
    """
    mdp = GraphMDP.from_json(graph_file_name)
    gamma = 0.9
    algo = ValueIteration(mdp, gamma)
    algo.value_iteration(1)
    assert almost_equal(algo.qvalue("a", "left"), 0.6) # no change from iteration 0
    expected = 0.5 + 0.5 * gamma * 0.6 # 0.77 # = 0.5 + 0.5 * 0.9 * 0.6
    assert almost_equal(algo.qvalue("a", "right"), expected) # more than iteration 0


def test_value_100():
    """
    test value of states after 100 iterations
    """
    mdp = GraphMDP.from_json(graph_file_name)
    gamma = 0.9
    algo = ValueIteration(mdp, gamma)
    algo.value_iteration(100)
    assert almost_equal(algo.qvalue("a", "left"), 0.6) # no change from iteration 0
    assert almost_equal(algo.qvalue("a", "right"), 0.90909090909) # more than iteration 0 & 1


def test_value_world_0():
    """
    test value of world states after 0 iterations
    """
    mdp = WorldMDP(
        World(
            """
    .  . . X
    .  @ . V
    S0 . . ."""
        )
    )
    algo = ValueIteration(mdp, 0.9)
    algo.value_iteration(0)
    for s in mdp.states():
        assert algo.value(s) == 0.0


def test_qvalues_world():
    """
    test qvalues for the world mdp after 0 iterations
    """
    mdp = WorldMDP(
        World(
            """
    .  . . X
    .  @ . V
    S0 . . ."""
        )
    )
    algo = ValueIteration(mdp, 0.9)
    algo.value_iteration(0)
    state = WorldState([(0, 2)], [])
    assert (
        algo.qvalue(state, [Action.EAST]) == REWARD_END_GAME + REWARD_AGENT_JUST_ARRIVED
    )


def test_value_world_100():
    """
    test value of world states after 100 iterations
    """
    mdp = WorldMDP(
        World(
            """
    .  . . X
    .  @ . V
    S0 . . ."""
        )
    )
    algo = ValueIteration(mdp, 0.9)
    algo.value_iteration(100)
    expected = [
        [1.62, 1.80, 2.0, 0.0],
        [1.458, 0.0, 1.80, 0.0],
        [1.3122, 1.458, 1.62, 1.458],
    ]
    for i in range(mdp.world.height):
        for j in range(mdp.world.width):
            state = WorldState([(i, j)], [])
            assert almost_equal(algo.value(state), expected[i][j])


if __name__ == "__main__":
    print("hello world")
    test_value_0()
    test_value_end_states()
    test_qvalues_0()
    test_max_action_0()
    test_value_1()
    test_value_100()
    test_value_world_0()
    test_qvalues_world()
    test_value_world_100()
    print("ok")


----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\tests\test_value_iteration.py -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\tests\world_mdp.py -----
from itertools import product
from typing import Iterable
from lle import World, WorldState, Action
from src.mdp import MDP


class WorldMDP(MDP[WorldState, list[Action]]):
    def __init__(self, world: World):
        self.world = world

    def available_actions(self, state: WorldState):
        self.world.set_state(state)
        available = self.world.available_actions()
        return list(product(*available))

    def transitions(
        self, 
        state: WorldState, 
        action: list[Action]
    ) -> list[tuple[WorldState, float]]:
        self.world.set_state(state)
        self.world.step(action)
        return [(self.world.get_state(), 1.0)]

    def is_final(self, state: WorldState) -> bool:
        self.world.set_state(state)
        return self.world.done

    def reward(
        self,
        state: WorldState,
        action: list[Action],
        new_state: WorldState,
    ) -> float:
        # Step the world and check if the new state is the same as the given one
        # If if is not the same, then test all the other available actions.
        self.world.set_state(state)
        actions = [action] + [[a] for a in self.world.available_actions()[0]]
        for a in actions:
            r = self.world.step(a)
            if self.world.get_state() == new_state:
                return r
            self.world.set_state(state)
        raise ValueError("The new state is not reachable from the given state")

    def states(self) -> Iterable[WorldState]:
        all_positions = set(product(range(self.world.height), range(self.world.width)))
        all_positions = all_positions.difference(set(self.world.wall_pos))
        agents_positions = list(product(all_positions, repeat=self.world.n_agents))
        collection_status = list(product([True, False], repeat=self.world.n_gems))

        for pos, collected in product(agents_positions, collection_status):
            s = WorldState(list(pos), list(collected))
            yield s


----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\tests\world_mdp.py -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\tests\__init__.py -----


----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\tests\__init__.py -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\tests\graphs\cliff -----
.  . . . .
.  @ . . .
.  @ X @ X
S0 . . . .
V  V V V V

----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\tests\graphs\cliff -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\tests\graphs\cliff.json -----
{
    "states": [
        "A1",
        "A2",
        "A3",
        "A4",
        "A5",
        "B1",
        "B2",
        "B3",
        "B4",
        "B5",
        "C1",
        "C2",
        "C3",
        "C4",
        "C5",
        "D1",
        "D2",
        "D3",
        "D4",
        "D5",
        "E1",
        "E2",
        "E3",
        "E4",
        "E5"
    ],
    "start_state": "A2",
    "end_states": [
        "A1",
        "B1",
        "C1",
        "D1",
        "E1"
    ]
}

----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\tests\graphs\cliff.json -----

----- Start of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\tests\graphs\graph1.json -----
{
    "states": [
        "a",
        "b",
        "c"
    ],
    "start_state": "a",
    "end_states": [
        "b",
        "c"
    ],
    "actions": [
        "left",
        "right"
    ],
    "transitions": {
        "a": {
            "left": [
                {
                    "to": "b",
                    "probability": 0.8,
                    "reward": 1
                },
                {
                    "to": "c",
                    "probability": 0.2,
                    "reward": -1
                }
            ],
            "right": [
                {
                    "to": "a",
                    "probability": 0.5,
                    "reward": 0
                },
                {
                    "to": "b",
                    "probability": 0.5,
                    "reward": 1
                }
            ]
        }
    }
}

----- End of d:\bourg\Documents\GitHub\artificial-intelligence-reinforcement-learning\4-rl\tests\graphs\graph1.json -----

