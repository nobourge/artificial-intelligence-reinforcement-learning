of 3
FACULTÃ‰ DES SCIENCES
DÃ‰PARTEMENT Dâ€™INFORMATIQUE
INFO-F-311: Intelligence Artiï¬cielle
Projet 2: Recherche adversariale
Yannick Molinghen Pascal Tribel Tom Lenaerts
1. PrÃ©ambule
Dans ce projet, vous allez implÃ©menter des techniques dâ€™intelligence articiï¬elle basÃ©es sur de la
recherche dans des graphes en considÃ©rant un ou plusieurs adversaires. On vous fournit des ï¬chiers
de base pour le projet que vous pouvez les tÃ©lÃ©charger sur lâ€™universitÃ© virtuelle.
1.1. Rust
Lâ€™environnement dans lequel vous allez travailler est implÃ©mentÃ© en Rust. Pour pouvoir compiler le
projet, vous aurez besoin dâ€™installer le compilateur adaptÃ© avec la commande ci-dessous issue du site
oï¬ƒciel.
curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
Si vous utilisez Windows, vous pouvez utiliser Windows Subsystem for Linux (WSL) ou vous rÃ©fÃ©rer
Ã  la procÃ©dure dÃ©crite dans la documentation.
1.2. Poetry
Le projet nÃ©cessite Python â‰¥ 3.10 et utilise Poetry pour gÃ©rer les dÃ©pendances Python et les environ-
nements virtuels. Ouvrez un terminal dans le rÃ©pertoire du projet, installez Poetry puis installez les
mises dÃ©pendances du projet:
poetry shell
poetry install
Ces commandes vont automatiquement crÃ©er un environnement virtuel puis installer les dÃ©pendances
du projet.
1.3. Tests automatiques
Vous pouvez tester vos mÃ©thodes avec la commande pytest. Pour lancer uniquement les tests dâ€™un
ï¬chier en particulier, vous pouvez le donner en paramÃ¨tre:
pytest tests/tests_bfs.py
Il y a un ï¬chier de test pour chacune des tÃ¢ches que vous devez remplir.
1.4. Fichiers existants
On vous donne les ï¬chiers mdp.py, adversarial_search.py et world_mdp.py, mais vous ne devrez tra-
vailler que dans les deux derniers. mdp.py contient la dÃ©claration de lâ€™interface dâ€™un Markov Decision
Process, adversarial_search.py contient les algorithmes de recherche que vous allez implÃ©menter
tandis que world_mdp.py contient lâ€™adaptation du lle.World en MDP dans lequel chaque agent agit
chacun Ã  son tour.
Important: Vous pouvez modiï¬er TOUT ce que vous voulez dans le code qui vous est donnÃ© Ã 
lâ€™exception des tests unitaires. Ce code vous est uniquement donnÃ© comme guide.
1
FACULTÃ‰ DES SCIENCES
DÃ‰PARTEMENT Dâ€™INFORMATIQUE
1.5. Laser Learning Environment
Comme lors du premier projet, vous allez utiliser la librairie lle illustrÃ©e dans la Figure 1 pour ce
projet. Il existe un jupyter notebook sur le git de du projet qui vous montre les fonctionnalitÃ©s de lâ€™en-
vironnement et qui vous permet de vous familiariser Ã  son utilisation.
Figure 1: Rendu de l'environnement
Rapport de bugs: Lâ€™environnement LLE est encore rÃ©cent, et quelques bugs pourraient subsister.
Dans le cas oÃ¹ vous pensez en avoir trouvÃ© un, nâ€™hÃ©sitez pas Ã  le signaler par email Ã  lâ€™adresse
yannick.molinghen@ulb.be en joignant le code minimal nÃ©cessaire Ã  le reproduire. Chaque pre-
miÃ¨re personne Ã  rapporter un bug (et qui est conï¬rmÃ©) donnera lieu Ã  un point supplÃ©mentaire
avec un maximum de 2 points sur le projet.
2. Adaptation du World en MDP
Comme LLE est initialement un environnement cooperatif, vous allez adapter lâ€™environnement de
sorte quâ€™il fonctionne comme un MDP compÃ©titif dans lequel un agent agit Ã  la fois. Votre but est
de maximiser le nombre de gemmes collectÃ©es par lâ€™agent 0, les autres agents Ã©tant considÃ©rÃ©s
comme des adversaires. Pour ce faire, implÃ©mentez les classes WorldMDP et WorldState dans le ï¬chier
world_mdp.py.
WorldState
Comme il sâ€™agit dâ€™un MDP Ã  plusieurs agents et Ã  tour par tour, chaque Ã©tat doit retenir Ã  quel agent
câ€™est le tour dâ€™eï¬€ectuer une action.
WorldMDP
â€¢ AprÃ¨s avoir fait reset(), câ€™est Ã  lâ€™agent 0 dâ€™eï¬€ectuer une action. Ainsi,
world.trasition(Action.NORTH) fera uniquement bouger lâ€™agent 0 vers le nord tandis que tous les
autres resteront sur place. Ensuite, câ€™est Ã  lâ€™agent 1 de bouger et ainsi de suite.
â€¢ La mÃ©thode world.available_actions(state) renvoie les actions accessibles Ã  lâ€™agent courant.
â€¢ La valeur dâ€™un Ã©tat correspond Ã  la somme des rewards obtenues par les actions de lâ€™agent 0 (câ€™est-
Ã -dire les gemmes collectÃ©es + arriver sur une case de ï¬n).
â€¢ Si lâ€™agent 0 meurt lors dâ€™une transition, la valeur de lâ€™Ã©tat tombe Ã  la valeur lle.REWARD_AGENT_DIED
(-1) immÃ©diatement, sans prendre en compte les gemmes dÃ©jÃ  collectÃ©es.
2MDP = marqueur decisional proces, c'est une transition d'Ã©tatChaque joueur joue l'un aprÃ¨s l'autre, il faut implÃ©menter ce comportement, nous on joue l'agent0 (donc on doit maximiser son score Ã  lui). gem = 1 pt arrivÃ©e sur exit = 1 pt
FACULTÃ‰ DES SCIENCES
DÃ‰PARTEMENT Dâ€™INFORMATIQUE
3. Algorithmes de recherche
3.1. Minimax
Dans le ï¬chier adversarial_search.py implÃ©mentez lâ€™algorithme du minimax dans la fonction
minimax qui renvoie lâ€™action Ã  eï¬€ectuer par lâ€™agent 0 dans lâ€™Ã©tat donnÃ©. Cette fonction nâ€™accepte que
des Ã©tats pour lesquels câ€™est Ã  lâ€™agent 0 de jouer et lÃ¨ve une ValueError dans le cas contraire. On vous
suggÃ¨re de diviser cet algorithme en une fonction _min et une fonction _max. Nâ€™oubliez pas quâ€™il peut
y avoir plus dâ€™un adversaire.
3.2. Alpha-beta pruning
De maniÃ¨re similaire Ã  la tÃ¢che prÃ©cÃ©dente, implÃ©mentez lâ€™algorithme ğ›¼ğ›½-pruning dans la fonction
alpha_beta. Cette fonction renvoie lâ€™action Ã  eï¬€ectuer pour lâ€™agent 0 dans lâ€™Ã©tat donnÃ©.
3.3. Expectimax
Dans minimax et ğ›¼ğ›½-pruning, nous avons supposÃ© que lâ€™adversaire agissait de maniÃ¨re optimale.
Cependant, ce nâ€™est pas toujours le cas pour des humains. Lâ€™algorithme Â« expectimax Â» permet de
modÃ©liser le comportement probabiliste dâ€™humains qui pourraient prendre des choix sous-optimaux.
La nature dâ€™expectimax demande que nous connaissions la probabilitÃ© que lâ€™adversaire prenne chaque
action. Nous allons ici supposer que les autres agents prennent des actions uniformÃ©ment alÃ©atoires.
3.4. Meilleure Ã©valuation
Comme dernier exercice, on vous demande dâ€™implÃ©menter une sous-classe de WorldMDP dans laquelle
la valeur dâ€™un Ã©tat est calculÃ©e de maniÃ¨re plus intelligente que simplement en considÃ©rant le score de
lâ€™agent 0. Ecrivez cette sous-classe et vÃ©riï¬ez que pour une mÃªme carte, le nombre de noeuds Ã©tendus
est en eï¬€et plus bas quâ€™avec la fonction dâ€™Ã©valuation de base. Il nâ€™y a pas de test pour cet exercice.
4. Rapport
On vous demande dâ€™Ã©crire un court rapport (environ 2 pages) sur deux sujets:
â€¢ Expliquez lâ€™idÃ©e derriÃ¨re votre fonction dâ€™Ã©valuation utilisÃ©e en Section 3.4
â€¢ Comparez le nombre dâ€™Ã©tats Ã©tendus dans diï¬€Ã©rents algorithmes pour un mÃªme niveau. Pour ce faire,
inventez trois cartes (documenation dans le jupyter notebook et sur le wiki du git) que vous illustrez
dans le rapport. Pour chacune des cartes, reportez graphiquement le nombre dâ€™Ã©tats Ã©tendus dans la
recherche dâ€™action pour
â€¢ minimax,
â€¢ minimax avec votre meilleure fonction dâ€™Ã©valuation,
â€¢ alpha_beta,
â€¢ alpha_beta avec la meilleure fonction dâ€™Ã©valuation.