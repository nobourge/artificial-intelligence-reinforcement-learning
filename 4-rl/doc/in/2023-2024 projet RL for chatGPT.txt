Dans ce projet, vous allez implÃ©menter des algorithmes dâ€™apprentissage par renforcement (reinforce-
ment learning). On vous fournit des fichiers de base pour le projet que vous pouvez les tÃ©lÃ©charger sur
lâ€™universitÃ© virtuelle.
2 Introduction
Intuitivement, un processus de dÃ©cision markovien (Markov Decision Process, MDP) est composÃ© dâ€™Ã©-
tats dans lesquels un agent effectue des actions qui ont une certaine probabilitÃ© dâ€™atterrir dans un autre
Ã©tat.
AprÃ¨s chaque transition, lâ€™agent reÃ§oit une rÃ©compense qui donne une indication sur la qualitÃ© de
lâ€™action: plus la rÃ©compense est Ã©levÃ©e, meilleure est lâ€™action. Le but dâ€™un agent est de maximiser la
somme des rÃ©compenses au cours dâ€™un Ã©pisode (câ€™est-Ã -dire une partie).
Par consÃ©quent, pour chaque Ã©tat ğ‘ , on peut calculer sa valeur ğ‘‰(ğ‘ ) qui correspond Ã  la meilleure
somme des rÃ©compenses possible Ã  partir de ğ‘  grÃ¢ce Ã  lâ€™Ã©quation de Bellman prÃ©sentÃ©e dans lâ€™E-
quation 1.
ğ‘‰(ğ‘ ) = maxâˆ‘ğ‘ƒ(ğ‘ ,ğ‘,ğ‘ â€²)[ğ‘…(ğ‘ ,ğ‘,ğ‘ â€²)+ğ›¾ğ‘‰(ğ‘ â€²)] (1)
ğ‘âˆˆğ´
3 Value iteration
Lâ€™algorithme de Â«value iterationÂ» a pour but dâ€™approximer itÃ©rativement la valeur ğ‘‰(ğ‘ ) de chaque Ã©tat
s Ã  lâ€™aide de lâ€™Equation 2.
ğ‘‰ = maxâˆ‘ğ‘ƒ(ğ‘ ,ğ‘,ğ‘ â€²)[ğ‘…(ğ‘ ,ğ‘,ğ‘ â€²)+ğ›¾ğ‘‰ ]
ğ‘˜+1 ğ‘˜(ğ‘ â€²) (2)
ğ‘
ğ‘ â€²
Lâ€™algorithme de value iteration prend en paramÃ¨tre un entier ğ‘˜ qui dÃ©termine combien dâ€™itÃ©ration ef-
fectuer. ImplÃ©mentez lâ€™algorithme value iteration dans le fichier value_iteration.py.
Conseils:
â€¢ Quand vous implÃ©mentez lâ€™algorithme, faites attention Ã  vous baser sur ğ‘‰ pour calculer ğ‘‰
ğ‘˜ ğ‘˜+1
et Ã  ne pas modifier ğ‘‰ durant lâ€™itÃ©ration.
ğ‘˜
â€¢ Nâ€™oubliez pas que la valeur dâ€™un Ã©tat terminal est 0 par dÃ©finition.
4 Trouvez les bons paramÃ¨tres
Dans cet exercice, on vous demande de trouver les bons paramÃ¨tres du MDP et de value iteration pour
induire un certain comportement Ã  lâ€™agent.
Le MDP considÃ©rÃ© est celui prÃ©sentÃ© dans la Figure 1. Il sâ€™agit dâ€™un lle.World dont les rewards ont Ã©tÃ©
modifiÃ©es et qui comprend sept Ã©tats terminaux:
â€¢ Cinq â€œravinsâ€ en bas de la carte. Lâ€™agent meurt sâ€™il sâ€™y rend et reÃ§oit une â€œrÃ©compenseâ€ de âˆ’10;
â€¢ Une sortie proche qui rapporte une rÃ©compense de 1;
â€¢ Une sortie lointaine qui rapporte une rÃ©compense de 10.
Figure 1: MDP pour lequel trouver les bons paramÃ¨tres
Le but de cet exercice est dâ€™induire les comportements suivants:
1. PrÃ©fÃ©rer la sortie proche (+1) en longeant la falaise.
2. PrÃ©fÃ©rer la sortie proche (+1) en Ã©vitant la falaise.
3. PrÃ©fÃ©rer la sortie distante (+10) en longeant la falaise.
4. PrÃ©fÃ©rer la sortie distante (+10) en Ã©vitant la falaise.
5. Eviter de terminer le jeu (lâ€™Ã©pisode ne se termine jamais).
Pour ce faire, choisissez des valeurs adÃ©quates de chaque comportement dans la fonction correspon-
dante pour reward_live (la rÃ©compense Ã  chaque Ã©tape pour continuer le jeu), gamma (le discount fac-
tor) et noise (la probabilitÃ© de prendre une action alÃ©atoire) dans le fichier analysis.py.
5 Q-learning
Lâ€™algorithme de Q-learning consiste Ã  interagir avec lâ€™environnement pour mettre Ã  jour une fonction
dâ€™Ã©valuation ğ‘„(ğ‘ ,ğ‘). Contrairement aux exercices prÃ©cÃ©dents, on Ã©value ici la valeur dâ€™une action dans
un Ã©tat ğ‘„(ğ‘ ,ğ‘) et pas la valeur de lâ€™Ã©tat ğ‘‰(ğ‘ ).
Une fois que votre agent est entraÃ®nÃ©, vous pouvez exploiter la stratÃ©gie apprise en prenant lâ€™action
ayant la plus haute q-value max ğ‘„(ğ‘ ,ğ‘) dans chaque Ã©tat.
ğ‘âˆˆğ´
Pour entraÃ®ner votre agent, il est nÃ©cessaire dâ€™explorer lâ€™environnement. Pour ce faire, vous devez im-
plÃ©menter la stratÃ©gie dâ€™exploration dite Â«ğœº-greedyÂ» qui consiste Ã  choisir une action alÃ©atoire avec
une probabilitÃ© Îµ, et Ã  prendre la meilleure action avec une probabilitÃ© 1âˆ’ğœ€.
Note: Il nâ€™y a quasiment pas de tests pour les exercices liÃ©s au q-learning car ceux-ci sont trÃ¨s diffi-
ciles Ã  tester. On vous demande par contre dâ€™analyser vos rÃ©sultats dans le rapport (voir Section 6).
5.1 Q-learning tabulaire
Pour entraÃ®ner la fonction ğ‘„(ğ‘ ,ğ‘), on emploie une mÃ©thode adaptÃ©e de lâ€™Ã©quation de Bellman (Equa-
tion 1) illustrÃ©e dans lâ€™Equation 3, oÃ¹ ğ›¼ est le learning rate.
ğ‘„(ğ‘ ,ğ‘) â† (1âˆ’ğ›¼)ğ‘„(ğ‘ ,ğ‘)+ğ›¼[ğ‘…(ğ‘ ,ğ‘,ğ‘ â€²)+ğ›¾ğ‘‰(ğ‘ â€²)] (3)
ImplÃ©mentation
ImplÃ©mentez lâ€™algorithme de Q-learning dans le fichier qlearning.py. Faites en sorte que cette implÃ©-
mentation utilise un dictionnaire pour stocker les qvalues de chaque Ã©tat.
Conseils:
â€¢ Pour hasher des tableaux numpy, nous vous conseillons dâ€™utiliser hash(array.tobytes()).
â€¢ Pour favoriser lâ€™exploration, initialisez vos ğ‘„(ğ‘ ,ğ‘) Ã  1 et non Ã  0.
EntraÃ®nement et rapport
EntraÃ®nez votre algorithme sur les niveaux 1, 3 et 6 de LLE. Dans votre rapport, crÃ©ez un graphique
pour chacun de ces trois niveaux qui montre le score (câ€™est-Ã -dire la somme des rewards par Ã©pisode)
au cours de lâ€™entrainement. Voir Section 6 pour plus dâ€™informations sur le rapport.
Table 1: De gauche Ã  droite: niveau 1, niveau 3 et niveau 6 de LLE
Exemple
Lâ€™extrait de code ci-dessous montre ce Ã  quoi pourrait ressembler votre boucle dâ€™entrainement. Il nâ€™y
a aucune obligation de suivre ce canvas.
from lle import LLE
from rlenv.wrappers import TimeLimit
env = TimeLimit(LLE.level(1), 80) # Maximum 80 time steps
agents = [QAgent(lr, gamma, ...), QAgent(lr, gamma, ...), ...]
observation = env.reset()
done = truncated = False
score = 0
while not (done or truncated):
actions = [a.choose_action(observation) for a in agents]
next_observation, reward, done, truncated, info = env.step(actions)
for a in agents:
a.update(...)
score += reward
...
Lorsque vous rÃ©cupÃ©rez une observation, vous pouvez accÃ©der Ã  son contenu avec observation.data
qui contient un tableau numpy dont la forme est (n_agents, ...).
Conseil: LLE est un environnement multi-agent et utilise la librairie rlenv prÃ©vue Ã  cet effet. Nâ€™ou-
bliez pas de bien prendre en compte le caractÃ¨re multi-agent dans la reprÃ©sentation de vos donnÃ©es.
5.2 Approximate Q-learning
Le but de cet exercice est dâ€™implÃ©menter lâ€™Approximate Q-Learning (AQL) pour votre agent. Pour rap-
pel, AQL suppose lâ€™existence dâ€™une fonction ğ‘“(ğ‘ ) qui associe Ã  chaque Ã©tat (et pour chaque agent) un
vecteur de features [ğ‘“ (ğ‘ ),ğ‘“ (ğ‘ ),â€¦,ğ‘“ (ğ‘ )] avec ğ‘“ (ğ‘ ) : ğ‘† â†’ â„.
1 2 ğ‘› ğ‘˜
ConcrÃ¨tement, dans le cas de LLE, ğ‘“ (ğ‘ ) pourrait Ãªtre le nombre de gemmes non collectÃ©es, ğ‘“ (ğ‘ ) la
1 2
distance (en lignes) Ã  la gemme la plus proche, ğ‘“ (ğ‘ ) la distance (en colonnes) Ã  la gemme la plus
3
proche, ğ‘“ (ğ‘ ) la prÃ©sense (1) ou lâ€™absence (0) dâ€™un laser nord de lâ€™agent, etc.
4
Dans AQL, la fonction ğ‘„(ğ‘ ,ğ‘) dâ€™un agent ğ‘› est dÃ©finie comme indiquÃ© dans lâ€™Equation 4.
ğ‘˜
ğ‘„ (ğ‘ ,ğ‘) = âˆ‘ğ‘“ (ğ‘ )ğ‘¤ (4)
ğ‘› ğ‘– ğ‘,ğ‘–
ğ‘–=1
Dans le Q-learning, on dÃ©finit la TD-error ğ›¿ (temporal difference error) comme la diffÃ©rence entre la
Q-value estimÃ©e et la Q-value effective (estimÃ©e avec lâ€™Ã©quation de Bellman).
ğ›¿ = [ğ‘…(ğ‘ ,ğ‘,ğ‘ â€²)+ğ›¾ğ‘‰(ğ‘ â€²)]âˆ’ğ‘„(ğ‘ ,ğ‘) (5)
Dans AQL, le principe est de mettre Ã  jour les poids ğ‘¤ de ğ‘„ (ğ‘ ,ğ‘) de sorte Ã  diminuer la TD-error ğ›¿
ğ‘›
conformÃ©ment Ã  lâ€™Equation 6
ğ‘¤ â† ğ‘¤ +ğ›¼ğ›¿ğ‘“(ğ‘ ) (6)
ğ‘,ğ‘– ğ‘,ğ‘–
ImplÃ©mentation
Dans le fichier approximate_qlearning.py:
1. DÃ©finissez une fonction feature_extraction(world) qui prend en entrÃ©e le World et renvoie
pour chaque agent les features que vous aurez choisies.
2. ImplÃ©mentez lâ€™algorithme de Approximate Q-Learning.
Entrainement et rapport
EntraÃ®nez votre algorithme AQL sur les niveaux 1, 3 et 6 de LLE. Dans votre rapport, crÃ©ez un graphique
pour chacun de ces trois niveaux qui montre le score (câ€™est-Ã -dire la somme des rewards par Ã©pisode)
au cours de lâ€™entrainement. Voir Section 6 pour plus dâ€™informations sur le rapport.
6 Rapport
6.1 Value iteration
â€¢ Quelle limitation voyez-vous Ã  lâ€™algorithme de Value Iteration ? Dans quelle mesure est-il applic-
able Ã  un niveau avec un nombre plus important dâ€™agents ou avec une carte plus grande ?
â€¢ Expliquez pourquoi Value Iteration nâ€™est pas un algorithme de RL mais plutÃ´t un planificateur
hors-ligne.
â€¢ Appliquez lâ€™algorithme de Value Iteration sur le niveau 1 de LLE jusquâ€™Ã  ce que la policy se stabilise.
Ensuite, montrez graphiquement la stratÃ©gie dÃ©couverte par lâ€™algorithme.
6.2 ExpÃ©riences avec le Q-learning
Le but ici est de mesurer les performances des deux algorithmes (Q-learning et AQL) dans les niveaux
Ã©voquÃ©s prÃ©cedemment. Vous devez ensuite prÃ©senter ces donnÃ©es dans des graphiques adaptÃ©s (type
de graphique, Ã©chelle, axes, titre).
MÃ©trique
La mÃ©trique que vous devez tracer est le score, câ€™est-Ã -dire la somme des rÃ©compenses obtenues au
cours dâ€™un Ã©pisode.
MÃ©thode
Pour chaque expÃ©rience, donnez les paramÃ¨tres que vous avez utilisÃ©s:
â€¢ le discount factor ğ›¾
â€¢ le learning rate ğ›¼
â€¢ la fonction de feature extraction ğ‘“(ğ‘ )
â€¢ le niveau dont il est question
â€¢ la limite de temps que vous avez utilisÃ©e
â€¢ la maniÃ¨re dont Ã©volue votre ğœ€ au cours du temps
â€¢ â€¦
Graphiques
Nous vous conseillons la librairie matplotlib pour tracer des graphiques. Cette librairie est installÃ©e
automatiquement si vous avez utilisÃ© poetry.
Les graphiques que vous devez produire sont ceux qui indiquent le score moyen au cours du temps,
calculÃ© sur minimum 10 entrainements. Vos graphiques doivent aussi montrer la dÃ©viation standard
en plus de la moyenne, comme dans ce deuxiÃ¨me exemple de la documentation de matplotlib.
Remise
Le livrable de ce projet se prÃ©sente sous la forme dâ€™un fichier zip contenant vos sources python ainsi
que votre rapport en PDF. Nous vous encourageons Ã  utiliser un outil tel que Typst ou Latex (par ex-
emple avec Overleaf) pour rÃ©diger votre rapport.
Ce travail est individuel et doit Ãªtre rendu sur lâ€™UniversitÃ© Virtuelle pour le 10/12/2023 Ã  23:59.
7 Annexes
7.1 PrÃ©cisions sur lle.LLE
La classe lle.LLE est directement dÃ©diÃ©e au RL multi-agents (MARL) coopÃ©ratif. Par soucis de gÃ©nÃ©ral-
itÃ©, LLE utilise le vocabulaire des observations plutÃ´t que des Ã©tats afin de diffÃ©rencier ce que les agents
voient de lâ€™Ã©tat de lâ€™environnement.
Dans LLE, une observation peut se reprÃ©senter couche par couche, comme montrÃ© dans la Figure 2.
Figure 2: ReprÃ©sentation dâ€™une observation correspondant Ã  la situation de lâ€™imade de droite dans la
Table 1. Les cases noires reprÃ©sentent des 1, les cases grises des âˆ’1 et les cases blanches des 0. Par
soucis de concision, certaines couches (agent 2, agent 3, laser 2, laser 3) ont Ã©tÃ© ommise.
Le type dâ€™observation indiquÃ© dans la Figure 2 peut Ãªtre utilisÃ© en utilisant ObservationType.LAYERED.
Dans ce cas, chaque agent reÃ§oit une observation (x, height, width) oÃ¹ x dÃ©pend du nombre dâ€™a-
gents.
from lle import LLE, ObservationType
env = LLE.level(6, ObservationType.LAYERED)
print(env.observation_shape) # (12, 12, 13)
Cependant, par dÃ©faut (et pour pouvoir Ãªtre utilisÃ©es dans des rÃ©seaux de neurones linÃ©aires), LLE utilise
le type dâ€™observation ObservationType.FLATTENED, qui encodent exactement les mÃªmes informations
que ObservationType.LAYERED, mais applaties sur une seule dimension.
from lle import LLE, ObservationType
env = LLE.level(6, ObservationType.FLATTENED) # On peut ommettre le second paramÃ¨tre
print(env.observation_shape) # Affiche (1872,), cÃ d 12 * 12 * 13
Notez que chaque observation possÃ¨de aussi des informations sur lâ€™Ã©tat du World dans
observation.state, Ã  la diffÃ©rence que le WorldState auquel vous Ãªtes habituÃ©s a lui aussi Ã©tÃ© trans-
formÃ© en tableau numpy pour Ãªtre utilisable dans le contexte du deep MARL.
7